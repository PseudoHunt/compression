{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "from safetensors.torch import save_file\n",
        "import resnet\n",
        "#import mobilenetv2\n",
        "\n",
        "# ==== Config ====\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 8\n",
        "FIXED_T = 100.5\n",
        "LR = 0.001\n",
        "NUM_ITERATIONS = 100\n",
        "CR_target = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==== Dataset ====\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ==== Load model ====\n",
        "model = resnet.resnet18(pretrained=False, device=DEVICE).to(DEVICE)\n",
        "#model = mobilenetv2.mobilenet_v2(pretrained=False, device=DEVICE).to(DEVICE)\n",
        "state_dict = torch.load(\"/content/resnet18.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "# ==== Evaluation ====\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            pred = model(x)\n",
        "            _, p = torch.max(pred, 1)\n",
        "            correct += (p == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"🌟 Accuracy: {acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "# ==== Activation Capture ====\n",
        "temp_activations = {}\n",
        "def activation_hook(layer_name):\n",
        "    def hook(module, input, output):\n",
        "        temp_activations[layer_name] = input[0].detach().clone()\n",
        "    return hook\n",
        "\n",
        "for name, layer in model.named_modules():\n",
        "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "        layer.register_forward_hook(activation_hook(name))\n",
        "\n",
        "# ==== Quantizer ====\n",
        "class MinMaxQuantization(nn.Module):\n",
        "    def __init__(self, weight, num_levels=2**NUM_BITS, fixed_T=FIXED_T, entropy_budget=None):\n",
        "        super().__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.fixed_T = fixed_T\n",
        "        self.entropy_budget = entropy_budget\n",
        "        w_min_init = weight.min().detach()\n",
        "        w_max_init = weight.max().detach()\n",
        "        pad = 0.05 * (w_max_init - w_min_init)\n",
        "        self.w_min = nn.Parameter(w_min_init - pad)\n",
        "        self.w_max = nn.Parameter(w_max_init + pad)\n",
        "\n",
        "    def forward(self, w):\n",
        "        EPS = 1e-6\n",
        "        w_min = self.w_min.clamp(max=self.w_max.item() - EPS)\n",
        "        w_max = self.w_max.clamp(min=w_min.item() + EPS)\n",
        "        w_norm = (w - w_min) / (w_max - w_min + EPS)\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "        dists = -torch.abs(w_norm.unsqueeze(-1) - q_levels)\n",
        "        soft_probs = torch.softmax(dists * self.fixed_T, dim=-1)\n",
        "        w_q = (soft_probs * q_levels).sum(dim=-1)\n",
        "        w_deq = w_q * (w_max - w_min) + w_min\n",
        "        bin_mass = soft_probs.sum(dim=0)\n",
        "        bin_probs = bin_mass / bin_mass.sum()\n",
        "        entropy = -(bin_probs * (bin_probs + EPS).log()).sum()\n",
        "        budget_penalty = (entropy / (self.entropy_budget + EPS)) ** 2\n",
        "        return w_deq, entropy, budget_penalty, soft_probs\n",
        "\n",
        "    def export_hard_quant(self, w):\n",
        "        EPS = 1e-6\n",
        "        w_min = self.w_min.clamp(max=self.w_max.item() - EPS)\n",
        "        w_max = self.w_max.clamp(min=w_min.item() + EPS)\n",
        "        w_norm = (w - w_min) / (w_max - w_min + EPS)\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "        dists = -torch.abs(w_norm.unsqueeze(-1) - q_levels)\n",
        "        q_indices = torch.argmax(dists, dim=-1).clamp(0, self.num_levels - 1).to(torch.int32)\n",
        "        w_q = q_levels[q_indices]\n",
        "        w_deq = w_q * (w_max - w_min) + w_min\n",
        "        return {\n",
        "            \"q_indices\": q_indices.cpu(),\n",
        "            \"w_min\": w_min.cpu().unsqueeze(0),\n",
        "            \"w_max\": w_max.cpu().unsqueeze(0),\n",
        "            \"dequant\": w_deq.cpu()\n",
        "        }\n",
        "\n",
        "# ==== Optimization & Export ====\n",
        "def optimize_all_layers(model, test_loader):\n",
        "    model.eval()\n",
        "    data_iterator = iter(test_loader)\n",
        "    x, y = next(data_iterator)\n",
        "    x, y = x[:128].to(DEVICE), y[:128].to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model(x)\n",
        "\n",
        "    safetensor_dict = {}\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        module_name = name.rsplit('.', 1)[0]\n",
        "        try:\n",
        "            mod = dict(model.named_modules())[module_name]\n",
        "            if isinstance(mod, nn.BatchNorm2d) or name.endswith(\".bias\"):\n",
        "                base_name = name.replace('.', '_')\n",
        "                safetensor_dict[f\"{base_name}.dequant\"] = param.detach().cpu()\n",
        "                print(f\"🟢 Stored unquantized: {name}\")\n",
        "                continue\n",
        "        except KeyError:\n",
        "            print(f\"⚠️ Module not found for param: {name}\")\n",
        "\n",
        "        print(f\"\\n🔧 Optimizing {name}...\")\n",
        "        layer_name = name.replace(\".weight\", \"\")\n",
        "        matched_key = next((k for k in temp_activations if layer_name in k), None)\n",
        "        activation_input = temp_activations.get(matched_key, None)\n",
        "        if activation_input is not None:\n",
        "            activation_input = activation_input.detach().clone().to(DEVICE)\n",
        "            del temp_activations[matched_key]\n",
        "\n",
        "        original_param = param.detach().clone()\n",
        "        entropy_budget = (32 * original_param.numel()) / CR_target\n",
        "        quant_layer = MinMaxQuantization(original_param, entropy_budget=entropy_budget).to(DEVICE)\n",
        "        optimizer = optim.Adam(quant_layer.parameters(), lr=LR)\n",
        "        mse_loss_fn = nn.MSELoss()\n",
        "        original_param_data = param.data.clone()\n",
        "\n",
        "        for iteration in range(NUM_ITERATIONS):\n",
        "            optimizer.zero_grad()\n",
        "            q_param, entropy, penalty, soft_probs = quant_layer(original_param)\n",
        "            recon_loss = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            if activation_input is not None:\n",
        "                try:\n",
        "                    conv_layer = next((m for n, m in model.named_modules()\n",
        "                                       if n == layer_name and isinstance(m, (nn.Conv2d, nn.Linear))), None)\n",
        "                    if isinstance(conv_layer, nn.Conv2d):\n",
        "                        q_out = nn.functional.conv2d(activation_input, q_param,\n",
        "                                                     stride=conv_layer.stride, padding=conv_layer.padding,\n",
        "                                                     dilation=conv_layer.dilation, groups=conv_layer.groups)\n",
        "                        o_out = nn.functional.conv2d(activation_input, original_param,\n",
        "                                                     stride=conv_layer.stride, padding=conv_layer.padding,\n",
        "                                                     dilation=conv_layer.dilation, groups=conv_layer.groups)\n",
        "                        recon_loss = mse_loss_fn(q_out, o_out)\n",
        "                    elif isinstance(conv_layer, nn.Linear):\n",
        "                        flat = activation_input.view(activation_input.size(0), -1)\n",
        "                        q_out = nn.functional.linear(flat, q_param)\n",
        "                        o_out = nn.functional.linear(flat, original_param)\n",
        "                        recon_loss = mse_loss_fn(q_out, o_out)\n",
        "                except:\n",
        "                    recon_loss = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                param.data = q_param.detach()\n",
        "                class_loss = nn.CrossEntropyLoss()(model(x), y)\n",
        "                param.data = original_param_data\n",
        "\n",
        "            # if class_loss > 0.2:\n",
        "            #     break\n",
        "\n",
        "            total_loss = 0.1 * recon_loss + 0.9 * class_loss + 0.0 * entropy\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 10 == 0:\n",
        "                print(f\"Iter {iteration}: recon={recon_loss.item():.6f}, class={class_loss.item():.4f}, entropy={entropy.item():.2f}\")\n",
        "\n",
        "        final_param = quant_layer.export_hard_quant(original_param)\n",
        "        with torch.no_grad():\n",
        "            param.copy_(final_param[\"dequant\"].to(param.device))\n",
        "\n",
        "        base_name = name.replace('.', '_')\n",
        "        safetensor_dict[f\"{base_name}.q_indices\"] = final_param[\"q_indices\"]\n",
        "        safetensor_dict[f\"{base_name}.w_min\"] = final_param[\"w_min\"]\n",
        "        safetensor_dict[f\"{base_name}.w_max\"] = final_param[\"w_max\"]\n",
        "        safetensor_dict[f\"{base_name}.dequant\"] = final_param[\"dequant\"]\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        with torch.no_grad():\n",
        "            model(x)\n",
        "\n",
        "    # Store skipped biases and BN params as-is\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if \"bn\" in name or \".bias\" in name:\n",
        "    #         base_name = name.replace('.', '_')\n",
        "    #         safetensor_dict[f\"{base_name}.dequant\"] = param.detach().cpu()\n",
        "    #         print(f\"🟢 Stored unquantized: {name}\")\n",
        "    # 🔥 Additionally save running_mean and running_var\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            base_name = name.replace('.', '_')\n",
        "            safetensor_dict[f\"{base_name}.running_mean\"] = module.running_mean.detach().cpu()\n",
        "            safetensor_dict[f\"{base_name}.running_var\"] = module.running_var.detach().cpu()\n",
        "            print(f\"🟢 Stored BN running stats: {name}\")\n",
        "\n",
        "    save_file(safetensor_dict, \"quantized_model.safetensors\")\n",
        "    print(\"💾 Saved all quantized weights to 'quantized_model.safetensors'\")\n",
        "    print(\"✅ Full-layer quantization complete.\")\n",
        "\n",
        "# ==== Run ====\n",
        "print(\"\\n📊 Accuracy BEFORE quantization:\")\n",
        "evaluate(model, test_loader)\n",
        "optimize_all_layers(model, test_loader)\n",
        "print(\"\\n📊 Accuracy AFTER quantization:\")\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "id": "WGii3gm8P6i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the model as .pt file and load a new instance of the model with the saved weights.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "# Save the model's state_dict\n",
        "torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "# Load a new instance of the model\n",
        "new_model = resnet.resnet18(pretrained=False, device=device).to(device)\n",
        "\n",
        "# Load the saved weights\n",
        "new_model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "# Set the new model to evaluation mode\n",
        "new_model.eval()\n"
      ],
      "metadata": {
        "id": "d0FDOYiCP1bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaioEHqaPsrU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from safetensors.torch import load_file\n",
        "import resnet  # your resnet18.py\n",
        "\n",
        "# ==== Config ====\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "QUANT_PATH = \"quantized_model.safetensors\"\n",
        "WEIGHT_PATH = \"/content/resnet18.pt\"  # Update this path\n",
        "\n",
        "# ==== Load Models ====\n",
        "original_model = resnet.resnet18(pretrained=False)\n",
        "original_model.load_state_dict(torch.load(WEIGHT_PATH, map_location=\"cpu\"))\n",
        "original_model.eval()\n",
        "\n",
        "quantized_model = resnet.resnet18(pretrained=False)\n",
        "quantized_model.eval()\n",
        "\n",
        "# ==== Load Quantized Weights from Safetensors ====\n",
        "quant_data = load_file(QUANT_PATH)\n",
        "\n",
        "# # ==== Apply Dequantized Weights (directly from .dequant field) ====\n",
        "# with torch.no_grad():\n",
        "#      for name, param in quantized_model.named_parameters():\n",
        "#         base = name.replace('.', '_')\n",
        "#         key_dequant = f\"{base}.dequant\"\n",
        "#         if key_dequant in quant_data:\n",
        "#             dequant = quant_data[key_dequant]\n",
        "#             try:\n",
        "#                 param.copy_(dequant.to(param.device))\n",
        "#                 print(f\"🔁 Loaded .dequant: {name}\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"⚠️ Error loading {name}: {e}\")\n",
        "#         else:\n",
        "#             print(f\"⏭️ Skipping: {name}\")\n",
        "\n",
        "\n",
        "# ==== Dequantization Function for Quantized Weights ====\n",
        "def reconstruct_param(q_indices, w_min, w_max, num_bits=NUM_BITS):\n",
        "    q_levels = torch.linspace(0, 1, 2**num_bits, device=q_indices.device)\n",
        "    q_values = q_levels[q_indices]\n",
        "    return q_values * (w_max - w_min) + w_min\n",
        "\n",
        "# ==== Load All Parameters ====\n",
        "named_modules = dict(model.named_modules())\n",
        "\n",
        "with torch.no_grad():\n",
        "    for name, param in quantized_model.named_parameters():\n",
        "        base_name = name.replace('.', '_')\n",
        "        key_qidx = f\"{base_name}.q_indices\"\n",
        "        key_wmin = f\"{base_name}.w_min\"\n",
        "        key_wmax = f\"{base_name}.w_max\"\n",
        "        key_dequant = f\"{base_name}.dequant\"\n",
        "\n",
        "        # If quantized weights\n",
        "        if key_qidx in quant_data and key_wmin in quant_data and key_wmax in quant_data:\n",
        "            print(f\"🔁 Reconstructing quantized weight: {name}\")\n",
        "            q_idx = quant_data[key_qidx].to(param.device)\n",
        "            w_min = quant_data[key_wmin].to(param.device).squeeze(0)\n",
        "            w_max = quant_data[key_wmax].to(param.device).squeeze(0)\n",
        "\n",
        "            dequant = reconstruct_param(q_idx, w_min, w_max, num_bits=NUM_BITS)\n",
        "            if dequant.shape == param.shape:\n",
        "                param.copy_(dequant)\n",
        "            else:\n",
        "                print(f\"⚠️ Shape mismatch for {name}: dequant shape {dequant.shape} vs param shape {param.shape}\")\n",
        "\n",
        "        # If unquantized biases or BN parameters\n",
        "        elif key_dequant in quant_data:\n",
        "            print(f\"🔁 Loading unquantized parameter: {name}\")\n",
        "            param.copy_(quant_data[key_dequant].to(param.device))\n",
        "\n",
        "        else:\n",
        "            print(f\"⏭️ No matching entry for: {name}\")\n",
        "\n",
        "# ==== Load BatchNorm Running Stats ====\n",
        "for name, module in quantized_model.named_modules():\n",
        "    if isinstance(module, nn.BatchNorm2d):\n",
        "        base_name = name.replace('.', '_')\n",
        "\n",
        "        running_mean_key = f\"{base_name}.running_mean\"\n",
        "        running_var_key = f\"{base_name}.running_var\"\n",
        "\n",
        "        if running_mean_key in quant_data:\n",
        "            print(f\"🔁 Loading BN running_mean: {name}\")\n",
        "            module.running_mean.copy_(quant_data[running_mean_key].to(module.running_mean.device))\n",
        "\n",
        "        if running_var_key in quant_data:\n",
        "            print(f\"🔁 Loading BN running_var: {name}\")\n",
        "            module.running_var.copy_(quant_data[running_var_key].to(module.running_var.device))\n",
        "\n",
        "print(\"✅ Quantized model loaded successfully!\")\n",
        "\n",
        "# ==== Attach Hooks to Capture Layer Outputs ====\n",
        "original_outputs = {}\n",
        "quant_outputs = {}\n",
        "\n",
        "def get_hook(storage_dict, name):\n",
        "    def hook(module, input, output):\n",
        "        storage_dict[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "for name, module in new_model.named_modules():\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)):\n",
        "        module.register_forward_hook(get_hook(original_outputs, name))\n",
        "\n",
        "for name, module in quantized_model.named_modules():\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)):\n",
        "        module.register_forward_hook(get_hook(quant_outputs, name))\n",
        "\n",
        "# ==== Forward Pass ====\n",
        "#sample_input = torch.randn(1, 3, 32, 32)\n",
        "data_iterator = iter(test_loader)\n",
        "x, y = next(data_iterator)\n",
        "#x, y = x[:2].to(DEVICE), y[:2].to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    out_original = new_model(x)\n",
        "    out_quant = quantized_model(x)\n",
        "print(out_original)\n",
        "print(out_quant)\n",
        "# ==== Compare Layer Outputs ====\n",
        "layer_diffs = []\n",
        "for layer in original_outputs:\n",
        "    if layer in quant_outputs:\n",
        "        o = original_outputs[layer]\n",
        "        q = quant_outputs[layer]\n",
        "        rel_err = torch.norm(o - q) / (torch.norm(o) + 1e-8)\n",
        "        layer_diffs.append((layer, rel_err.item()))\n",
        "\n",
        "# ==== Display Result ====\n",
        "diff_df = pd.DataFrame(layer_diffs, columns=[\"Layer\", \"RelativeError\"])\n",
        "diff_df = diff_df.sort_values(\"RelativeError\", ascending=False)\n",
        "print(diff_df.to_string(index=False))\n"
      ]
    }
  ]
}