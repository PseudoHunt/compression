# -*- coding: utf-8 -*-
"""MixPrecisionCAQ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUUgaq7Dz1B4QTqcPaqNKL7C9hsFkKSf
"""

# Mixed-precision group-wise quantization for OPT-125M with per-layer 4-bit code-space entropy
# Requirements: pip install torch transformers
import math
import random
from typing import Dict, Tuple, List

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM


@torch.no_grad()
def affine_minmax_quantize_to_int(x: torch.Tensor, n_bits: int) -> torch.Tensor:
    """
    Group-level min-max affine quantization to integer bins [0, 2^n_bits - 1].
    x: 1D tensor (the group)
    Returns integer tensor of same shape, dtype=torch.int32
    """
    qmin, qmax = 0, (1 << n_bits) - 1
    x_min = x.min()
    x_max = x.max()
    if x_max <= x_min:  # degenerate group
        return torch.zeros_like(x, dtype=torch.int32)
    scale = (x_max - x_min) / float(qmax)
    q = torch.round((x - x_min) / scale).clamp(qmin, qmax)
    return q.to(torch.int32)


def rebin_to_4bit(q_int: torch.Tensor, n_bits: int) -> torch.Tensor:
    """
    Map 2b/3b/4b integer bins into a single 4-bit code space [0..15].
    - 2-bit -> multiply by 4 (shift left by 2)
    - 3-bit -> multiply by 2 (shift left by 1)
    - 4-bit -> unchanged
    """
    if n_bits == 2:
        return (q_int * 4).to(torch.int32)          # {0,4,8,12}
    elif n_bits == 3:
        return (q_int * 2).to(torch.int32)          # {0,2,4,...,14}
    elif n_bits == 4:
        return q_int.to(torch.int32)                # {0..15}
    else:
        raise ValueError("n_bits must be 2, 3, or 4.")


def shannon_entropy_base2(int_codes: torch.Tensor, num_bins: int = 16) -> float:
    """
    Compute H = -sum p * log2 p of a 1D integer array with values in [0..num_bins-1].
    """
    hist = torch.bincount(int_codes, minlength=num_bins).float()
    total = hist.sum()
    if total == 0:
        return 0.0
    probs = hist / total
    nz = probs[probs > 0]
    return float(-(nz * torch.log2(nz)).sum().item())


def assign_group_bits(
    num_groups: int,
    pct_2bit: float,
    pct_3bit: float,
    pct_4bit: float,
    policy: str = "random",
    group_scores: torch.Tensor = None,
    seed: int = 0,
) -> List[int]:
    """
    Assign each group a bitwidth (2,3,4) according to given percentages.
    - policy="random": random assignment with seed
    - policy="variance": use scores (e.g., per-group variance). Highest scores -> 4-bit, next -> 3-bit, rest -> 2-bit
    Returns: list of length num_groups with values in {2,3,4}
    """
    if not math.isclose(pct_2bit + pct_3bit + pct_4bit, 1.0, rel_tol=1e-6):
        raise ValueError("Percentages must sum to 1.0")

    n2 = int(round(pct_2bit * num_groups))
    n3 = int(round(pct_3bit * num_groups))
    n4 = num_groups - n2 - n3
    if n4 < 0:
        # Adjust robustly if rounding overflows
        overflow = -n4
        # Reduce from the largest of n2/n3
        if n2 >= n3:
            n2 -= overflow
        else:
            n3 -= overflow
        n4 = 0

    indices = list(range(num_groups))

    if policy == "variance" and group_scores is not None and len(group_scores) == num_groups:
        # Sort indices by descending score (high score = more bits)
        sorted_idx = [i for _, i in sorted(zip(group_scores.tolist(), indices), key=lambda t: -t[0])]
    else:
        random.Random(seed).shuffle(indices)
        sorted_idx = indices

    bits = [2] * num_groups
    for i in sorted_idx[:n4]:
        bits[i] = 4
    for i in sorted_idx[n4:n4 + n3]:
        bits[i] = 3
    # remaining are 2-bit
    return bits


@torch.no_grad()
def quantize_tensor_mixed_to_4bit_codes(
    W: torch.Tensor,
    group_size: int,
    pct_2bit: float,
    pct_3bit: float,
    pct_4bit: float,
    assign_policy: str = "random",
    seed: int = 0,
) -> Tuple[torch.Tensor, List[int]]:
    """
    Quantize a weight tensor W group-wise with mixed precision, and map to 4-bit code space.
    Returns:
      q4_codes: flattened 1D int32 tensor with values in [0..15]
      group_bits: list of bitwidths per group
    """
    flat = W.view(-1).contiguous()
    N = flat.numel()
    num_groups = (N + group_size - 1) // group_size

    # Optional: variance-based scoring for assignment
    if assign_policy == "variance":
        scores = []
        for g in range(num_groups):
            s = g * group_size
            e = min(s + group_size, N)
            grp = flat[s:e]
            # Use unbiased variance as an importance heuristic
            if grp.numel() > 1:
                scores.append(torch.var(grp, unbiased=True).item())
            else:
                scores.append(0.0)
        group_scores = torch.tensor(scores)
    else:
        group_scores = None

    group_bits = assign_group_bits(
        num_groups, pct_2bit, pct_3bit, pct_4bit,
        policy=assign_policy, group_scores=group_scores, seed=seed
    )

    q4_out = torch.empty(N, dtype=torch.int32, device=flat.device)

    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        grp = flat[s:e]
        n_bits = group_bits[g]

        q_int = affine_minmax_quantize_to_int(grp, n_bits)  # to [0..(2^n_bits-1)]
        q4 = rebin_to_4bit(q_int, n_bits)                  # to [0..15]
        q4_out[s:e] = q4

    return q4_out, group_bits


@torch.no_grad()
def quantize_opt125m_and_report_entropy(
    model_name: str = "facebook/opt-125m",
    group_size: int = 128,
    pct_2bit: float = 0.2,
    pct_3bit: float = 0.3,
    pct_4bit: float = 0.5,
    assign_policy: str = "random",   # or "variance"
    seed: int = 0,
    device: str = "cpu",
) -> Dict[str, Dict]:
    """
    Quantizes each Linear layer's weights group-wise with mixed precision and computes per-layer entropy
    of the combined 4-bit codes.

    Returns a dict:
      {
        layer_name: {
          "entropy_bits": float,
          "num_params": int,
          "num_groups": int,
          "bit_alloc_counts": {2: n2, 3: n3, 4: n4},
        }, ...
      }
    """
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=None)
    model.to(device)
    model.eval()

    results = {}

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            W = module.weight.detach().to(device)
            q4_codes, group_bits = quantize_tensor_mixed_to_4bit_codes(
                W, group_size, pct_2bit, pct_3bit, pct_4bit,
                assign_policy=assign_policy, seed=seed
            )
            H = shannon_entropy_base2(q4_codes, num_bins=16)
            n_groups = len(group_bits)
            counts = {2: group_bits.count(2), 3: group_bits.count(3), 4: group_bits.count(4)}
            results[name] = {
                "entropy_bits": H,
                "num_params": W.numel(),
                "num_groups": n_groups,
                "bit_alloc_counts": counts,
            }

            # (Optional) keep the quantized codes around if you want:
            # module.register_buffer("_q4_codes", q4_codes.view_as(W).to(torch.uint8))
            break

    return results


if __name__ == "__main__":
    # --- Configure your percentages here (they must sum to 1.0) ---
    pct_2bit = 0.80  # x%
    pct_3bit = 0.00  # y%
    pct_4bit = 0.20  # z%

    summary = quantize_opt125m_and_report_entropy(
        model_name="facebook/opt-125m",
        group_size=128,
        pct_2bit=pct_2bit,
        pct_3bit=pct_3bit,
        pct_4bit=pct_4bit,
        assign_policy="random",  # change to "variance" to allocate more bits to high-variance groups
        seed=42,
        device="cpu",
    )

    # Pretty print
    print("Per-layer entropy of combined 4-bit codes (OPT-125M):")
    for layer, info in summary.items():
        print(f"{layer:55s} | H={info['entropy_bits']:.4f} bits | params={info['num_params']:,} "
              f"| groups={info['num_groups']:,} | bit alloc {info['bit_alloc_counts']}")



"""# Histogram of weights"""

import matplotlib.pyplot as plt
from transformers import AutoModelForCausalLM
import torch.nn as nn
import torch
import math
import random
from typing import Dict, Tuple, List

@torch.no_grad()
def affine_minmax_quantize_to_int(x: torch.Tensor, n_bits: int) -> torch.Tensor:
    """
    Group-level min-max affine quantization to integer bins [0, 2^n_bits - 1].
    x: 1D tensor (the group)
    Returns integer tensor of same shape, dtype=torch.int32
    """
    qmin, qmax = 0, (1 << n_bits) - 1
    x_min = x.min()
    x_max = x.max()
    if x_max <= x_min:  # degenerate group
        return torch.zeros_like(x, dtype=torch.int32)
    scale = (x_max - x_min) / float(qmax)
    q = torch.round((x - x_min) / scale).clamp(qmin, qmax)
    return q.to(torch.int32)


def rebin_to_4bit(q_int: torch.Tensor, n_bits: int) -> torch.Tensor:
    """
    Map 2b/3b/4b integer bins into a single 4-bit code space [0..15].
    - 2-bit -> multiply by 4 (shift left by 2)
    - 3-bit -> multiply by 2 (shift left by 1)
    - 4-bit -> unchanged
    """
    if n_bits == 2:
        return (q_int * 4).to(torch.int32)          # {0,4,8,12}
    elif n_bits == 3:
        return (q_int * 2).to(torch.int32)          # {0,2,4,...,14}
    elif n_bits == 4:
        return q_int.to(torch.int32)                # {0..15}
    else:
        raise ValueError("n_bits must be 2, 3, or 4.")


def shannon_entropy_base2(int_codes: torch.Tensor, num_bins: int = 16) -> float:
    """
    Compute H = -sum p * log2 p of a 1D integer array with values in [0..num_bins-1].
    """
    hist = torch.bincount(int_codes, minlength=num_bins).float()
    total = hist.sum()
    if total == 0:
        return 0.0
    probs = hist / total
    nz = probs[probs > 0]
    return float(-(nz * torch.log2(nz)).sum().item())


def assign_group_bits(
    num_groups: int,
    pct_2bit: float,
    pct_3bit: float,
    pct_4bit: float,
    policy: str = "random",
    group_scores: torch.Tensor = None,
    seed: int = 0,
) -> List[int]:
    """
    Assign each group a bitwidth (2,3,4) according to given percentages.
    - policy="random": random assignment with seed
    - policy="variance": use scores (e.g., per-group variance). Highest scores -> 4-bit, next -> 3-bit, rest -> 2-bit
    Returns: list of length num_groups with values in {2,3,4}
    """
    if not math.isclose(pct_2bit + pct_3bit + pct_4bit, 1.0, rel_tol=1e-6):
        raise ValueError("Percentages must sum to 1.0")

    n2 = int(round(pct_2bit * num_groups))
    n3 = int(round(pct_3bit * num_groups))
    n4 = num_groups - n2 - n3
    if n4 < 0:
        # Adjust robustly if rounding overflows
        overflow = -n4
        # Reduce from the largest of n2/n3
        if n2 >= n3:
            n2 -= overflow
        else:
            n3 -= overflow
        n4 = 0

    indices = list(range(num_groups))

    if policy == "variance" and group_scores is not None and len(group_scores) == num_groups:
        # Sort indices by descending score (high score = more bits)
        sorted_idx = [i for _, i in sorted(zip(group_scores.tolist(), indices), key=lambda t: -t[0])]
    else:
        random.Random(seed).shuffle(indices)
        sorted_idx = indices

    bits = [2] * num_groups
    for i in sorted_idx[:n4]:
        bits[i] = 4
    for i in sorted_idx[n4:n4 + n3]:
        bits[i] = 3
    # remaining are 2-bit
    return bits


@torch.no_grad()
def quantize_tensor_mixed_to_4bit_codes(
    W: torch.Tensor,
    group_size: int,
    pct_2bit: float,
    pct_3bit: float,
    pct_4bit: float,
    assign_policy: str = "random",
    seed: int = 0,
) -> Tuple[torch.Tensor, List[int]]:
    """
    Quantize a weight tensor W group-wise with mixed precision, and map to 4-bit code space.
    Returns:
      q4_codes: flattened 1D int32 tensor with values in [0..15]
      group_bits: list of bitwidths per group
    """
    flat = W.view(-1).contiguous()
    N = flat.numel()
    num_groups = (N + group_size - 1) // group_size

    # Optional: variance-based scoring for assignment
    if assign_policy == "variance":
        scores = []
        for g in range(num_groups):
            s = g * group_size
            e = min(s + group_size, N)
            grp = flat[s:e]
            # Use unbiased variance as an importance heuristic
            if grp.numel() > 1:
                scores.append(torch.var(grp, unbiased=True).item())
            else:
                scores.append(0.0)
        group_scores = torch.tensor(scores)
    else:
        group_scores = None

    group_bits = assign_group_bits(
        num_groups, pct_2bit, pct_3bit, pct_4bit,
        policy=assign_policy, group_scores=group_scores, seed=seed
    )

    q4_out = torch.empty(N, dtype=torch.int32, device=flat.device)

    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        grp = flat[s:e]
        n_bits = group_bits[g]

        q_int = affine_minmax_quantize_to_int(grp, n_bits)  # to [0..(2^n_bits-1)]
        q4 = rebin_to_4bit(q_int, n_bits)                  # to [0..15]
        q4_out[s:e] = q4

    return q4_out, group_bits


# Assuming you have the q4_codes from the last layer processed in the previous cell
# If you want to visualize a different layer, you might need to modify the quantize_opt125m_and_report_entropy function
# to store or return the q4_codes for all layers.
# For now, let's use the q4_codes from the last processed layer (k_proj in the example output)

# The q4_codes are stored temporarily in the function execution.
# To visualize them, we need to re-run the quantization for a specific layer
# or modify the original function to return the q4_codes for each layer.

# Let's modify the function call to get the q4_codes for the first linear layer found
# in this example, it's model.decoder.layers.0.self_attn.k_proj
model = AutoModelForCausalLM.from_pretrained("facebook/opt-125m", torch_dtype=torch.float32, device_map=None)
model.eval()

target_layer_name = None
for name, module in model.named_modules():
    if isinstance(module, nn.Linear):
        target_layer_name = name
        W = module.weight.detach()
        q4_codes, group_bits = quantize_tensor_mixed_to_4bit_codes(
            W, group_size=128, pct_2bit=0.80, pct_3bit=0.00, pct_4bit=0.20,
            assign_policy="random", seed=42,
        )
        break # Process only the first linear layer for visualization example

if target_layer_name is not None:
    print(f"Visualizing histogram for layer: {target_layer_name}")
    plt.figure(figsize=(8, 6))
    plt.hist(q4_codes.numpy(), bins=16, range=(0, 16), density=True, alpha=0.7)
    plt.title(f'Histogram of Quantized 4-bit Codes for {target_layer_name}')
    plt.xlabel('4-bit Code Value [0-15]')
    plt.ylabel('Density')
    plt.xticks(range(16))
    plt.grid(axis='y', alpha=0.75)
    plt.show()
else:
    print("No linear layers found in the model.")

"""# check to verify equivalence"""

# Verify equivalence: 2-bit dequant vs. 4-bit-embedded dequant with adjusted params
# Requirements: pip install torch

import torch

torch.manual_seed(0)

def quantize_2bit_minmax(x):
    """
    Asymmetric min-max 2-bit quantization (unsigned codes 0..3).
    Returns q2 (int32), scale2 (float), x_min (float)
    """
    x_min = x.min()
    x_max = x.max()
    if x_max <= x_min:  # degenerate group
        q2 = torch.zeros_like(x, dtype=torch.int32)
        scale2 = torch.tensor(1.0, device=x.device)  # arbitrary nonzero to avoid div by zero
        return q2, scale2, x_min
    scale2 = (x_max - x_min) / 3.0  # 2^2 - 1 = 3
    q2 = torch.round((x - x_min) / scale2).clamp(0, 3).to(torch.int32)
    return q2, scale2, x_min

def dequant_2bit(q2, scale2, x_min):
    """Dequantize from native 2-bit params."""
    return q2.to(torch.float32) * scale2 + x_min

def embed_as_4bit_from_2bit(q2):
    """Left-shift by 2: {0,1,2,3} -> {0,4,8,12} in 4-bit code space."""
    return (q2 << 2).to(torch.int32)

def dequant_from_4bit_embedded(code4, scale2, x_min):
    """
    Dequantize using ONLY '4-bit-style' params for the embedded codes.
    In asymmetric min-max with min aligned, zp stays 0; only scale changes.
    scale4 = scale2 / 4
    """
    scale4 = scale2 / 4.0
    return code4.to(torch.float32) * scale4 + x_min  # zp4 = 0

def check_equivalence(shape=(4096,), num_trials=5, atol=1e-6, verbose=True):
    """
    Repeats the test on random tensors (simulating groups). Returns True if all trials pass.
    """
    ok = True
    for t in range(num_trials):
        # Create a random group with some scale/shift to avoid trivial ranges
        x = (torch.randn(*shape) * 0.37 + 0.15).to(torch.float32)

        # 2-bit quantize/dequant (reference)
        q2, scale2, x_min = quantize_2bit_minmax(x)
        x_hat_ref = dequant_2bit(q2, scale2, x_min)

        # Embed to 4-bit and dequant using adjusted 4-bit params
        code4 = embed_as_4bit_from_2bit(q2)
        x_hat_4 = dequant_from_4bit_embedded(code4, scale2, x_min)

        diff = (x_hat_ref - x_hat_4).abs()
        max_err = float(diff.max().item())
        passed = max_err <= atol

        if verbose:
            print(f"[trial {t+1}] max|Δ| = {max_err:.3e}  ->  {'OK' if passed else 'FAIL'}")

        ok = ok and passed
    return ok

if __name__ == "__main__":
    # Quick checks on a few shapes (including non-multiple-of-128 to mimic tail groups)
    all_ok = True
    for shape in [(128,), (257,), (4096,), (12345,)]:
        print(f"\nTesting shape={shape}")
        all_ok = check_equivalence(shape=shape, num_trials=3, atol=1e-6, verbose=True) and all_ok

    print("\nRESULT:", "ALL TESTS PASSED ✅" if all_ok else "SOME TESTS FAILED ❌")