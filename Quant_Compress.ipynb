{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G5UHIj81lDZc",
        "outputId": "0e786aea-a48b-40e0-9411-0b79a2f79b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy BEFORE Quantization:\n",
            "\n",
            "Starting per-layer quantization optimization...\n",
            "Initial Classification Loss Before Optimization: 0.352729\n",
            "\n",
            "Optimizing conv1.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1654, w_max=0.1546\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1656, w_max=0.1551\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1654, w_max=0.1554\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1653, w_max=0.1551\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1650, w_max=0.1549\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1650, w_max=0.1549\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1651, w_max=0.1549\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1651, w_max=0.1550\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1651, w_max=0.1550\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.1650, w_max=0.1550\n",
            "\n",
            "Optimizing layer1.0.conv1.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0759, w_max=0.0634\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0759, w_max=0.0633\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0759, w_max=0.0633\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0758, w_max=0.0633\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0758, w_max=0.0633\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0758, w_max=0.0633\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0758, w_max=0.0632\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0758, w_max=0.0632\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0758, w_max=0.0632\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0757, w_max=0.0632\n",
            "\n",
            "Optimizing layer1.0.conv2.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0434, w_max=0.0471\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0435, w_max=0.0469\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0435, w_max=0.0469\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0434, w_max=0.0469\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0435, w_max=0.0469\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0434, w_max=0.0469\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0434, w_max=0.0469\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0434, w_max=0.0469\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0434, w_max=0.0469\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0434, w_max=0.0469\n",
            "\n",
            "Optimizing layer1.1.conv1.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0440, w_max=0.0486\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0440, w_max=0.0486\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0486\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0486\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0487\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0487\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0486\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0486\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0486\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0441, w_max=0.0486\n",
            "\n",
            "Optimizing layer1.1.conv2.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0475, w_max=0.0400\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0475, w_max=0.0400\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0400\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0401\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0401\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0401\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0401\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0401\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0401\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0474, w_max=0.0401\n",
            "\n",
            "Optimizing layer2.0.conv1.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0411\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0341, w_max=0.0410\n",
            "\n",
            "Optimizing layer2.0.conv2.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0424\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0424\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0423\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0423\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0351, w_max=0.0423\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0423\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0423\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0423\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0423\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0350, w_max=0.0423\n",
            "\n",
            "Optimizing layer2.1.conv1.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0300, w_max=0.0354\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0354\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0353\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0353\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0353\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0354\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0354\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0354\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0354\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0301, w_max=0.0354\n",
            "\n",
            "Optimizing layer2.1.conv2.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0239, w_max=0.0380\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0239, w_max=0.0380\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0238, w_max=0.0381\n",
            "\n",
            "Optimizing layer3.0.conv1.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0215, w_max=0.0386\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 30: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 40: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 50: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 60: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 70: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 80: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "Iter 90: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0214, w_max=0.0387\n",
            "\n",
            "Optimizing layer3.0.conv2.weight...\n",
            "Iter 0: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0190, w_max=0.0257\n",
            "Iter 10: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0189, w_max=0.0257\n",
            "Iter 20: recon_loss=0.00000000, class_loss=0.3527, w_min=-0.0189, w_max=0.0258\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-67c4b282edd0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# ====== Run Optimization and Evaluation ======\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m#evaluate(model, test_loader)  # Before quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0moptimize_per_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;31m#evaluate(model, test_loader)  # After quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-67c4b282edd0>\u001b[0m in \u001b[0;36moptimize_per_layer\u001b[0;34m(model, test_loader, num_iterations, lr)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mprev_class_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mquantized_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mper_device_and_dtype_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0;31m# When any inputs are FakeScriptObject, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;31m# skip c++ dispatcher and dispatch in python through _get_dispatch of python_dispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m_must_dispatch_in_python\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m     return pytree.tree_any(\n\u001b[0m\u001b[1;32m   1002\u001b[0m         lambda obj: isinstance(\n\u001b[1;32m   1003\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_class_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFakeScriptObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_any\u001b[0;34m(pred, tree, is_leaf)\u001b[0m\n\u001b[1;32m   1228\u001b[0m ) -> bool:\n\u001b[1;32m   1229\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;31m# Recursively flatten the children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_pytrees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m def tree_iter(\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPyTree\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "#import timm\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 8\n",
        "FIXED_T = 100.5  # Fixed temperature for soft rounding\n",
        "LR = 0.001  # Learning rate\n",
        "NUM_ITERATIONS = 100  # Per-layer optimization iterations\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "#train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "print(\"\\nAccuracy BEFORE Quantization:\")\n",
        "\n",
        "# Function to evaluate model accuracy\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# ====== Hook for capturing activations ======\n",
        "temp_activations = {}\n",
        "\n",
        "def activation_hook(layer_name):\n",
        "    def hook(module, input, output):\n",
        "        temp_activations[layer_name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "# Register hooks for Conv2D layers\n",
        "for name, layer in model.named_modules():\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        layer.register_forward_hook(activation_hook(name))\n",
        "\n",
        "# ====== Differentiable Min-Max Quantization Module ======\n",
        "class MinMaxQuantization(nn.Module):\n",
        "    def __init__(self, weight, num_levels=2**NUM_BITS, fixed_T=FIXED_T):\n",
        "        super().__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.fixed_T = fixed_T\n",
        "\n",
        "        w_min_init = weight.min().detach()\n",
        "        w_max_init = weight.max().detach()\n",
        "        range_padding = 0.05 * (w_max_init - w_min_init)\n",
        "        self.w_min = nn.Parameter(w_min_init - range_padding)\n",
        "        self.w_max = nn.Parameter(w_max_init + range_padding)\n",
        "\n",
        "    def forward(self, w):\n",
        "        EPSILON = 1e-6\n",
        "        w_min_clamped = self.w_min.clamp(max=self.w_max.item() - EPSILON)\n",
        "        w_max_clamped = self.w_max.clamp(min=w_min_clamped.item() + EPSILON)\n",
        "        w_normalized = (w - w_min_clamped) / (w_max_clamped - w_min_clamped + EPSILON)\n",
        "\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "        distances = -torch.abs(w_normalized.unsqueeze(-1) - q_levels)\n",
        "        soft_weights = torch.softmax(distances * self.fixed_T, dim=-1)\n",
        "        w_quantized = (soft_weights * q_levels).sum(dim=-1)\n",
        "        w_dequantized = w_quantized * (w_max_clamped - w_min_clamped) + w_min_clamped\n",
        "        return w_dequantized\n",
        "\n",
        "# ====== Per-Layer Optimization with Activation Usage ======\n",
        "def optimize_per_layer(model, test_loader, num_iterations=NUM_ITERATIONS, lr=LR):\n",
        "    model.eval()\n",
        "    updated_state_dict = model.state_dict()\n",
        "    quantization_layers = {}\n",
        "\n",
        "    print(\"\\nStarting per-layer quantization optimization...\")\n",
        "\n",
        "    # Get one batch for activations and loss computation\n",
        "    data_iterator = iter(test_loader)\n",
        "    images, labels = next(data_iterator)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Capture activations\n",
        "    with torch.no_grad():\n",
        "        model(images)\n",
        "\n",
        "    # Initial accuracy reference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        initial_loss = nn.CrossEntropyLoss()(outputs, labels).item()\n",
        "    print(f\"Initial Classification Loss Before Optimization: {initial_loss:.6f}\")\n",
        "\n",
        "    # Optimize layer by layer\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name:\n",
        "            print(f\"\\nOptimizing {name}...\")\n",
        "            layer_name = name.replace(\".weight\", \"\")\n",
        "\n",
        "            if layer_name not in temp_activations:\n",
        "                print(f\"Skipping {layer_name}: No activation found.\")\n",
        "                continue\n",
        "\n",
        "            original_weight = param.clone().detach()\n",
        "            quant_layer = MinMaxQuantization(original_weight).to(device)\n",
        "            optimizer = optim.Adam(quant_layer.parameters(), lr=lr)\n",
        "            mse_loss_fn = nn.MSELoss()\n",
        "            activation_input = temp_activations[layer_name]  # Correct input for this layer\n",
        "\n",
        "            prev_class_loss = float('inf')\n",
        "            for iteration in range(num_iterations):\n",
        "                optimizer.zero_grad()\n",
        "                quantized_weight = quant_layer(original_weight)\n",
        "\n",
        "                quantized_output = nn.functional.conv2d(\n",
        "                    activation_input, quantized_weight, stride=param.shape[2], padding=param.shape[3]\n",
        "                )\n",
        "                original_output = nn.functional.conv2d(\n",
        "                    activation_input, original_weight, stride=param.shape[2], padding=param.shape[3]\n",
        "                )\n",
        "\n",
        "                recon_loss = mse_loss_fn(quantized_output, original_output)\n",
        "                class_loss = nn.CrossEntropyLoss()(model(images), labels)\n",
        "\n",
        "                if class_loss > prev_class_loss:\n",
        "                    print(f\"Early stop at iter {iteration}: class_loss increased.\")\n",
        "                    break\n",
        "                prev_class_loss = class_loss\n",
        "\n",
        "                total_loss = 0.1 * recon_loss + 0.9 * class_loss\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if iteration % 10 == 0:\n",
        "                    print(f\"Iter {iteration}: recon_loss={recon_loss.item():.8f}, \"\n",
        "                          f\"class_loss={class_loss.item():.4f}, \"\n",
        "                          f\"w_min={quant_layer.w_min.item():.4f}, \"\n",
        "                          f\"w_max={quant_layer.w_max.item():.4f}\")\n",
        "\n",
        "            updated_state_dict[name] = quant_layer(original_weight).detach()\n",
        "\n",
        "    model.load_state_dict(updated_state_dict)\n",
        "    print(\"\\nPer-layer optimization complete.\")\n",
        "\n",
        "# ====== Run Optimization and Evaluation ======\n",
        "#evaluate(model, test_loader)  # Before quantization\n",
        "optimize_per_layer(model, test_loader)\n",
        "#evaluate(model, test_loader)  # After quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1u2M_UFC18C"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTK0DkVIrJuZ"
      },
      "outputs": [],
      "source": [
        "class MinMaxQuantization(nn.Module):\n",
        "    def __init__(self, weight, num_levels=2**NUM_BITS, fixed_T=FIXED_T, entropy_budget=None):\n",
        "        super().__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.fixed_T = fixed_T\n",
        "        self.entropy_budget = entropy_budget\n",
        "\n",
        "        w_min_init = weight.min().detach()\n",
        "        w_max_init = weight.max().detach()\n",
        "        range_padding = 0.05 * (w_max_init - w_min_init)\n",
        "        self.w_min = nn.Parameter(w_min_init - range_padding)\n",
        "        self.w_max = nn.Parameter(w_max_init + range_padding)\n",
        "\n",
        "    def forward(self, w):\n",
        "        EPSILON = 1e-6\n",
        "        w_min_clamped = self.w_min.clamp(max=self.w_max.item() - EPSILON)\n",
        "        w_max_clamped = self.w_max.clamp(min=w_min_clamped.item() + EPSILON)\n",
        "        w_normalized = (w - w_min_clamped) / (w_max_clamped - w_min_clamped + EPSILON)\n",
        "\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "        distances = -torch.abs(w_normalized.unsqueeze(-1) - q_levels)\n",
        "        soft_weights = torch.softmax(distances * self.fixed_T, dim=-1)\n",
        "        w_quantized = (soft_weights * q_levels).sum(dim=-1)\n",
        "        w_dequantized = w_quantized * (w_max_clamped - w_min_clamped) + w_min_clamped\n",
        "\n",
        "        # Entropy (differentiable)\n",
        "        prob = soft_weights + EPSILON\n",
        "        entropy = -(prob * prob.log()).sum()\n",
        "\n",
        "        # Budget penalty\n",
        "        if self.entropy_budget is not None:\n",
        "            #budget_penalty = torch.relu(entropy - self.entropy_budget)\n",
        "            budget_penalty = (entropy / self.entropy_budget)\n",
        "        else:\n",
        "            budget_penalty = torch.tensor(0.0, device=w.device)\n",
        "\n",
        "        return w_dequantized, entropy, budget_penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSGL6jClrNxN"
      },
      "outputs": [],
      "source": [
        "def optimize_per_layer(model, test_loader, num_iterations=100, lr=1e-3, CR_target=10):\n",
        "    model.eval()\n",
        "    updated_state_dict = model.state_dict()\n",
        "    print(\"\\nStarting per-layer quantization optimization...\")\n",
        "\n",
        "    # Use a smaller batch for activation capture + class loss\n",
        "    data_iterator = iter(test_loader)\n",
        "    images, labels = next(data_iterator)\n",
        "    images, labels = images[:2].to(device), labels[:2].to(device)\n",
        "\n",
        "    # Run one forward pass to trigger activation hooks\n",
        "    with torch.no_grad():\n",
        "        model(images)\n",
        "\n",
        "    # Freeze all weights to save memory\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    total_params = sum(p.numel() for n, p in model.named_parameters() if \"conv\" in n and \"weight\" in n)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name:\n",
        "            print(f\"\\n🔧 Optimizing {name}...\")\n",
        "            layer_name = name.replace(\".weight\", \"\")\n",
        "\n",
        "            if layer_name not in temp_activations or temp_activations[layer_name] is None:\n",
        "                print(f\"⏭️ Skipping {layer_name}: No activation found.\")\n",
        "                continue\n",
        "\n",
        "            original_weight = param.detach().clone()\n",
        "            num_weights = original_weight.numel()\n",
        "            entropy_budget = (32 * num_weights) / CR_target\n",
        "\n",
        "            # Create quantization module\n",
        "            quant_layer = MinMaxQuantization(original_weight, entropy_budget=entropy_budget).to(device)\n",
        "            optimizer = torch.optim.Adam(quant_layer.parameters(), lr=lr)\n",
        "            mse_loss_fn = nn.MSELoss()\n",
        "\n",
        "            activation_input = temp_activations[layer_name].detach().clone().to(device)\n",
        "            del temp_activations[layer_name]  # Release after use\n",
        "\n",
        "            prev_entropy = None\n",
        "\n",
        "            for iteration in range(num_iterations):\n",
        "                optimizer.zero_grad()\n",
        "                quantized_weight, entropy, budget_penalty = quant_layer(original_weight)\n",
        "\n",
        "                # Forward through current conv layer\n",
        "                quantized_output = nn.functional.conv2d(\n",
        "                    activation_input, quantized_weight,\n",
        "                    stride=param.shape[2], padding=param.shape[3]\n",
        "                )\n",
        "\n",
        "                original_output = nn.functional.conv2d(\n",
        "                    activation_input, original_weight,\n",
        "                    stride=param.shape[2], padding=param.shape[3]\n",
        "                )\n",
        "\n",
        "                recon_loss = mse_loss_fn(quantized_output, original_output)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    class_loss = nn.CrossEntropyLoss()(model(images), labels)\n",
        "\n",
        "                total_loss = (\n",
        "                    0.1 * recon_loss +\n",
        "                    0.9 * class_loss +\n",
        "                    0.001 * budget_penalty  # soft constraint\n",
        "                )\n",
        "\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if iteration % 10 == 0:\n",
        "                    delta_entropy = (\n",
        "                        entropy.item() - prev_entropy if prev_entropy is not None else 0.0\n",
        "                    )\n",
        "                    print(f\"Iter {iteration:03d}: recon={recon_loss.item():.6f}, \"\n",
        "                          f\"class={class_loss.item():.4f}, \"\n",
        "                          f\"entropy={entropy.item():.2f}, \"\n",
        "                          f\"Δentropy={delta_entropy:.2f}, \"\n",
        "                          f\"budget={entropy_budget:.2f}, \"\n",
        "                          f\"penalty={budget_penalty.item():.4f}, \"\n",
        "                          f\"w_min={quant_layer.w_min.item():.4f}, \"\n",
        "                          f\"w_max={quant_layer.w_max.item():.4f}\")\n",
        "                    prev_entropy = entropy.item()\n",
        "\n",
        "            # Overwrite quantized weights in-place\n",
        "            with torch.no_grad():\n",
        "                quantized_w, _, _ = quant_layer(original_weight)\n",
        "                param.copy_(quantized_w.to(param.device))\n",
        "\n",
        "            # Cleanup to prevent memory buildup\n",
        "            del quant_layer, optimizer, activation_input\n",
        "            del quantized_weight, entropy, budget_penalty\n",
        "            del quantized_output, original_output\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n✅ Per-layer optimization complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9iCDNQCrRv8",
        "outputId": "2305a11b-b397-4952-f136-0c076cd496f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 86.53%\n",
            "\n",
            "Starting per-layer quantization optimization...\n",
            "\n",
            "🔧 Optimizing conv1.weight...\n",
            "Iter 000: recon=0.000000, class=0.0103, entropy=4539.62, Δentropy=0.00, budget=5529.60, penalty=0.8210, w_min=-0.1649, w_max=0.1551\n",
            "Iter 010: recon=0.000000, class=0.0103, entropy=4538.21, Δentropy=-1.41, budget=5529.60, penalty=0.8207, w_min=-0.1631, w_max=0.1570\n",
            "Iter 020: recon=0.000000, class=0.0103, entropy=4538.19, Δentropy=-0.01, budget=5529.60, penalty=0.8207, w_min=-0.1630, w_max=0.1567\n",
            "Iter 030: recon=0.000000, class=0.0103, entropy=4539.19, Δentropy=1.00, budget=5529.60, penalty=0.8209, w_min=-0.1630, w_max=0.1566\n",
            "Iter 040: recon=0.000000, class=0.0103, entropy=4538.41, Δentropy=-0.78, budget=5529.60, penalty=0.8207, w_min=-0.1630, w_max=0.1566\n",
            "Iter 050: recon=0.000000, class=0.0103, entropy=4538.26, Δentropy=-0.14, budget=5529.60, penalty=0.8207, w_min=-0.1629, w_max=0.1568\n",
            "Iter 060: recon=0.000000, class=0.0103, entropy=4540.37, Δentropy=2.11, budget=5529.60, penalty=0.8211, w_min=-0.1628, w_max=0.1569\n",
            "Iter 070: recon=0.000000, class=0.0103, entropy=4538.77, Δentropy=-1.60, budget=5529.60, penalty=0.8208, w_min=-0.1628, w_max=0.1569\n",
            "Iter 080: recon=0.000000, class=0.0103, entropy=4539.97, Δentropy=1.19, budget=5529.60, penalty=0.8210, w_min=-0.1630, w_max=0.1564\n",
            "Iter 090: recon=0.000000, class=0.0103, entropy=4538.91, Δentropy=-1.05, budget=5529.60, penalty=0.8208, w_min=-0.1629, w_max=0.1563\n",
            "\n",
            "🔧 Optimizing layer1.0.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=0.0103, entropy=96807.68, Δentropy=0.00, budget=117964.80, penalty=0.8206, w_min=-0.0770, w_max=0.0624\n",
            "Iter 010: recon=0.000000, class=0.0103, entropy=96855.97, Δentropy=48.29, budget=117964.80, penalty=0.8211, w_min=-0.0786, w_max=0.0609\n",
            "Iter 020: recon=0.000000, class=0.0103, entropy=96831.33, Δentropy=-24.64, budget=117964.80, penalty=0.8208, w_min=-0.0794, w_max=0.0602\n",
            "Iter 030: recon=0.000000, class=0.0103, entropy=96817.38, Δentropy=-13.95, budget=117964.80, penalty=0.8207, w_min=-0.0794, w_max=0.0602\n",
            "Iter 040: recon=0.000000, class=0.0103, entropy=96811.43, Δentropy=-5.95, budget=117964.80, penalty=0.8207, w_min=-0.0802, w_max=0.0595\n",
            "Iter 050: recon=0.000000, class=0.0103, entropy=96880.50, Δentropy=69.07, budget=117964.80, penalty=0.8213, w_min=-0.0827, w_max=0.0570\n",
            "Iter 060: recon=0.000000, class=0.0103, entropy=96878.30, Δentropy=-2.20, budget=117964.80, penalty=0.8212, w_min=-0.0845, w_max=0.0550\n",
            "Iter 070: recon=0.000000, class=0.0103, entropy=96878.84, Δentropy=0.54, budget=117964.80, penalty=0.8213, w_min=-0.0853, w_max=0.0542\n",
            "Iter 080: recon=0.000000, class=0.0103, entropy=96884.00, Δentropy=5.16, budget=117964.80, penalty=0.8213, w_min=-0.0866, w_max=0.0527\n",
            "Iter 090: recon=0.000000, class=0.0103, entropy=96821.53, Δentropy=-62.47, budget=117964.80, penalty=0.8208, w_min=-0.0869, w_max=0.0523\n",
            "\n",
            "🔧 Optimizing layer1.0.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0103, entropy=96856.46, Δentropy=0.00, budget=117964.80, penalty=0.8211, w_min=-0.0423, w_max=0.0481\n",
            "Iter 010: recon=0.000000, class=0.0103, entropy=96854.39, Δentropy=-2.07, budget=117964.80, penalty=0.8210, w_min=-0.0437, w_max=0.0485\n",
            "Iter 020: recon=0.000000, class=0.0103, entropy=96852.89, Δentropy=-1.50, budget=117964.80, penalty=0.8210, w_min=-0.0412, w_max=0.0510\n",
            "Iter 030: recon=0.000000, class=0.0103, entropy=96855.23, Δentropy=2.34, budget=117964.80, penalty=0.8211, w_min=-0.0395, w_max=0.0534\n",
            "Iter 040: recon=0.000000, class=0.0103, entropy=96853.24, Δentropy=-1.99, budget=117964.80, penalty=0.8210, w_min=-0.0378, w_max=0.0556\n",
            "Iter 050: recon=0.000000, class=0.0103, entropy=96850.22, Δentropy=-3.02, budget=117964.80, penalty=0.8210, w_min=-0.0360, w_max=0.0572\n",
            "Iter 060: recon=0.000000, class=0.0103, entropy=96849.66, Δentropy=-0.55, budget=117964.80, penalty=0.8210, w_min=-0.0351, w_max=0.0580\n",
            "Iter 070: recon=0.000000, class=0.0103, entropy=96849.09, Δentropy=-0.57, budget=117964.80, penalty=0.8210, w_min=-0.0339, w_max=0.0586\n",
            "Iter 080: recon=0.000000, class=0.0103, entropy=96830.17, Δentropy=-18.92, budget=117964.80, penalty=0.8208, w_min=-0.0291, w_max=0.0617\n",
            "Iter 090: recon=0.000000, class=0.0103, entropy=96753.59, Δentropy=-76.58, budget=117964.80, penalty=0.8202, w_min=-0.0226, w_max=0.0651\n",
            "\n",
            "🔧 Optimizing layer1.1.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=0.0088, entropy=96854.85, Δentropy=0.00, budget=117964.80, penalty=0.8210, w_min=-0.0451, w_max=0.0475\n",
            "Iter 010: recon=0.000000, class=0.0088, entropy=96855.98, Δentropy=1.13, budget=117964.80, penalty=0.8211, w_min=-0.0480, w_max=0.0434\n",
            "Iter 020: recon=0.000000, class=0.0088, entropy=96855.08, Δentropy=-0.91, budget=117964.80, penalty=0.8211, w_min=-0.0505, w_max=0.0414\n",
            "Iter 030: recon=0.000000, class=0.0088, entropy=96855.81, Δentropy=0.73, budget=117964.80, penalty=0.8211, w_min=-0.0517, w_max=0.0409\n",
            "Iter 040: recon=0.000000, class=0.0088, entropy=96855.02, Δentropy=-0.80, budget=117964.80, penalty=0.8211, w_min=-0.0525, w_max=0.0393\n",
            "Iter 050: recon=0.000000, class=0.0088, entropy=96855.48, Δentropy=0.47, budget=117964.80, penalty=0.8211, w_min=-0.0519, w_max=0.0395\n",
            "Iter 060: recon=0.000000, class=0.0088, entropy=96854.50, Δentropy=-0.98, budget=117964.80, penalty=0.8210, w_min=-0.0507, w_max=0.0412\n",
            "Iter 070: recon=0.000000, class=0.0088, entropy=96855.20, Δentropy=0.70, budget=117964.80, penalty=0.8211, w_min=-0.0496, w_max=0.0438\n",
            "Iter 080: recon=0.000000, class=0.0088, entropy=96853.58, Δentropy=-1.62, budget=117964.80, penalty=0.8210, w_min=-0.0506, w_max=0.0442\n",
            "Iter 090: recon=0.000000, class=0.0088, entropy=96854.39, Δentropy=0.81, budget=117964.80, penalty=0.8210, w_min=-0.0465, w_max=0.0480\n",
            "\n",
            "🔧 Optimizing layer1.1.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0088, entropy=96855.52, Δentropy=0.00, budget=117964.80, penalty=0.8211, w_min=-0.0465, w_max=0.0410\n",
            "Iter 010: recon=0.000000, class=0.0088, entropy=96854.75, Δentropy=-0.77, budget=117964.80, penalty=0.8210, w_min=-0.0422, w_max=0.0442\n",
            "Iter 020: recon=0.000000, class=0.0088, entropy=96855.34, Δentropy=0.59, budget=117964.80, penalty=0.8211, w_min=-0.0387, w_max=0.0468\n",
            "Iter 030: recon=0.000000, class=0.0088, entropy=96853.39, Δentropy=-1.95, budget=117964.80, penalty=0.8210, w_min=-0.0376, w_max=0.0477\n",
            "Iter 040: recon=0.000000, class=0.0088, entropy=96854.34, Δentropy=0.95, budget=117964.80, penalty=0.8210, w_min=-0.0367, w_max=0.0482\n",
            "Iter 050: recon=0.000000, class=0.0088, entropy=96852.22, Δentropy=-2.12, budget=117964.80, penalty=0.8210, w_min=-0.0310, w_max=0.0522\n",
            "Iter 060: recon=0.000000, class=0.0088, entropy=96829.72, Δentropy=-22.50, budget=117964.80, penalty=0.8208, w_min=-0.0232, w_max=0.0576\n",
            "Iter 070: recon=0.000000, class=0.0088, entropy=96165.31, Δentropy=-664.41, budget=117964.80, penalty=0.8152, w_min=-0.0114, w_max=0.0611\n",
            "Iter 080: recon=0.000187, class=0.0088, entropy=75862.44, Δentropy=-20302.88, budget=117964.80, penalty=0.6431, w_min=0.0053, w_max=0.0714\n",
            "Iter 090: recon=0.000535, class=0.0088, entropy=72851.72, Δentropy=-3010.72, budget=117964.80, penalty=0.6176, w_min=0.0071, w_max=0.0768\n",
            "\n",
            "🔧 Optimizing layer2.0.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=3.6936, entropy=193714.28, Δentropy=0.00, budget=235929.60, penalty=0.8211, w_min=-0.0330, w_max=0.0421\n",
            "Iter 010: recon=0.000000, class=3.6936, entropy=193712.81, Δentropy=-1.47, budget=235929.60, penalty=0.8211, w_min=-0.0313, w_max=0.0449\n",
            "Iter 020: recon=0.000000, class=3.6936, entropy=193712.77, Δentropy=-0.05, budget=235929.60, penalty=0.8211, w_min=-0.0312, w_max=0.0452\n",
            "Iter 030: recon=0.000000, class=3.6936, entropy=193711.12, Δentropy=-1.64, budget=235929.60, penalty=0.8211, w_min=-0.0295, w_max=0.0466\n",
            "Iter 040: recon=0.000000, class=3.6936, entropy=193710.30, Δentropy=-0.83, budget=235929.60, penalty=0.8211, w_min=-0.0288, w_max=0.0475\n",
            "Iter 050: recon=0.000000, class=3.6936, entropy=193711.06, Δentropy=0.77, budget=235929.60, penalty=0.8211, w_min=-0.0287, w_max=0.0473\n",
            "Iter 060: recon=0.000000, class=3.6936, entropy=193708.81, Δentropy=-2.25, budget=235929.60, penalty=0.8210, w_min=-0.0267, w_max=0.0495\n",
            "Iter 070: recon=0.000000, class=3.6936, entropy=193709.16, Δentropy=0.34, budget=235929.60, penalty=0.8210, w_min=-0.0276, w_max=0.0488\n",
            "Iter 080: recon=0.000000, class=3.6936, entropy=193709.00, Δentropy=-0.16, budget=235929.60, penalty=0.8210, w_min=-0.0278, w_max=0.0490\n",
            "Iter 090: recon=0.000000, class=3.6936, entropy=193705.56, Δentropy=-3.44, budget=235929.60, penalty=0.8210, w_min=-0.0249, w_max=0.0520\n",
            "\n",
            "🔧 Optimizing layer2.0.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=3.6918, entropy=387423.53, Δentropy=0.00, budget=471859.20, penalty=0.8211, w_min=-0.0340, w_max=0.0434\n",
            "Iter 010: recon=0.000000, class=3.6918, entropy=387426.19, Δentropy=2.66, budget=471859.20, penalty=0.8211, w_min=-0.0338, w_max=0.0439\n",
            "Iter 020: recon=0.000000, class=3.6918, entropy=387425.53, Δentropy=-0.66, budget=471859.20, penalty=0.8211, w_min=-0.0338, w_max=0.0437\n",
            "Iter 030: recon=0.000000, class=3.6918, entropy=387424.50, Δentropy=-1.03, budget=471859.20, penalty=0.8211, w_min=-0.0311, w_max=0.0455\n",
            "Iter 040: recon=0.000000, class=3.6918, entropy=387421.59, Δentropy=-2.91, budget=471859.20, penalty=0.8211, w_min=-0.0307, w_max=0.0458\n",
            "Iter 050: recon=0.000000, class=3.6918, entropy=387422.94, Δentropy=1.34, budget=471859.20, penalty=0.8211, w_min=-0.0309, w_max=0.0462\n",
            "Iter 060: recon=0.000000, class=3.6918, entropy=387422.16, Δentropy=-0.78, budget=471859.20, penalty=0.8211, w_min=-0.0308, w_max=0.0465\n",
            "Iter 070: recon=0.000000, class=3.6918, entropy=387420.81, Δentropy=-1.34, budget=471859.20, penalty=0.8211, w_min=-0.0316, w_max=0.0460\n",
            "Iter 080: recon=0.000000, class=3.6918, entropy=387425.72, Δentropy=4.91, budget=471859.20, penalty=0.8211, w_min=-0.0319, w_max=0.0463\n",
            "Iter 090: recon=0.000000, class=3.6918, entropy=387422.97, Δentropy=-2.75, budget=471859.20, penalty=0.8211, w_min=-0.0343, w_max=0.0438\n",
            "\n",
            "🔧 Optimizing layer2.1.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=3.6922, entropy=387426.09, Δentropy=0.00, budget=471859.20, penalty=0.8211, w_min=-0.0290, w_max=0.0364\n",
            "Iter 010: recon=0.000000, class=3.6922, entropy=387422.22, Δentropy=-3.88, budget=471859.20, penalty=0.8211, w_min=-0.0301, w_max=0.0345\n",
            "Iter 020: recon=0.000000, class=3.6922, entropy=387418.50, Δentropy=-3.72, budget=471859.20, penalty=0.8210, w_min=-0.0332, w_max=0.0314\n",
            "Iter 030: recon=0.000000, class=3.6922, entropy=387412.88, Δentropy=-5.62, budget=471859.20, penalty=0.8210, w_min=-0.0343, w_max=0.0302\n",
            "Iter 040: recon=0.000000, class=3.6922, entropy=387415.12, Δentropy=2.25, budget=471859.20, penalty=0.8210, w_min=-0.0342, w_max=0.0294\n",
            "Iter 050: recon=0.000000, class=3.6922, entropy=387406.94, Δentropy=-8.19, budget=471859.20, penalty=0.8210, w_min=-0.0340, w_max=0.0286\n",
            "Iter 060: recon=0.000000, class=3.6922, entropy=387389.19, Δentropy=-17.75, budget=471859.20, penalty=0.8210, w_min=-0.0356, w_max=0.0252\n",
            "Iter 070: recon=0.000000, class=3.6922, entropy=387317.62, Δentropy=-71.56, budget=471859.20, penalty=0.8208, w_min=-0.0369, w_max=0.0213\n",
            "Iter 080: recon=0.000001, class=3.6922, entropy=382793.38, Δentropy=-4524.25, budget=471859.20, penalty=0.8112, w_min=-0.0389, w_max=0.0084\n",
            "Iter 090: recon=0.000926, class=3.6922, entropy=310574.88, Δentropy=-72218.50, budget=471859.20, penalty=0.6582, w_min=-0.0464, w_max=-0.0031\n",
            "\n",
            "🔧 Optimizing layer2.1.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=4.9599, entropy=387423.78, Δentropy=0.00, budget=471859.20, penalty=0.8211, w_min=-0.0229, w_max=0.0390\n",
            "Iter 010: recon=0.000000, class=4.9599, entropy=387424.81, Δentropy=1.03, budget=471859.20, penalty=0.8211, w_min=-0.0226, w_max=0.0402\n",
            "Iter 020: recon=0.000000, class=4.9599, entropy=387425.12, Δentropy=0.31, budget=471859.20, penalty=0.8211, w_min=-0.0253, w_max=0.0385\n",
            "Iter 030: recon=0.000000, class=4.9599, entropy=387422.66, Δentropy=-2.47, budget=471859.20, penalty=0.8211, w_min=-0.0292, w_max=0.0347\n",
            "Iter 040: recon=0.000000, class=4.9599, entropy=387424.22, Δentropy=1.56, budget=471859.20, penalty=0.8211, w_min=-0.0323, w_max=0.0318\n",
            "Iter 050: recon=0.000000, class=4.9599, entropy=387421.38, Δentropy=-2.84, budget=471859.20, penalty=0.8211, w_min=-0.0358, w_max=0.0281\n",
            "Iter 060: recon=0.000000, class=4.9599, entropy=387422.25, Δentropy=0.88, budget=471859.20, penalty=0.8211, w_min=-0.0338, w_max=0.0304\n",
            "Iter 070: recon=0.000000, class=4.9599, entropy=387422.44, Δentropy=0.19, budget=471859.20, penalty=0.8211, w_min=-0.0330, w_max=0.0313\n",
            "Iter 080: recon=0.000000, class=4.9599, entropy=387423.56, Δentropy=1.12, budget=471859.20, penalty=0.8211, w_min=-0.0331, w_max=0.0312\n",
            "Iter 090: recon=0.000000, class=4.9599, entropy=387423.56, Δentropy=0.00, budget=471859.20, penalty=0.8211, w_min=-0.0331, w_max=0.0313\n",
            "\n",
            "🔧 Optimizing layer3.0.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=4.9599, entropy=774848.50, Δentropy=0.00, budget=943718.40, penalty=0.8211, w_min=-0.0225, w_max=0.0376\n",
            "Iter 010: recon=0.000000, class=4.9599, entropy=774847.12, Δentropy=-1.38, budget=943718.40, penalty=0.8211, w_min=-0.0229, w_max=0.0370\n",
            "Iter 020: recon=0.000000, class=4.9599, entropy=774846.81, Δentropy=-0.31, budget=943718.40, penalty=0.8211, w_min=-0.0244, w_max=0.0352\n",
            "Iter 030: recon=0.000000, class=4.9599, entropy=774847.69, Δentropy=0.88, budget=943718.40, penalty=0.8211, w_min=-0.0244, w_max=0.0351\n",
            "Iter 040: recon=0.000000, class=4.9599, entropy=774848.38, Δentropy=0.69, budget=943718.40, penalty=0.8211, w_min=-0.0240, w_max=0.0342\n",
            "Iter 050: recon=0.000000, class=4.9599, entropy=774845.31, Δentropy=-3.06, budget=943718.40, penalty=0.8211, w_min=-0.0240, w_max=0.0335\n",
            "Iter 060: recon=0.000000, class=4.9599, entropy=774847.25, Δentropy=1.94, budget=943718.40, penalty=0.8211, w_min=-0.0226, w_max=0.0348\n",
            "Iter 070: recon=0.000000, class=4.9599, entropy=774849.25, Δentropy=2.00, budget=943718.40, penalty=0.8211, w_min=-0.0250, w_max=0.0334\n",
            "Iter 080: recon=0.000000, class=4.9599, entropy=774849.88, Δentropy=0.62, budget=943718.40, penalty=0.8211, w_min=-0.0260, w_max=0.0326\n",
            "Iter 090: recon=0.000000, class=4.9599, entropy=774841.50, Δentropy=-8.38, budget=943718.40, penalty=0.8211, w_min=-0.0282, w_max=0.0301\n",
            "\n",
            "🔧 Optimizing layer3.0.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=4.9592, entropy=1549688.00, Δentropy=0.00, budget=1887436.80, penalty=0.8211, w_min=-0.0200, w_max=0.0247\n",
            "Iter 010: recon=0.000000, class=4.9592, entropy=1549692.50, Δentropy=4.50, budget=1887436.80, penalty=0.8211, w_min=-0.0174, w_max=0.0283\n",
            "Iter 020: recon=0.000000, class=4.9592, entropy=1549691.00, Δentropy=-1.50, budget=1887436.80, penalty=0.8211, w_min=-0.0169, w_max=0.0303\n",
            "Iter 030: recon=0.000000, class=4.9592, entropy=1549692.00, Δentropy=1.00, budget=1887436.80, penalty=0.8211, w_min=-0.0146, w_max=0.0323\n",
            "Iter 040: recon=0.000000, class=4.9592, entropy=1549685.75, Δentropy=-6.25, budget=1887436.80, penalty=0.8211, w_min=-0.0133, w_max=0.0335\n",
            "Iter 050: recon=0.000000, class=4.9592, entropy=1549678.75, Δentropy=-7.00, budget=1887436.80, penalty=0.8210, w_min=-0.0125, w_max=0.0335\n",
            "Iter 060: recon=0.000000, class=4.9592, entropy=1549637.00, Δentropy=-41.75, budget=1887436.80, penalty=0.8210, w_min=-0.0110, w_max=0.0334\n",
            "Iter 070: recon=0.000000, class=4.9592, entropy=1549409.62, Δentropy=-227.38, budget=1887436.80, penalty=0.8209, w_min=-0.0088, w_max=0.0320\n",
            "Iter 080: recon=0.000264, class=4.9592, entropy=1180889.25, Δentropy=-368520.38, budget=1887436.80, penalty=0.6257, w_min=0.0039, w_max=0.0405\n",
            "Iter 090: recon=0.000120, class=4.9592, entropy=1218008.75, Δentropy=37119.50, budget=1887436.80, penalty=0.6453, w_min=0.0008, w_max=0.0429\n",
            "\n",
            "🔧 Optimizing layer3.1.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=14.8879, entropy=1549701.50, Δentropy=0.00, budget=1887436.80, penalty=0.8211, w_min=-0.0189, w_max=0.0208\n",
            "Iter 010: recon=0.000000, class=14.8879, entropy=1549692.25, Δentropy=-9.25, budget=1887436.80, penalty=0.8211, w_min=-0.0173, w_max=0.0226\n",
            "Iter 020: recon=0.000000, class=14.8879, entropy=1549706.00, Δentropy=13.75, budget=1887436.80, penalty=0.8211, w_min=-0.0173, w_max=0.0229\n",
            "Iter 030: recon=0.000000, class=14.8879, entropy=1549689.75, Δentropy=-16.25, budget=1887436.80, penalty=0.8211, w_min=-0.0179, w_max=0.0229\n",
            "Iter 040: recon=0.000000, class=14.8879, entropy=1549697.50, Δentropy=7.75, budget=1887436.80, penalty=0.8211, w_min=-0.0162, w_max=0.0246\n",
            "Iter 050: recon=0.000000, class=14.8879, entropy=1549697.25, Δentropy=-0.25, budget=1887436.80, penalty=0.8211, w_min=-0.0159, w_max=0.0249\n",
            "Iter 060: recon=0.000000, class=14.8879, entropy=1549697.25, Δentropy=0.00, budget=1887436.80, penalty=0.8211, w_min=-0.0162, w_max=0.0248\n",
            "Iter 070: recon=0.000000, class=14.8879, entropy=1549697.75, Δentropy=0.50, budget=1887436.80, penalty=0.8211, w_min=-0.0157, w_max=0.0253\n",
            "Iter 080: recon=0.000000, class=14.8879, entropy=1549695.25, Δentropy=-2.50, budget=1887436.80, penalty=0.8211, w_min=-0.0139, w_max=0.0265\n",
            "Iter 090: recon=0.000000, class=14.8879, entropy=1549322.62, Δentropy=-372.62, budget=1887436.80, penalty=0.8209, w_min=-0.0069, w_max=0.0308\n",
            "\n",
            "🔧 Optimizing layer3.1.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=19.8686, entropy=1549695.75, Δentropy=0.00, budget=1887436.80, penalty=0.8211, w_min=-0.0109, w_max=0.0116\n",
            "Iter 010: recon=0.000000, class=19.8686, entropy=1549698.62, Δentropy=2.88, budget=1887436.80, penalty=0.8211, w_min=-0.0129, w_max=0.0098\n",
            "Iter 020: recon=0.000000, class=19.8686, entropy=1549688.75, Δentropy=-9.88, budget=1887436.80, penalty=0.8211, w_min=-0.0143, w_max=0.0083\n",
            "Iter 030: recon=0.000000, class=19.8686, entropy=1549681.00, Δentropy=-7.75, budget=1887436.80, penalty=0.8211, w_min=-0.0140, w_max=0.0083\n",
            "Iter 040: recon=0.000000, class=19.8686, entropy=1549280.12, Δentropy=-400.88, budget=1887436.80, penalty=0.8208, w_min=-0.0164, w_max=0.0039\n",
            "Iter 050: recon=0.000321, class=19.8686, entropy=1145489.62, Δentropy=-403790.50, budget=1887436.80, penalty=0.6069, w_min=-0.0241, w_max=-0.0049\n",
            "Iter 060: recon=0.000337, class=19.8686, entropy=1145393.38, Δentropy=-96.25, budget=1887436.80, penalty=0.6069, w_min=-0.0259, w_max=-0.0040\n",
            "Iter 070: recon=0.000032, class=19.8686, entropy=1178280.50, Δentropy=32887.12, budget=1887436.80, penalty=0.6243, w_min=-0.0259, w_max=-0.0016\n",
            "Iter 080: recon=0.000157, class=19.8686, entropy=1148400.25, Δentropy=-29880.25, budget=1887436.80, penalty=0.6084, w_min=-0.0280, w_max=-0.0029\n",
            "Iter 090: recon=0.000056, class=19.8686, entropy=1163074.00, Δentropy=14673.75, budget=1887436.80, penalty=0.6162, w_min=-0.0281, w_max=-0.0018\n",
            "\n",
            "🔧 Optimizing layer4.0.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=2.7678, entropy=3099387.00, Δentropy=0.00, budget=3774873.60, penalty=0.8211, w_min=-0.0047, w_max=0.0053\n",
            "Iter 010: recon=0.000000, class=2.7678, entropy=3099143.00, Δentropy=-244.00, budget=3774873.60, penalty=0.8210, w_min=-0.0077, w_max=0.0020\n",
            "Iter 020: recon=0.000715, class=2.7678, entropy=2294354.00, Δentropy=-804789.00, budget=3774873.60, penalty=0.6078, w_min=-0.0115, w_max=-0.0012\n",
            "Iter 030: recon=0.000466, class=2.7678, entropy=2300578.00, Δentropy=6224.00, budget=3774873.60, penalty=0.6094, w_min=-0.0136, w_max=-0.0012\n",
            "Iter 040: recon=0.000239, class=2.7678, entropy=2321634.00, Δentropy=21056.00, budget=3774873.60, penalty=0.6150, w_min=-0.0150, w_max=-0.0009\n",
            "Iter 050: recon=0.000153, class=2.7678, entropy=2346153.00, Δentropy=24519.00, budget=3774873.60, penalty=0.6215, w_min=-0.0161, w_max=-0.0007\n",
            "Iter 060: recon=0.000141, class=2.7678, entropy=2351538.75, Δentropy=5385.75, budget=3774873.60, penalty=0.6229, w_min=-0.0169, w_max=-0.0006\n",
            "Iter 070: recon=0.000147, class=2.7678, entropy=2348674.00, Δentropy=-2864.75, budget=3774873.60, penalty=0.6222, w_min=-0.0176, w_max=-0.0006\n",
            "Iter 080: recon=0.000155, class=2.7678, entropy=2345219.50, Δentropy=-3454.50, budget=3774873.60, penalty=0.6213, w_min=-0.0182, w_max=-0.0006\n",
            "Iter 090: recon=0.000161, class=2.7678, entropy=2342579.50, Δentropy=-2640.00, budget=3774873.60, penalty=0.6206, w_min=-0.0187, w_max=-0.0006\n",
            "\n",
            "🔧 Optimizing layer4.0.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=2.7678, entropy=6198767.50, Δentropy=0.00, budget=7549747.20, penalty=0.8211, w_min=-0.0023, w_max=0.0104\n",
            "Iter 010: recon=0.000639, class=2.7678, entropy=4587201.00, Δentropy=-1611566.50, budget=7549747.20, penalty=0.6076, w_min=0.0024, w_max=0.0161\n",
            "Iter 020: recon=0.000065, class=2.7678, entropy=4692986.50, Δentropy=105785.50, budget=7549747.20, penalty=0.6216, w_min=0.0004, w_max=0.0155\n",
            "Iter 030: recon=0.000196, class=2.7678, entropy=4617467.00, Δentropy=-75519.50, budget=7549747.20, penalty=0.6116, w_min=0.0012, w_max=0.0172\n",
            "Iter 040: recon=0.000067, class=2.7678, entropy=4689961.00, Δentropy=72494.00, budget=7549747.20, penalty=0.6212, w_min=0.0005, w_max=0.0174\n",
            "Iter 050: recon=0.000136, class=2.7678, entropy=4635485.00, Δentropy=-54476.00, budget=7549747.20, penalty=0.6140, w_min=0.0009, w_max=0.0185\n",
            "Iter 060: recon=0.000079, class=2.7678, entropy=4674307.00, Δentropy=38822.00, budget=7549747.20, penalty=0.6191, w_min=0.0007, w_max=0.0190\n",
            "Iter 070: recon=0.000107, class=2.7678, entropy=4650702.50, Δentropy=-23604.50, budget=7549747.20, penalty=0.6160, w_min=0.0008, w_max=0.0198\n",
            "Iter 080: recon=0.000099, class=2.7678, entropy=4655780.00, Δentropy=5077.50, budget=7549747.20, penalty=0.6167, w_min=0.0008, w_max=0.0205\n",
            "Iter 090: recon=0.000093, class=2.7678, entropy=4659942.50, Δentropy=4162.50, budget=7549747.20, penalty=0.6172, w_min=0.0007, w_max=0.0211\n",
            "\n",
            "🔧 Optimizing layer4.1.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=2.6121, entropy=6198791.50, Δentropy=0.00, budget=7549747.20, penalty=0.8211, w_min=-0.0008, w_max=0.0060\n",
            "Iter 010: recon=0.001269, class=2.6121, entropy=4597205.00, Δentropy=-1601586.50, budget=7549747.20, penalty=0.6089, w_min=0.0005, w_max=0.0082\n",
            "Iter 020: recon=0.000144, class=2.6121, entropy=4741674.50, Δentropy=144469.50, budget=7549747.20, penalty=0.6281, w_min=0.0000, w_max=0.0087\n",
            "Iter 030: recon=0.000069, class=2.6121, entropy=4869340.00, Δentropy=127665.50, budget=7549747.20, penalty=0.6450, w_min=0.0002, w_max=0.0089\n",
            "Iter 040: recon=0.000236, class=2.6121, entropy=4685665.00, Δentropy=-183675.00, budget=7549747.20, penalty=0.6206, w_min=0.0003, w_max=0.0088\n",
            "Iter 050: recon=0.000185, class=2.6121, entropy=4710798.50, Δentropy=25133.50, budget=7549747.20, penalty=0.6240, w_min=0.0002, w_max=0.0085\n",
            "Iter 060: recon=0.000138, class=2.6121, entropy=4746625.00, Δentropy=35826.50, budget=7549747.20, penalty=0.6287, w_min=0.0002, w_max=0.0083\n",
            "Iter 070: recon=0.000180, class=2.6121, entropy=4713451.00, Δentropy=-33174.00, budget=7549747.20, penalty=0.6243, w_min=0.0002, w_max=0.0082\n",
            "Iter 080: recon=0.000153, class=2.6121, entropy=4733451.00, Δentropy=20000.00, budget=7549747.20, penalty=0.6270, w_min=0.0002, w_max=0.0081\n",
            "Iter 090: recon=0.000164, class=2.6121, entropy=4724829.00, Δentropy=-8622.00, budget=7549747.20, penalty=0.6258, w_min=0.0002, w_max=0.0080\n",
            "\n",
            "🔧 Optimizing layer4.1.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=2.7734, entropy=6198867.00, Δentropy=0.00, budget=7549747.20, penalty=0.8211, w_min=-0.0046, w_max=0.0121\n",
            "Iter 010: recon=0.000000, class=2.7734, entropy=6198863.50, Δentropy=-3.50, budget=7549747.20, penalty=0.8211, w_min=-0.0083, w_max=0.0092\n",
            "Iter 020: recon=0.000000, class=2.7734, entropy=6198868.50, Δentropy=5.00, budget=7549747.20, penalty=0.8211, w_min=-0.0067, w_max=0.0119\n",
            "Iter 030: recon=0.000000, class=2.7734, entropy=6198261.50, Δentropy=-607.00, budget=7549747.20, penalty=0.8210, w_min=-0.0016, w_max=0.0163\n",
            "Iter 040: recon=0.000780, class=2.7734, entropy=4591718.50, Δentropy=-1606543.00, budget=7549747.20, penalty=0.6082, w_min=0.0016, w_max=0.0210\n",
            "Iter 050: recon=0.000247, class=2.7734, entropy=4620743.00, Δentropy=29024.50, budget=7549747.20, penalty=0.6120, w_min=0.0011, w_max=0.0212\n",
            "Iter 060: recon=0.000026, class=2.7734, entropy=4759613.00, Δentropy=138870.00, budget=7549747.20, penalty=0.6304, w_min=0.0001, w_max=0.0203\n",
            "Iter 070: recon=0.000143, class=2.7734, entropy=4643181.00, Δentropy=-116432.00, budget=7549747.20, penalty=0.6150, w_min=0.0006, w_max=0.0209\n",
            "Iter 080: recon=0.000080, class=2.7734, entropy=4674283.00, Δentropy=31102.00, budget=7549747.20, penalty=0.6191, w_min=0.0005, w_max=0.0212\n",
            "Iter 090: recon=0.000064, class=2.7734, entropy=4687318.00, Δentropy=13035.00, budget=7549747.20, penalty=0.6209, w_min=0.0003, w_max=0.0216\n",
            "\n",
            "✅ Per-layer optimization complete.\n",
            "Test Accuracy: 10.00%\n"
          ]
        }
      ],
      "source": [
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "evaluate(model, test_loader)  # Before\n",
        "optimize_per_layer(model, test_loader)\n",
        "evaluate(model, test_loader)  # After\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwQnjndrswyN",
        "outputId": "4d8d1028-3b8b-498a-9071-52a9e6b2dd7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: constriction in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16 in /usr/local/lib/python3.11/dist-packages (from constriction) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install constriction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "5MhXY0e4syVo",
        "outputId": "978c6376-9fea-41bc-fa70-e73c5b6a468b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'constriction.stream'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a7ae237427ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconstriction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'constriction.stream'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from constriction.stream import stack\n",
        "from collections import Counter\n",
        "\n",
        "def compress_tensor_with_constriction(tensor):\n",
        "    # Step 1: Flatten and convert to numpy\n",
        "    flat = tensor.detach().cpu().view(-1).numpy()\n",
        "\n",
        "    # Step 2: Quantize values to integer symbols\n",
        "    unique_vals, inverse = np.unique(flat, return_inverse=True)\n",
        "    symbols = inverse.astype(np.uint32)  # Shape: [N]\n",
        "    num_symbols = len(unique_vals)\n",
        "\n",
        "    # Step 3: Frequency histogram\n",
        "    counts = np.bincount(symbols, minlength=num_symbols)\n",
        "    total = np.sum(counts)\n",
        "    probs = counts / total\n",
        "\n",
        "    # Step 4: Build CDF for range encoding\n",
        "    cdf = np.zeros(num_symbols + 1, dtype=np.uint32)\n",
        "    precision = 16  # Range encoder precision (bits)\n",
        "    cdf[1:] = np.cumsum((probs * (1 << precision)).astype(np.uint32))\n",
        "    cdf[-1] = 1 << precision  # Ensure total probability = 2^precision\n",
        "\n",
        "    # Step 5: Range encode using constriction\n",
        "    encoder = stack.AnsCoder()\n",
        "    encoder.encode(symbols.tolist(), cdf.tolist())\n",
        "\n",
        "    compressed_bytes = encoder.get_compressed().nbytes\n",
        "    original_bits = flat.nbytes * 8  # float32 input\n",
        "    compressed_bits = compressed_bytes * 8\n",
        "    cr = original_bits / compressed_bits\n",
        "\n",
        "    return cr, original_bits, compressed_bits, unique_vals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WSLE9SF0krx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACM1pHins0s0"
      },
      "outputs": [],
      "source": [
        "total_orig_bits = 0\n",
        "total_comp_bits = 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        ratio, orig_bits, comp_bits, _ = compress_tensor_with_constriction(param)\n",
        "        total_orig_bits += orig_bits\n",
        "        total_comp_bits += comp_bits\n",
        "        print(f\"{name}: CR = {ratio:.2f}×, Orig = {orig_bits}, Comp = {comp_bits}\")\n",
        "\n",
        "final_cr = total_orig_bits / total_comp_bits\n",
        "print(f\"\\n🎯 Final Model Compression Ratio (with constriction): {final_cr:.2f}×\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn-aoeBLtRIJ",
        "outputId": "eda43e0c-e79a-4b6a-f346-56f00e2225df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: range-coder in /usr/local/lib/python3.11/dist-packages (1.1)\n"
          ]
        }
      ],
      "source": [
        "pip install range-coder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wHUEVsmtSIs"
      },
      "outputs": [],
      "source": [
        "from range_coder import RangeEncoder, RangeDecoder\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import io\n",
        "\n",
        "def compress_tensor(tensor):\n",
        "    flat = tensor.cpu().view(-1).detach().numpy()\n",
        "\n",
        "    # Step 1: Create integer symbols\n",
        "    unique_vals, inverse = np.unique(flat, return_inverse=True)\n",
        "    symbol_sequence = inverse  # indices into unique_vals\n",
        "    counts = Counter(symbol_sequence)\n",
        "    total = sum(counts.values())\n",
        "\n",
        "    # Step 2: Normalize frequencies\n",
        "    freqs = {sym: max(1, int((cnt / total) * 10000)) for sym, cnt in counts.items()}\n",
        "    sym_list = list(freqs.keys())\n",
        "    freq_list = [freqs[sym] for sym in sym_list]\n",
        "\n",
        "    # Step 3: Range encode\n",
        "    encoder = RangeEncoder()\n",
        "    compressed = io.BytesIO()\n",
        "    encoder.encode(freq_list, symbol_sequence.tolist(), compressed)\n",
        "\n",
        "    compressed_bytes = compressed.getbuffer().nbytes\n",
        "    original_bits = flat.nbytes * 8  # float32\n",
        "    compressed_bits = compressed_bytes * 8\n",
        "\n",
        "    ratio = original_bits / compressed_bits\n",
        "    return ratio, original_bits, compressed_bits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "iORf8wgXtY4g",
        "outputId": "da4f714b-0672-4c65-bc03-94f8f449a2a2"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "function missing required argument 'filepath' (pos 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-54cf226793c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"conv\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"weight\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_bits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_bits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_original_bits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0morig_bits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_compressed_bits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcomp_bits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-719965706044>\u001b[0m in \u001b[0;36mcompress_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Step 3: Range encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRangeEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mcompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: function missing required argument 'filepath' (pos 1)"
          ]
        }
      ],
      "source": [
        "total_original_bits, total_compressed_bits = 0, 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        ratio, orig_bits, comp_bits = compress_tensor(param)\n",
        "        total_original_bits += orig_bits\n",
        "        total_compressed_bits += comp_bits\n",
        "        print(f\"{name}: CR = {ratio:.2f}, Orig = {orig_bits}, Compressed = {comp_bits}\")\n",
        "\n",
        "overall_ratio = total_original_bits / total_compressed_bits\n",
        "print(f\"\\n🌟 Final Compression Ratio (Range Encoded): {overall_ratio:.2f}×\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIKY-uLM0mrW",
        "outputId": "1e908e1f-315c-4b56-8883-548bd565759d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (0.23.0)\n"
          ]
        }
      ],
      "source": [
        "pip install zstandard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSw32uIJ0rP6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import zstandard as zstd\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "def compress_tensor_with_zstd(tensor, dtype=torch.float16, compression_level=3):\n",
        "    # Convert to lower precision and serialize\n",
        "    tensor = tensor.detach().cpu().to(dtype)\n",
        "    byte_buffer = io.BytesIO()\n",
        "    np.save(byte_buffer, tensor.numpy(), allow_pickle=False)\n",
        "    raw_bytes = byte_buffer.getvalue()\n",
        "\n",
        "    # Compress with Zstandard\n",
        "    compressor = zstd.ZstdCompressor(level=compression_level)\n",
        "    compressed = compressor.compress(raw_bytes)\n",
        "\n",
        "    # Sizes\n",
        "    original_bits = len(raw_bytes) * 8\n",
        "    compressed_bits = len(compressed) * 8\n",
        "    compression_ratio = original_bits / compressed_bits\n",
        "\n",
        "    return compression_ratio, original_bits, compressed_bits, compressed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEV04UO50v85",
        "outputId": "6b949913-922d-4551-ab74-ff4082dd5bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight: Zstd CR = 1.05×, Orig = 28672, Comp = 27192\n",
            "layer1.0.conv1.weight: Zstd CR = 1.07×, Orig = 590848, Comp = 551912\n",
            "layer1.0.conv2.weight: Zstd CR = 1.09×, Orig = 590848, Comp = 544280\n",
            "layer1.1.conv1.weight: Zstd CR = 1.08×, Orig = 590848, Comp = 545144\n",
            "layer1.1.conv2.weight: Zstd CR = 1.08×, Orig = 590848, Comp = 544992\n",
            "layer2.0.conv1.weight: Zstd CR = 1.09×, Orig = 1180672, Comp = 1083992\n",
            "layer2.0.conv2.weight: Zstd CR = 1.09×, Orig = 2360320, Comp = 2166728\n",
            "layer2.1.conv1.weight: Zstd CR = 1.09×, Orig = 2360320, Comp = 2169616\n",
            "layer2.1.conv2.weight: Zstd CR = 1.09×, Orig = 2360320, Comp = 2171408\n",
            "layer3.0.conv1.weight: Zstd CR = 1.09×, Orig = 4719616, Comp = 4335304\n",
            "layer3.0.conv2.weight: Zstd CR = 1.09×, Orig = 9438208, Comp = 8687568\n",
            "layer3.1.conv1.weight: Zstd CR = 1.08×, Orig = 9438208, Comp = 8711256\n",
            "layer3.1.conv2.weight: Zstd CR = 1.08×, Orig = 9438208, Comp = 8704256\n",
            "layer4.0.conv1.weight: Zstd CR = 1.09×, Orig = 18875392, Comp = 17341664\n",
            "layer4.0.conv2.weight: Zstd CR = 1.10×, Orig = 37749760, Comp = 34400016\n",
            "layer4.1.conv1.weight: Zstd CR = 1.13×, Orig = 37749760, Comp = 33412736\n",
            "layer4.1.conv2.weight: Zstd CR = 1.10×, Orig = 37749760, Comp = 34333576\n",
            "\n",
            "📦 Final Model Compression Ratio with Zstd: 1.10×\n"
          ]
        }
      ],
      "source": [
        "total_orig_bits = 0\n",
        "total_comp_bits = 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        cr, ob, cb, _ = compress_tensor_with_zstd(param, dtype=torch.float16)\n",
        "        total_orig_bits += ob\n",
        "        total_comp_bits += cb\n",
        "        print(f\"{name}: Zstd CR = {cr:.2f}×, Orig = {ob}, Comp = {cb}\")\n",
        "\n",
        "final_cr = total_orig_bits / total_comp_bits\n",
        "print(f\"\\n📦 Final Model Compression Ratio with Zstd: {final_cr:.2f}×\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 4\n",
        "FIXED_T = 100.5\n",
        "LR = 0.001\n",
        "NUM_ITERATIONS = 100\n",
        "CR_target = 10  # Global compression ratio target\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Model\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"🎯 Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Activation storage\n",
        "temp_activations = {}\n",
        "def activation_hook(layer_name):\n",
        "    def hook(module, input, output):\n",
        "        temp_activations[layer_name] = input[0].detach().clone()\n",
        "    return hook\n",
        "\n",
        "# Register hooks\n",
        "for name, layer in model.named_modules():\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        layer.register_forward_hook(activation_hook(name))\n",
        "layer_soft_probs = {}\n",
        "# Quantization Module\n",
        "class MinMaxQuantization(nn.Module):\n",
        "    def __init__(self, weight, num_levels=2**NUM_BITS, fixed_T=FIXED_T, entropy_budget=None):\n",
        "        super().__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.fixed_T = fixed_T\n",
        "        self.entropy_budget = entropy_budget\n",
        "\n",
        "        w_min_init = weight.min().detach()\n",
        "        w_max_init = weight.max().detach()\n",
        "        pad = 0.05 * (w_max_init - w_min_init)\n",
        "        self.w_min = nn.Parameter(w_min_init - pad)\n",
        "        self.w_max = nn.Parameter(w_max_init + pad)\n",
        "\n",
        "    def forward(self, w):\n",
        "        EPS = 1e-6\n",
        "        w_min = self.w_min.clamp(max=self.w_max.item() - EPS)\n",
        "        w_max = self.w_max.clamp(min=w_min.item() + EPS)\n",
        "        w_norm = (w - w_min) / (w_max - w_min + EPS)\n",
        "\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "        dists = -torch.abs(w_norm.unsqueeze(-1) - q_levels)\n",
        "        soft_probs = torch.softmax(dists * self.fixed_T, dim=-1)\n",
        "        w_q = (soft_probs * q_levels).sum(dim=-1)\n",
        "        w_deq = w_q * (w_max - w_min) + w_min\n",
        "\n",
        "        #entropy = -(soft_probs + EPS).mul((soft_probs + EPS).log()).sum()\n",
        "        # Entropy across quantization bins\n",
        "        bin_mass = soft_probs.sum(dim=0)            # [num_bins]\n",
        "        bin_probs = bin_mass / bin_mass.sum()\n",
        "        entropy = -(bin_probs * (bin_probs + EPS).log()).sum()\n",
        "\n",
        "        budget_penalty = (entropy / (self.entropy_budget + EPS)) ** 2\n",
        "        return w_deq, entropy, budget_penalty, soft_probs\n",
        "\n",
        "# Optimization\n",
        "def optimize_per_layer(model, test_loader):\n",
        "    model.eval()\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    total_params = sum(p.numel() for n, p in model.named_parameters() if \"conv\" in n and \"weight\" in n)\n",
        "\n",
        "    # Get batch\n",
        "    data_iterator = iter(test_loader)\n",
        "    images, labels = next(data_iterator)\n",
        "    images, labels = images[:2].to(device), labels[:2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model(images)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name:\n",
        "            print(f\"\\n🔧 Optimizing {name}...\")\n",
        "            layer_name = name.replace(\".weight\", \"\")\n",
        "            if layer_name not in temp_activations:\n",
        "                print(f\"⏭️ Skipping {layer_name} (no activation).\")\n",
        "                continue\n",
        "\n",
        "            original_weight = param.detach().clone()\n",
        "            num_weights = original_weight.numel()\n",
        "            entropy_budget = (32 * num_weights) / CR_target\n",
        "\n",
        "            quant_layer = MinMaxQuantization(original_weight, entropy_budget=entropy_budget).to(device)\n",
        "            optimizer = optim.Adam(quant_layer.parameters(), lr=LR)\n",
        "            mse_loss_fn = nn.MSELoss()\n",
        "            activation_input = temp_activations[layer_name].detach().clone().to(device)\n",
        "            del temp_activations[layer_name]\n",
        "\n",
        "            prev_entropy = None\n",
        "            original_param_data = param.data.clone()\n",
        "            prev_class_loss = float('inf')\n",
        "            for iteration in range(NUM_ITERATIONS):\n",
        "                optimizer.zero_grad()\n",
        "                q_weight, entropy, penalty, soft_probs  = quant_layer(original_weight)\n",
        "\n",
        "                q_out = nn.functional.conv2d(activation_input, q_weight,\n",
        "                                             stride=param.shape[2], padding=param.shape[3])\n",
        "                o_out = nn.functional.conv2d(activation_input, original_weight,\n",
        "                                             stride=param.shape[2], padding=param.shape[3])\n",
        "                recon_loss = mse_loss_fn(q_out, o_out)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    param.data = q_weight.detach()\n",
        "                    outputs = model(images)\n",
        "                    class_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                    param.data = original_param_data  # restore\n",
        "                    #class_loss = nn.CrossEntropyLoss()(model(images), labels)\n",
        "\n",
        "                if 0.2 < class_loss:\n",
        "                  break\n",
        "                prev_class_loss = class_loss\n",
        "\n",
        "                total_loss = 0.1 * recon_loss + 0.9 * class_loss + 0.1 * entropy\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if iteration % 10 == 0:\n",
        "                    delta_entropy = entropy.item() - prev_entropy if prev_entropy is not None else 0.0\n",
        "                    print(f\"Iter {iteration:03d}: recon={recon_loss.item():.6f}, \"\n",
        "                          f\"class={class_loss.item():.4f}, entropy={entropy.item():.2f}, \"\n",
        "                          f\"Δentropy={delta_entropy:.2f}, total_loss={total_loss:.4f}, \"\n",
        "                          f\"penalty={penalty.item():.4f}, \"\n",
        "                          f\"w_min={quant_layer.w_min.item():.4f}, w_max={quant_layer.w_max.item():.4f}\")\n",
        "                    prev_entropy = entropy.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                layer_soft_probs[name] = soft_probs.detach().cpu()\n",
        "                final_weight, _, _,_ = quant_layer(original_weight)\n",
        "                param.copy_(final_weight.to(param.device))\n",
        "\n",
        "            del quant_layer, optimizer, activation_input\n",
        "            del q_weight, entropy, penalty, q_out, o_out\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # 🧠; Refresh activations after every residual block\n",
        "            if 1:#:any(name.startswith(f\"layer{l}.0.conv1\") for l in [1, 2, 3, 4]):\n",
        "                data_iterator = iter(test_loader)\n",
        "                images, labels = next(data_iterator)\n",
        "                images, labels = images[:2].to(device), labels[:2].to(device)\n",
        "                with torch.no_grad():\n",
        "                    model(images)\n",
        "                print(f\"🔁 Refreshed activations after {name}\")\n",
        "\n",
        "    print(\"\\n✅ Per-layer optimization complete.\")\n"
      ],
      "metadata": {
        "id": "DbWpxV7GH1Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, test_loader)        # Accuracy before quantization\n",
        "optimize_per_layer(model, test_loader)\n",
        "evaluate(model, test_loader)        # Accuracy after quantization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ywfUAaIb0P",
        "outputId": "98d7a8f1-035e-44e4-fa44-e52fd76e0918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Accuracy: 86.53%\n",
            "\n",
            "🔧 Optimizing conv1.weight...\n",
            "Iter 000: recon=0.000457, class=0.0098, entropy=5.08, Δentropy=0.00, total_loss=0.5169, penalty=0.0000, w_min=-0.1669, w_max=0.1531\n",
            "Iter 010: recon=0.000352, class=0.0099, entropy=5.03, Δentropy=-0.05, total_loss=0.5120, penalty=0.0000, w_min=-0.1766, w_max=0.1618\n",
            "Iter 020: recon=0.000508, class=0.0098, entropy=4.99, Δentropy=-0.04, total_loss=0.5074, penalty=0.0000, w_min=-0.1860, w_max=0.1718\n",
            "Iter 030: recon=0.000556, class=0.0098, entropy=4.95, Δentropy=-0.04, total_loss=0.5034, penalty=0.0000, w_min=-0.1954, w_max=0.1803\n",
            "Iter 040: recon=0.000454, class=0.0099, entropy=4.91, Δentropy=-0.04, total_loss=0.4997, penalty=0.0000, w_min=-0.2051, w_max=0.1870\n",
            "Iter 050: recon=0.000334, class=0.0100, entropy=4.87, Δentropy=-0.03, total_loss=0.4964, penalty=0.0000, w_min=-0.2144, w_max=0.1934\n",
            "Iter 060: recon=0.000285, class=0.0102, entropy=4.84, Δentropy=-0.03, total_loss=0.4933, penalty=0.0000, w_min=-0.2234, w_max=0.2002\n",
            "Iter 070: recon=0.000309, class=0.0102, entropy=4.81, Δentropy=-0.03, total_loss=0.4904, penalty=0.0000, w_min=-0.2320, w_max=0.2080\n",
            "Iter 080: recon=0.000323, class=0.0102, entropy=4.78, Δentropy=-0.03, total_loss=0.4874, penalty=0.0000, w_min=-0.2406, w_max=0.2153\n",
            "Iter 090: recon=0.000335, class=0.0101, entropy=4.75, Δentropy=-0.03, total_loss=0.4845, penalty=0.0000, w_min=-0.2492, w_max=0.2227\n",
            "🔁 Refreshed activations after conv1.weight\n",
            "\n",
            "🔧 Optimizing layer1.0.conv1.weight...\n",
            "Iter 000: recon=0.000001, class=0.0097, entropy=7.30, Δentropy=0.00, total_loss=0.7390, penalty=0.0000, w_min=-0.0750, w_max=0.0644\n",
            "Iter 010: recon=0.000001, class=0.0098, entropy=7.24, Δentropy=-0.06, total_loss=0.7326, penalty=0.0000, w_min=-0.0812, w_max=0.0702\n",
            "Iter 020: recon=0.000002, class=0.0100, entropy=7.18, Δentropy=-0.06, total_loss=0.7266, penalty=0.0000, w_min=-0.0887, w_max=0.0777\n",
            "Iter 030: recon=0.000002, class=0.0103, entropy=7.12, Δentropy=-0.06, total_loss=0.7211, penalty=0.0000, w_min=-0.0970, w_max=0.0845\n",
            "Iter 040: recon=0.000003, class=0.0107, entropy=7.07, Δentropy=-0.05, total_loss=0.7163, penalty=0.0000, w_min=-0.1049, w_max=0.0916\n",
            "Iter 050: recon=0.000004, class=0.0111, entropy=7.02, Δentropy=-0.05, total_loss=0.7120, penalty=0.0000, w_min=-0.1127, w_max=0.0985\n",
            "Iter 060: recon=0.000005, class=0.0113, entropy=6.98, Δentropy=-0.04, total_loss=0.7081, penalty=0.0000, w_min=-0.1203, w_max=0.1051\n",
            "Iter 070: recon=0.000005, class=0.0112, entropy=6.94, Δentropy=-0.04, total_loss=0.7044, penalty=0.0000, w_min=-0.1275, w_max=0.1115\n",
            "Iter 080: recon=0.000006, class=0.0112, entropy=6.91, Δentropy=-0.03, total_loss=0.7012, penalty=0.0000, w_min=-0.1346, w_max=0.1177\n",
            "Iter 090: recon=0.000007, class=0.0112, entropy=6.88, Δentropy=-0.03, total_loss=0.6981, penalty=0.0000, w_min=-0.1414, w_max=0.1237\n",
            "🔁 Refreshed activations after layer1.0.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer1.0.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0119, entropy=7.87, Δentropy=0.00, total_loss=0.7982, penalty=0.0000, w_min=-0.0443, w_max=0.0481\n",
            "Iter 010: recon=0.000000, class=0.0117, entropy=7.71, Δentropy=-0.17, total_loss=0.7813, penalty=0.0000, w_min=-0.0542, w_max=0.0580\n",
            "Iter 020: recon=0.000000, class=0.0116, entropy=7.58, Δentropy=-0.13, total_loss=0.7680, penalty=0.0000, w_min=-0.0639, w_max=0.0673\n",
            "Iter 030: recon=0.000001, class=0.0108, entropy=7.47, Δentropy=-0.10, total_loss=0.7568, penalty=0.0000, w_min=-0.0733, w_max=0.0756\n",
            "Iter 040: recon=0.000001, class=0.0105, entropy=7.39, Δentropy=-0.08, total_loss=0.7486, penalty=0.0000, w_min=-0.0815, w_max=0.0836\n",
            "Iter 050: recon=0.000001, class=0.0109, entropy=7.33, Δentropy=-0.06, total_loss=0.7427, penalty=0.0000, w_min=-0.0879, w_max=0.0920\n",
            "Iter 060: recon=0.000002, class=0.0111, entropy=7.27, Δentropy=-0.06, total_loss=0.7366, penalty=0.0000, w_min=-0.0923, w_max=0.1010\n",
            "Iter 070: recon=0.000002, class=0.0112, entropy=7.21, Δentropy=-0.06, total_loss=0.7310, penalty=0.0000, w_min=-0.0969, w_max=0.1098\n",
            "Iter 080: recon=0.000003, class=0.0109, entropy=7.16, Δentropy=-0.05, total_loss=0.7254, penalty=0.0000, w_min=-0.1040, w_max=0.1166\n",
            "Iter 090: recon=0.000004, class=0.0107, entropy=7.11, Δentropy=-0.05, total_loss=0.7205, penalty=0.0000, w_min=-0.1103, w_max=0.1239\n",
            "🔁 Refreshed activations after layer1.0.conv2.weight\n",
            "\n",
            "🔧 Optimizing layer1.1.conv1.weight...\n",
            "Iter 000: recon=0.000001, class=0.0105, entropy=7.80, Δentropy=0.00, total_loss=0.7891, penalty=0.0000, w_min=-0.0451, w_max=0.0495\n",
            "Iter 010: recon=0.000002, class=0.0104, entropy=7.63, Δentropy=-0.16, total_loss=0.7728, penalty=0.0000, w_min=-0.0549, w_max=0.0595\n",
            "Iter 020: recon=0.000002, class=0.0106, entropy=7.51, Δentropy=-0.13, total_loss=0.7601, penalty=0.0000, w_min=-0.0642, w_max=0.0692\n",
            "Iter 030: recon=0.000004, class=0.0111, entropy=7.41, Δentropy=-0.10, total_loss=0.7505, penalty=0.0000, w_min=-0.0726, w_max=0.0784\n",
            "Iter 040: recon=0.000005, class=0.0111, entropy=7.32, Δentropy=-0.08, total_loss=0.7424, penalty=0.0000, w_min=-0.0797, w_max=0.0875\n",
            "Iter 050: recon=0.000006, class=0.0108, entropy=7.25, Δentropy=-0.07, total_loss=0.7350, penalty=0.0000, w_min=-0.0861, w_max=0.0963\n",
            "Iter 060: recon=0.000008, class=0.0110, entropy=7.19, Δentropy=-0.06, total_loss=0.7288, penalty=0.0000, w_min=-0.0929, w_max=0.1044\n",
            "Iter 070: recon=0.000009, class=0.0114, entropy=7.13, Δentropy=-0.06, total_loss=0.7236, penalty=0.0000, w_min=-0.0996, w_max=0.1120\n",
            "Iter 080: recon=0.000011, class=0.0120, entropy=7.08, Δentropy=-0.05, total_loss=0.7190, penalty=0.0000, w_min=-0.1059, w_max=0.1195\n",
            "Iter 090: recon=0.000013, class=0.0123, entropy=7.04, Δentropy=-0.05, total_loss=0.7147, penalty=0.0000, w_min=-0.1121, w_max=0.1267\n",
            "🔁 Refreshed activations after layer1.1.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer1.1.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0127, entropy=7.73, Δentropy=0.00, total_loss=0.7848, penalty=0.0000, w_min=-0.0485, w_max=0.0410\n",
            "Iter 010: recon=0.000000, class=0.0135, entropy=7.56, Δentropy=-0.17, total_loss=0.7684, penalty=0.0000, w_min=-0.0585, w_max=0.0507\n",
            "Iter 020: recon=0.000000, class=0.0134, entropy=7.43, Δentropy=-0.13, total_loss=0.7555, penalty=0.0000, w_min=-0.0683, w_max=0.0595\n",
            "Iter 030: recon=0.000000, class=0.0132, entropy=7.33, Δentropy=-0.10, total_loss=0.7451, penalty=0.0000, w_min=-0.0776, w_max=0.0674\n",
            "Iter 040: recon=0.000000, class=0.0132, entropy=7.25, Δentropy=-0.08, total_loss=0.7366, penalty=0.0000, w_min=-0.0864, w_max=0.0748\n",
            "Iter 050: recon=0.000001, class=0.0130, entropy=7.18, Δentropy=-0.07, total_loss=0.7293, penalty=0.0000, w_min=-0.0947, w_max=0.0820\n",
            "Iter 060: recon=0.000001, class=0.0130, entropy=7.11, Δentropy=-0.06, total_loss=0.7229, penalty=0.0000, w_min=-0.1026, w_max=0.0888\n",
            "Iter 070: recon=0.000001, class=0.0131, entropy=7.06, Δentropy=-0.06, total_loss=0.7173, penalty=0.0000, w_min=-0.1101, w_max=0.0954\n",
            "Iter 080: recon=0.000001, class=0.0131, entropy=7.00, Δentropy=-0.05, total_loss=0.7123, penalty=0.0000, w_min=-0.1173, w_max=0.1018\n",
            "Iter 090: recon=0.000001, class=0.0131, entropy=6.96, Δentropy=-0.05, total_loss=0.7077, penalty=0.0000, w_min=-0.1244, w_max=0.1080\n",
            "🔁 Refreshed activations after layer1.1.conv2.weight\n",
            "\n",
            "🔧 Optimizing layer2.0.conv1.weight...\n",
            "Iter 000: recon=0.000001, class=0.0139, entropy=7.98, Δentropy=0.00, total_loss=0.8103, penalty=0.0000, w_min=-0.0350, w_max=0.0421\n",
            "Iter 010: recon=0.000001, class=0.0132, entropy=7.77, Δentropy=-0.21, total_loss=0.7890, penalty=0.0000, w_min=-0.0448, w_max=0.0520\n",
            "Iter 020: recon=0.000002, class=0.0132, entropy=7.62, Δentropy=-0.16, total_loss=0.7735, penalty=0.0000, w_min=-0.0542, w_max=0.0614\n",
            "Iter 030: recon=0.000002, class=0.0126, entropy=7.50, Δentropy=-0.12, total_loss=0.7612, penalty=0.0000, w_min=-0.0631, w_max=0.0699\n",
            "Iter 040: recon=0.000003, class=0.0123, entropy=7.41, Δentropy=-0.09, total_loss=0.7517, penalty=0.0000, w_min=-0.0714, w_max=0.0774\n",
            "Iter 050: recon=0.000004, class=0.0124, entropy=7.33, Δentropy=-0.08, total_loss=0.7442, penalty=0.0000, w_min=-0.0791, w_max=0.0844\n",
            "Iter 060: recon=0.000005, class=0.0125, entropy=7.27, Δentropy=-0.06, total_loss=0.7379, penalty=0.0000, w_min=-0.0856, w_max=0.0918\n",
            "Iter 070: recon=0.000006, class=0.0123, entropy=7.21, Δentropy=-0.06, total_loss=0.7320, penalty=0.0000, w_min=-0.0911, w_max=0.0994\n",
            "Iter 080: recon=0.000006, class=0.0121, entropy=7.16, Δentropy=-0.05, total_loss=0.7267, penalty=0.0000, w_min=-0.0968, w_max=0.1066\n",
            "Iter 090: recon=0.000008, class=0.0123, entropy=7.11, Δentropy=-0.05, total_loss=0.7221, penalty=0.0000, w_min=-0.1027, w_max=0.1132\n",
            "🔁 Refreshed activations after layer2.0.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer2.0.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0125, entropy=8.57, Δentropy=0.00, total_loss=0.8683, penalty=0.0000, w_min=-0.0360, w_max=0.0434\n",
            "Iter 010: recon=0.000000, class=0.0129, entropy=8.37, Δentropy=-0.20, total_loss=0.8488, penalty=0.0000, w_min=-0.0458, w_max=0.0533\n",
            "Iter 020: recon=0.000000, class=0.0130, entropy=8.23, Δentropy=-0.15, total_loss=0.8344, penalty=0.0000, w_min=-0.0550, w_max=0.0627\n",
            "Iter 030: recon=0.000001, class=0.0139, entropy=8.11, Δentropy=-0.11, total_loss=0.8239, penalty=0.0000, w_min=-0.0640, w_max=0.0710\n",
            "Iter 040: recon=0.000001, class=0.0123, entropy=8.02, Δentropy=-0.09, total_loss=0.8132, penalty=0.0000, w_min=-0.0727, w_max=0.0784\n",
            "Iter 050: recon=0.000001, class=0.0126, entropy=7.95, Δentropy=-0.07, total_loss=0.8061, penalty=0.0000, w_min=-0.0801, w_max=0.0859\n",
            "Iter 060: recon=0.000001, class=0.0140, entropy=7.88, Δentropy=-0.06, total_loss=0.8008, penalty=0.0000, w_min=-0.0858, w_max=0.0941\n",
            "Iter 070: recon=0.000001, class=0.0134, entropy=7.82, Δentropy=-0.06, total_loss=0.7946, penalty=0.0000, w_min=-0.0918, w_max=0.1016\n",
            "Iter 080: recon=0.000002, class=0.0131, entropy=7.77, Δentropy=-0.05, total_loss=0.7890, penalty=0.0000, w_min=-0.0980, w_max=0.1085\n",
            "Iter 090: recon=0.000002, class=0.0127, entropy=7.72, Δentropy=-0.05, total_loss=0.7838, penalty=0.0000, w_min=-0.1037, w_max=0.1156\n",
            "🔁 Refreshed activations after layer2.0.conv2.weight\n",
            "\n",
            "🔧 Optimizing layer2.1.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=0.0124, entropy=8.67, Δentropy=0.00, total_loss=0.8782, penalty=0.0000, w_min=-0.0310, w_max=0.0364\n",
            "Iter 010: recon=0.000000, class=0.0128, entropy=8.44, Δentropy=-0.23, total_loss=0.8552, penalty=0.0000, w_min=-0.0409, w_max=0.0463\n",
            "Iter 020: recon=0.000001, class=0.0126, entropy=8.27, Δentropy=-0.17, total_loss=0.8383, penalty=0.0000, w_min=-0.0502, w_max=0.0555\n",
            "Iter 030: recon=0.000002, class=0.0128, entropy=8.14, Δentropy=-0.12, total_loss=0.8259, penalty=0.0000, w_min=-0.0590, w_max=0.0637\n",
            "Iter 040: recon=0.000002, class=0.0133, entropy=8.05, Δentropy=-0.10, total_loss=0.8167, penalty=0.0000, w_min=-0.0670, w_max=0.0713\n",
            "Iter 050: recon=0.000003, class=0.0138, entropy=7.97, Δentropy=-0.08, total_loss=0.8093, penalty=0.0000, w_min=-0.0738, w_max=0.0789\n",
            "Iter 060: recon=0.000003, class=0.0143, entropy=7.90, Δentropy=-0.07, total_loss=0.8029, penalty=0.0000, w_min=-0.0795, w_max=0.0867\n",
            "Iter 070: recon=0.000002, class=0.0151, entropy=7.84, Δentropy=-0.06, total_loss=0.7975, penalty=0.0000, w_min=-0.0854, w_max=0.0939\n",
            "Iter 080: recon=0.000003, class=0.0158, entropy=7.78, Δentropy=-0.06, total_loss=0.7926, penalty=0.0000, w_min=-0.0913, w_max=0.1006\n",
            "Iter 090: recon=0.000003, class=0.0164, entropy=7.73, Δentropy=-0.05, total_loss=0.7881, penalty=0.0000, w_min=-0.0969, w_max=0.1074\n",
            "🔁 Refreshed activations after layer2.1.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer2.1.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0163, entropy=8.49, Δentropy=0.00, total_loss=0.8639, penalty=0.0000, w_min=-0.0249, w_max=0.0390\n",
            "Iter 010: recon=0.000000, class=0.0153, entropy=8.26, Δentropy=-0.23, total_loss=0.8396, penalty=0.0000, w_min=-0.0346, w_max=0.0489\n",
            "Iter 020: recon=0.000000, class=0.0158, entropy=8.10, Δentropy=-0.16, total_loss=0.8240, penalty=0.0000, w_min=-0.0431, w_max=0.0586\n",
            "Iter 030: recon=0.000000, class=0.0157, entropy=7.98, Δentropy=-0.12, total_loss=0.8122, penalty=0.0000, w_min=-0.0487, w_max=0.0683\n",
            "Iter 040: recon=0.000000, class=0.0163, entropy=7.89, Δentropy=-0.09, total_loss=0.8035, penalty=0.0000, w_min=-0.0539, w_max=0.0771\n",
            "Iter 050: recon=0.000000, class=0.0164, entropy=7.81, Δentropy=-0.08, total_loss=0.7957, penalty=0.0000, w_min=-0.0593, w_max=0.0852\n",
            "Iter 060: recon=0.000000, class=0.0160, entropy=7.74, Δentropy=-0.07, total_loss=0.7886, penalty=0.0000, w_min=-0.0643, w_max=0.0929\n",
            "Iter 070: recon=0.000000, class=0.0159, entropy=7.68, Δentropy=-0.06, total_loss=0.7825, penalty=0.0000, w_min=-0.0690, w_max=0.1003\n",
            "Iter 080: recon=0.000000, class=0.0157, entropy=7.63, Δentropy=-0.05, total_loss=0.7770, penalty=0.0000, w_min=-0.0736, w_max=0.1074\n",
            "Iter 090: recon=0.000001, class=0.0155, entropy=7.58, Δentropy=-0.05, total_loss=0.7722, penalty=0.0000, w_min=-0.0780, w_max=0.1142\n",
            "🔁 Refreshed activations after layer2.1.conv2.weight\n",
            "\n",
            "🔧 Optimizing layer3.0.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=0.0160, entropy=8.36, Δentropy=0.00, total_loss=0.8505, penalty=0.0000, w_min=-0.0225, w_max=0.0396\n",
            "Iter 010: recon=0.000000, class=0.0152, entropy=8.13, Δentropy=-0.23, total_loss=0.8269, penalty=0.0000, w_min=-0.0323, w_max=0.0494\n",
            "Iter 020: recon=0.000000, class=0.0152, entropy=7.96, Δentropy=-0.17, total_loss=0.8098, penalty=0.0000, w_min=-0.0419, w_max=0.0582\n",
            "Iter 030: recon=0.000001, class=0.0153, entropy=7.84, Δentropy=-0.12, total_loss=0.7981, penalty=0.0000, w_min=-0.0471, w_max=0.0680\n",
            "Iter 040: recon=0.000001, class=0.0152, entropy=7.75, Δentropy=-0.09, total_loss=0.7885, penalty=0.0000, w_min=-0.0530, w_max=0.0765\n",
            "Iter 050: recon=0.000001, class=0.0166, entropy=7.67, Δentropy=-0.08, total_loss=0.7817, penalty=0.0000, w_min=-0.0578, w_max=0.0849\n",
            "Iter 060: recon=0.000001, class=0.0159, entropy=7.60, Δentropy=-0.07, total_loss=0.7742, penalty=0.0000, w_min=-0.0629, w_max=0.0925\n",
            "Iter 070: recon=0.000002, class=0.0158, entropy=7.54, Δentropy=-0.06, total_loss=0.7682, penalty=0.0000, w_min=-0.0677, w_max=0.0998\n",
            "Iter 080: recon=0.000002, class=0.0163, entropy=7.49, Δentropy=-0.05, total_loss=0.7636, penalty=0.0000, w_min=-0.0722, w_max=0.1067\n",
            "Iter 090: recon=0.000003, class=0.0167, entropy=7.45, Δentropy=-0.04, total_loss=0.7596, penalty=0.0000, w_min=-0.0764, w_max=0.1132\n",
            "🔁 Refreshed activations after layer3.0.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer3.0.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0176, entropy=8.96, Δentropy=0.00, total_loss=0.9116, penalty=0.0000, w_min=-0.0200, w_max=0.0267\n",
            "Iter 010: recon=0.000000, class=0.0161, entropy=8.70, Δentropy=-0.26, total_loss=0.8845, penalty=0.0000, w_min=-0.0296, w_max=0.0362\n",
            "Iter 020: recon=0.000000, class=0.0173, entropy=8.51, Δentropy=-0.19, total_loss=0.8664, penalty=0.0000, w_min=-0.0386, w_max=0.0419\n",
            "Iter 030: recon=0.000000, class=0.0163, entropy=8.38, Δentropy=-0.13, total_loss=0.8526, penalty=0.0000, w_min=-0.0438, w_max=0.0488\n",
            "Iter 040: recon=0.000000, class=0.0159, entropy=8.29, Δentropy=-0.09, total_loss=0.8434, penalty=0.0000, w_min=-0.0488, w_max=0.0552\n",
            "Iter 050: recon=0.000000, class=0.0144, entropy=8.22, Δentropy=-0.07, total_loss=0.8346, penalty=0.0000, w_min=-0.0537, w_max=0.0609\n",
            "Iter 060: recon=0.000000, class=0.0134, entropy=8.16, Δentropy=-0.06, total_loss=0.8276, penalty=0.0000, w_min=-0.0584, w_max=0.0662\n",
            "Iter 070: recon=0.000001, class=0.0126, entropy=8.11, Δentropy=-0.05, total_loss=0.8221, penalty=0.0000, w_min=-0.0627, w_max=0.0712\n",
            "Iter 080: recon=0.000001, class=0.0124, entropy=8.07, Δentropy=-0.04, total_loss=0.8178, penalty=0.0000, w_min=-0.0668, w_max=0.0758\n",
            "Iter 090: recon=0.000001, class=0.0127, entropy=8.03, Δentropy=-0.03, total_loss=0.8148, penalty=0.0000, w_min=-0.0705, w_max=0.0801\n",
            "🔁 Refreshed activations after layer3.0.conv2.weight\n",
            "\n",
            "🔧 Optimizing layer3.1.conv1.weight...\n",
            "Iter 000: recon=0.000000, class=0.0135, entropy=8.74, Δentropy=0.00, total_loss=0.8856, penalty=0.0000, w_min=-0.0189, w_max=0.0208\n",
            "Iter 010: recon=0.000000, class=0.0150, entropy=8.43, Δentropy=-0.30, total_loss=0.8567, penalty=0.0000, w_min=-0.0264, w_max=0.0297\n",
            "Iter 020: recon=0.000000, class=0.0158, entropy=8.24, Δentropy=-0.19, total_loss=0.8387, penalty=0.0000, w_min=-0.0338, w_max=0.0381\n",
            "Iter 030: recon=0.000000, class=0.0170, entropy=8.12, Δentropy=-0.12, total_loss=0.8277, penalty=0.0000, w_min=-0.0403, w_max=0.0455\n",
            "Iter 040: recon=0.000000, class=0.0200, entropy=8.05, Δentropy=-0.08, total_loss=0.8227, penalty=0.0000, w_min=-0.0459, w_max=0.0520\n",
            "Iter 050: recon=0.000000, class=0.0249, entropy=7.99, Δentropy=-0.05, total_loss=0.8218, penalty=0.0000, w_min=-0.0507, w_max=0.0575\n",
            "Iter 060: recon=0.000000, class=0.0324, entropy=7.96, Δentropy=-0.04, total_loss=0.8250, penalty=0.0000, w_min=-0.0548, w_max=0.0622\n",
            "Iter 070: recon=0.000000, class=0.0427, entropy=7.93, Δentropy=-0.03, total_loss=0.8316, penalty=0.0000, w_min=-0.0584, w_max=0.0664\n",
            "Iter 080: recon=0.000000, class=0.0549, entropy=7.91, Δentropy=-0.02, total_loss=0.8404, penalty=0.0000, w_min=-0.0616, w_max=0.0701\n",
            "Iter 090: recon=0.000000, class=0.0682, entropy=7.89, Δentropy=-0.02, total_loss=0.8508, penalty=0.0000, w_min=-0.0646, w_max=0.0734\n",
            "🔁 Refreshed activations after layer3.1.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer3.1.conv2.weight...\n",
            "Iter 000: recon=0.000000, class=0.0860, entropy=8.75, Δentropy=0.00, total_loss=0.9519, penalty=0.0000, w_min=-0.0129, w_max=0.0096\n",
            "Iter 010: recon=0.000000, class=0.0877, entropy=8.48, Δentropy=-0.26, total_loss=0.9274, penalty=0.0000, w_min=-0.0194, w_max=0.0124\n",
            "Iter 020: recon=0.000000, class=0.0997, entropy=8.31, Δentropy=-0.18, total_loss=0.9204, penalty=0.0000, w_min=-0.0247, w_max=0.0158\n",
            "Iter 030: recon=0.000000, class=0.1179, entropy=8.18, Δentropy=-0.12, total_loss=0.9243, penalty=0.0000, w_min=-0.0293, w_max=0.0191\n",
            "Iter 040: recon=0.000000, class=0.1352, entropy=8.10, Δentropy=-0.08, total_loss=0.9316, penalty=0.0000, w_min=-0.0334, w_max=0.0219\n",
            "Iter 050: recon=0.000000, class=0.1605, entropy=8.04, Δentropy=-0.06, total_loss=0.9486, penalty=0.0000, w_min=-0.0371, w_max=0.0243\n",
            "Iter 060: recon=0.000000, class=0.1877, entropy=8.00, Δentropy=-0.04, total_loss=0.9689, penalty=0.0000, w_min=-0.0404, w_max=0.0264\n",
            "🔁 Refreshed activations after layer3.1.conv2.weight\n",
            "\n",
            "🔧 Optimizing layer4.0.conv1.weight...\n",
            "🔁 Refreshed activations after layer4.0.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer4.0.conv2.weight...\n",
            "🔁 Refreshed activations after layer4.0.conv2.weight\n",
            "\n",
            "🔧 Optimizing layer4.1.conv1.weight...\n",
            "🔁 Refreshed activations after layer4.1.conv1.weight\n",
            "\n",
            "🔧 Optimizing layer4.1.conv2.weight...\n",
            "Iter 000: recon=0.000004, class=0.1871, entropy=8.91, Δentropy=0.00, total_loss=1.0594, penalty=0.0000, w_min=-0.0026, w_max=0.0141\n",
            "🔁 Refreshed activations after layer4.1.conv2.weight\n",
            "\n",
            "✅ Per-layer optimization complete.\n",
            "🎯 Accuracy: 55.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_and_compress_with_zstd(weight_tensor, soft_probs, num_bits=4, zstd_level=5):\n",
        "    assert soft_probs.shape[-1] == 2 ** num_bits, \"Soft probs must match number of bins\"\n",
        "    quantized = soft_probs.argmax(dim=-1).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    flat = quantized.flatten()\n",
        "    if len(flat) % 2 != 0:\n",
        "        flat = np.append(flat, 0)\n",
        "\n",
        "    packed = np.bitwise_or(flat[0::2] << 4, flat[1::2])\n",
        "    packed_bytes = packed.tobytes()\n",
        "\n",
        "    compressor = zstd.ZstdCompressor(level=zstd_level)\n",
        "    compressed = compressor.compress(packed_bytes)\n",
        "\n",
        "    original_bits = quantized.size * num_bits\n",
        "    compressed_bits = len(compressed) * 8\n",
        "    cr = original_bits / compressed_bits\n",
        "\n",
        "    return cr, compressed\n",
        "print(\"\\n📦 Final Compression Ratios (4-bit + Zstd):\")\n",
        "for name, param in model.named_parameters():\n",
        "    if name in layer_soft_probs:\n",
        "        soft_probs = layer_soft_probs[name]\n",
        "        cr, compressed = quantize_and_compress_with_zstd(param.data, soft_probs)\n",
        "        print(f\"{name}: CR = {cr:.2f}×, Size = {len(compressed)} bytes\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKKefB4Leph3",
        "outputId": "aeec5fcd-190d-42fd-9af3-54fc58b45c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📦 Final Compression Ratios (4-bit + Zstd):\n",
            "conv1.weight: CR = 1.71×, Size = 505 bytes\n",
            "layer1.0.conv1.weight: CR = 3.97×, Size = 4641 bytes\n",
            "layer1.0.conv2.weight: CR = 3.08×, Size = 5994 bytes\n",
            "layer1.1.conv1.weight: CR = 3.37×, Size = 5467 bytes\n",
            "layer1.1.conv2.weight: CR = 3.99×, Size = 4616 bytes\n",
            "layer2.0.conv1.weight: CR = 3.17×, Size = 11611 bytes\n",
            "layer2.0.conv2.weight: CR = 3.31×, Size = 22268 bytes\n",
            "layer2.1.conv1.weight: CR = 3.29×, Size = 22394 bytes\n",
            "layer2.1.conv2.weight: CR = 4.41×, Size = 16732 bytes\n",
            "layer3.0.conv1.weight: CR = 6.80×, Size = 21688 bytes\n",
            "layer3.0.conv2.weight: CR = 9.07×, Size = 32507 bytes\n",
            "layer3.1.conv1.weight: CR = 22.01×, Size = 13401 bytes\n",
            "layer3.1.conv2.weight: CR = 11.64×, Size = 25344 bytes\n",
            "layer4.0.conv1.weight: CR = 2.67×, Size = 220622 bytes\n",
            "layer4.0.conv2.weight: CR = 7.90×, Size = 149364 bytes\n",
            "layer4.1.conv1.weight: CR = 7.64×, Size = 154500 bytes\n",
            "layer4.1.conv2.weight: CR = 8.43×, Size = 139983 bytes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}