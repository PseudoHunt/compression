# -*- coding: utf-8 -*-
"""SensitivityMapQuant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aFzz7f5Yul67G2zlJxnr_P64eO0prXqI
"""

# Mixed-precision group-wise quantization for OPT-125M with per-layer 4-bit code-space entropy
# Requirements: pip install torch transformers
import math
from typing import Dict, Tuple, List

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM


@torch.no_grad()
def affine_minmax_quantize_to_int(x: torch.Tensor, n_bits: int) -> torch.Tensor:
    """
    Group-level min-max affine quantization to integer bins [0, 2^n_bits - 1].
    x: 1D tensor (the group)
    Returns integer tensor of same shape, dtype=torch.int32
    """
    if n_bits not in (2, 4):
        raise ValueError("n_bits must be 2 or 4 in this version.")
    qmin, qmax = 0, (1 << n_bits) - 1
    x_min = x.min()
    x_max = x.max()
    if x_max <= x_min:  # degenerate group
        return torch.zeros_like(x, dtype=torch.int32)
    scale = (x_max - x_min) / float(qmax)
    q = torch.round((x - x_min) / scale).clamp(qmin, qmax)
    return q.to(torch.int32)


def rebin_to_4bit(q_int: torch.Tensor, n_bits: int) -> torch.Tensor:
    """
    Map 2b/4b integer bins into a single 4-bit code space [0..15].
    - 2-bit -> multiply by 4 (shift left by 2) -> {0,4,8,12}
    - 4-bit -> unchanged                     -> {0..15}
    """
    if n_bits == 2:
        return (q_int * 4).to(torch.int32)
    elif n_bits == 4:
        return q_int.to(torch.int32)
    else:
        raise ValueError("n_bits must be 2 or 4.")


def shannon_entropy_base2(int_codes: torch.Tensor, num_bins: int = 16) -> float:
    """
    Compute H = -sum p * log2 p of a 1D integer array with values in [0..num_bins-1].
    """
    hist = torch.bincount(int_codes, minlength=num_bins).float()
    total = hist.sum()
    if total == 0:
        return 0.0
    probs = hist / total
    nz = probs[probs > 0]
    return float(-(nz * torch.log2(nz)).sum().item())


@torch.no_grad()
def build_random_sensitivity(
    W: torch.Tensor,
    seed: int = 0,
    distribution: str = "uniform",
) -> torch.Tensor:
    """
    Create a reproducible per-weight sensitivity tensor (same shape as W).
    distribution: "uniform" in [0,1) or "normal" ~ N(0,1) then abs().
    """
    gen = torch.Generator(device=W.device)
    gen.manual_seed(seed)
    shape = W.shape
    if distribution == "uniform":
        S = torch.rand(shape, generator=gen, device=W.device)
    elif distribution == "normal":
        S = torch.abs(torch.randn(shape, generator=gen, device=W.device))
    else:
        raise ValueError("distribution must be 'uniform' or 'normal'")
    return S


@torch.no_grad()
def sensitivity_to_group_bits(
    sensitivity: torch.Tensor,
    group_size: int,
    top_pct_sensitive: float,
) -> List[int]:
    """
    Decide 2-bit vs 4-bit per group from per-weight sensitivity.
    - Compute the threshold for the top X% most sensitive weights.
    - Any group containing >=1 top-X% element -> 4-bit, else 2-bit.
    Returns a list of {2,4} with length = num_groups.
    """
    flatS = sensitivity.view(-1).contiguous()
    N = flatS.numel()
    num_groups = (N + group_size - 1) // group_size

    # Top-X% mask
    k = max(1, int(math.ceil(top_pct_sensitive * N)))
    # torch.topk returns the k largest values; get the kth value as threshold
    vals, idx = torch.topk(flatS, k, largest=True, sorted=True)
    thresh = vals.min()  # smallest among top-k is the cutoff
    top_mask = (flatS >= thresh)  # boolean mask of top-X%

    # Group decision: if any True in the group -> 4-bit else 2-bit
    bits = []
    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        if top_mask[s:e].any():
            bits.append(4)
        else:
            bits.append(2)
    return bits


@torch.no_grad()
def quantize_tensor_mixed_to_4bit_codes_sensitivity(
    W: torch.Tensor,
    group_size: int,
    top_pct_sensitive: float,
    seed: int = 0,
    sens_distribution: str = "uniform",
) -> Tuple[torch.Tensor, List[int]]:
    """
    Quantize a weight tensor W group-wise with mixed precision (2b/4b),
    where groups are assigned 4-bit if they contain any weight in the
    top-X% sensitivity set; else 2-bit. All codes are then mapped to 4-bit space.

    Returns:
      q4_codes: flattened 1D int32 tensor with values in [0..15]
      group_bits: list of bitwidths per group (values in {2,4})
    """
    flat = W.view(-1).contiguous()
    N = flat.numel()
    num_groups = (N + group_size - 1) // group_size

    # Build random sensitivity and derive per-group bits
    S = build_random_sensitivity(W, seed=seed, distribution=sens_distribution)
    group_bits = sensitivity_to_group_bits(S, group_size, top_pct_sensitive)

    q4_out = torch.empty(N, dtype=torch.int32, device=flat.device)
    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        grp = flat[s:e]
        n_bits = group_bits[g]  # 2 or 4

        q_int = affine_minmax_quantize_to_int(grp, n_bits)  # [0..(2^n_bits-1)]
        q4 = rebin_to_4bit(q_int, n_bits)                  # -> [0..15]
        q4_out[s:e] = q4

    return q4_out, group_bits


@torch.no_grad()
def quantize_opt125m_and_report_entropy(
    model_name: str = "facebook/opt-125m",
    group_size: int = 128,
    top_pct_sensitive: float = 0.25,  # X% in [0,1], e.g., 0.25 means top 25% most sensitive
    seed: int = 0,
    device: str = "cpu",
    sens_distribution: str = "uniform",  # or "normal"
) -> Dict[str, Dict]:
    """
    Quantizes each Linear layer's weights group-wise with mixed precision (2b/4b) driven by
    per-weight sensitivity (random for now) and computes per-layer entropy of the combined 4-bit codes.

    Returns a dict:
      {
        layer_name: {
          "entropy_bits": float,
          "num_params": int,
          "num_groups": int,
          "bit_alloc_counts": {2: n2, 4: n4},
          "top_pct_sensitive": float,
        }, ...
      }
    """
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=None)
    model.to(device)
    model.eval()

    results = {}

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            W = module.weight.detach().to(device)
            q4_codes, group_bits = quantize_tensor_mixed_to_4bit_codes_sensitivity(
                W,
                group_size=group_size,
                top_pct_sensitive=top_pct_sensitive,
                seed=seed,
                sens_distribution=sens_distribution,
            )
            H = shannon_entropy_base2(q4_codes, num_bins=16)
            n_groups = len(group_bits)
            counts = {2: group_bits.count(2), 4: group_bits.count(4)}
            results[name] = {
                "entropy_bits": H,
                "num_params": W.numel(),
                "num_groups": n_groups,
                "bit_alloc_counts": counts,
                "top_pct_sensitive": top_pct_sensitive,
            }

            # NOTE: remove the break to process all Linear layers; keeping for speed while testing
            break

    return results


if __name__ == "__main__":
    # Choose what fraction of the most sensitive weights should force their group to 4-bit.
    TOP_PCT_SENSITIVE = 0.0020  # e.g., 30% top sensitivity -> groups touched by these go 4-bit

    summary = quantize_opt125m_and_report_entropy(
        model_name="facebook/opt-125m",
        group_size=128,
        top_pct_sensitive=TOP_PCT_SENSITIVE,
        seed=42,
        device="cpu",
        sens_distribution="uniform",  # or "normal"
    )

    print("Per-layer entropy of combined 4-bit codes (OPT-125M):")
    for layer, info in summary.items():
        print(f"{layer:55s} | H={info['entropy_bits']:.4f} bits | params={info['num_params']:,} "
              f"| groups={info['num_groups']:,} | bit alloc {info['bit_alloc_counts']} "
              f"| top%={info['top_pct_sensitive']*100:.1f}")

"""# look at the dequant. its too simple. need to use the common scale one."""

# Mixed-precision (2b/4b) group-wise quantization for OPT-125M
# with per-layer 4-bit code-space entropy + dequantization + in-place weight replacement
# Requirements: pip install torch transformers
import math
from typing import Dict, Tuple, List

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM


@torch.no_grad()
def affine_minmax_quantize_to_int_with_params(x: torch.Tensor, n_bits: int):
    """
    Quantize 1D tensor x to integer bins [0..2^n_bits-1] and also return (x_min, scale)
    so we can dequantize later.
    Returns:
      q_int (int32 same shape), x_min (scalar tensor), scale (scalar tensor)
    """
    assert x.dim() == 1, "Expected 1D group vector"
    if n_bits not in (2, 4):
        raise ValueError("n_bits must be 2 or 4 in this version.")
    qmin, qmax = 0, (1 << n_bits) - 1
    x_min = x.min()
    x_max = x.max()
    if x_max <= x_min:
        # Degenerate group: all codes 0; dequant -> x_min
        scale = torch.tensor(0.0, dtype=x.dtype, device=x.device)
        q_int = torch.zeros_like(x, dtype=torch.int32)
        return q_int, x_min, scale
    scale = (x_max - x_min) / float(qmax)
    q = torch.round((x - x_min) / scale).clamp(qmin, qmax)
    return q.to(torch.int32), x_min, scale


def rebin_to_4bit(q_int: torch.Tensor, n_bits: int) -> torch.Tensor:
    """
    Map 2b/4b integer bins into a single 4-bit code space [0..15].
    - 2-bit -> multiply by 4 (shift left by 2) -> {0,4,8,12}
    - 4-bit -> unchanged                     -> {0..15}
    """
    if n_bits == 2:
        return (q_int * 4).to(torch.int32)
    elif n_bits == 4:
        return q_int.to(torch.int32)
    else:
        raise ValueError("n_bits must be 2 or 4.")


def shannon_entropy_base2(int_codes: torch.Tensor, num_bins: int = 16) -> float:
    hist = torch.bincount(int_codes, minlength=num_bins).float()
    total = hist.sum()
    if total == 0:
        return 0.0
    probs = hist / total
    nz = probs[probs > 0]
    return float(-(nz * torch.log2(nz)).sum().item())


@torch.no_grad()
def build_random_sensitivity(W: torch.Tensor, seed: int = 0, distribution: str = "uniform") -> torch.Tensor:
    """
    Create a reproducible per-weight sensitivity tensor (same shape as W).
    distribution: "uniform" in [0,1) or "normal" ~ N(0,1) then abs().
    """
    gen = torch.Generator(device=W.device)
    gen.manual_seed(seed)
    if distribution == "uniform":
        S = torch.rand_like(W, generator=gen)
    elif distribution == "normal":
        S = torch.abs(torch.randn_like(W, generator=gen))
    else:
        raise ValueError("distribution must be 'uniform' or 'normal'")
    return S


@torch.no_grad()
def sensitivity_to_group_bits(sensitivity: torch.Tensor, group_size: int, top_pct_sensitive: float) -> List[int]:
    """
    Decide 2-bit vs 4-bit per group from per-weight sensitivity.
    Any group containing >=1 top-X% element -> 4-bit, else 2-bit.
    Returns a list of {2,4} with length = num_groups.
    """
    if not (0.0 < top_pct_sensitive <= 1.0):
        raise ValueError("top_pct_sensitive must be in (0,1].")
    flatS = sensitivity.view(-1).contiguous()
    N = flatS.numel()
    num_groups = (N + group_size - 1) // group_size

    k = max(1, int(math.ceil(top_pct_sensitive * N)))
    vals, _ = torch.topk(flatS, k, largest=True, sorted=True)
    thresh = vals.min()
    top_mask = (flatS >= thresh)

    bits = []
    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        bits.append(4 if top_mask[s:e].any() else 2)
    return bits


@torch.no_grad()
def quantize_tensor_mixed_to_4bit_codes_sensitivity(
    W: torch.Tensor,
    group_size: int,
    top_pct_sensitive: float,
    seed: int = 0,
    sens_distribution: str = "uniform",
) -> Tuple[torch.Tensor, List[int], torch.Tensor, torch.Tensor]:
    """
    Quantize a weight tensor W group-wise with mixed precision (2b/4b),
    where groups are 4-bit if they contain any weight in the top-X% sensitivity set; else 2-bit.
    Returns:
      q4_codes: flattened 1D int32 tensor with values in [0..15]
      group_bits: list of bitwidths per group (values in {2,4})
      x_mins: per-group x_min (float32) [num_groups]
      scales: per-group scale (float32) [num_groups]
    """
    flat = W.view(-1).contiguous()
    N = flat.numel()
    num_groups = (N + group_size - 1) // group_size

    # Build random sensitivity and derive per-group bits
    S = build_random_sensitivity(W, seed=seed, distribution=sens_distribution)
    group_bits = sensitivity_to_group_bits(S, group_size, top_pct_sensitive)

    q4_out = torch.empty(N, dtype=torch.int32, device=flat.device)
    x_mins = torch.empty(num_groups, dtype=torch.float32, device=flat.device)
    scales = torch.empty(num_groups, dtype=torch.float32, device=flat.device)

    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        grp = flat[s:e]
        n_bits = group_bits[g]  # 2 or 4

        q_int, x_min, scale = affine_minmax_quantize_to_int_with_params(grp, n_bits)
        q4 = rebin_to_4bit(q_int, n_bits)

        q4_out[s:e] = q4
        x_mins[g] = x_min
        scales[g] = scale

    return q4_out, group_bits, x_mins, scales


@torch.no_grad()
def dequantize_from_4bit_codes(
    q4_codes: torch.Tensor,
    group_bits: List[int],
    x_mins: torch.Tensor,
    scales: torch.Tensor,
    group_size: int,
    out_shape: torch.Size,
) -> torch.Tensor:
    """
    Dequantize the merged 4-bit code stream back to float using saved per-group params.
    Returns a float32 tensor with shape = out_shape.
    """
    N = q4_codes.numel()
    num_groups = len(group_bits)
    assert (N + group_size - 1) // group_size == num_groups, "Mismatch in groups vs codes length."

    out = torch.empty(N, dtype=torch.float32, device=q4_codes.device)
    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        codes = q4_codes[s:e]

        if group_bits[g] == 2:
            q_int = (codes // 4).to(torch.float32)  # reverse the rebin {0,4,8,12} -> {0,1,2,3}
        else:
            q_int = codes.to(torch.float32)         # already 0..15

        out[s:e] = x_mins[g].to(torch.float32) + scales[g].to(torch.float32) * q_int

    return out.view(out_shape)


@torch.no_grad()
def quantize_opt125m_and_report_entropy(
    model_name: str = "facebook/opt-125m",
    group_size: int = 128,
    top_pct_sensitive: float = 0.25,  # X% in [0,1], e.g., 0.25 means top 25% most sensitive
    seed: int = 0,
    device: str = "cpu",
    sens_distribution: str = "uniform",  # or "normal"
    replace_weights: bool = True,        # NEW: replace layer weights in-place with dequantized floats
) -> Dict[str, Dict]:
    """
    Quantizes each Linear layer's weights group-wise with mixed precision (2b/4b) driven by
    per-weight sensitivity (random for now), computes per-layer entropy of the combined 4-bit codes,
    DEquantizes back to float, and (optionally) replaces the original weights in-place.
    """
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=None)
    model.to(device)
    model.eval()

    results = {}

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            W = module.weight.detach().to(device)

            # Quantize -> 4-bit codes + per-group params
            q4_codes, group_bits, x_mins, scales = quantize_tensor_mixed_to_4bit_codes_sensitivity(
                W,
                group_size=group_size,
                top_pct_sensitive=top_pct_sensitive,
                seed=seed,
                sens_distribution=sens_distribution,
            )

            # Entropy on combined 4-bit code space
            H = shannon_entropy_base2(q4_codes, num_bins=16)
            n_groups = len(group_bits)
            counts_2 = sum(1 for b in group_bits if b == 2)
            counts_4 = n_groups - counts_2

            # ---- NEW: Dequantize and (optionally) replace original weights ----
            W_deq = dequantize_from_4bit_codes(
                q4_codes=q4_codes,
                group_bits=group_bits,
                x_mins=x_mins,
                scales=scales,
                group_size=group_size,
                out_shape=W.shape,
            )

            if replace_weights:
                module.weight.data.copy_(W_deq)  # in-place replacement

            results[name] = {
                "entropy_bits": H,
                "num_params": W.numel(),
                "num_groups": n_groups,
                "bit_alloc_counts": {2: counts_2, 4: counts_4},
                "top_pct_sensitive": top_pct_sensitive,
            }

    return results


if __name__ == "__main__":
    TOP_PCT_SENSITIVE = 0.30  # top 30% most sensitive weights flip their groups to 4-bit

    summary = quantize_opt125m_and_report_entropy(
        model_name="facebook/opt-125m",
        group_size=128,
        top_pct_sensitive=TOP_PCT_SENSITIVE,
        seed=42,
        device="cpu",
        sens_distribution="uniform",  # or "normal"
        replace_weights=True,         # set False to skip in-place replacement
    )

    print("Per-layer entropy of combined 4-bit codes (OPT-125M) after replacement:")
    for layer, info in summary.items():
        print(f"{layer:55s} | H={info['entropy_bits']:.4f} bits | params={info['num_params']:,} "
              f"| groups={info['num_groups']:,} | bit alloc {info['bit_alloc_counts']} "
              f"| top%={info['top_pct_sensitive']*100:.1f}")