# -*- coding: utf-8 -*-
"""CSQ_2n4_working_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lVRy8LQP1CW1w8WwdzUMp4VXzYFoMa5W
"""

# Mixed-precision (2b-in-4b via cosine snapping) group-wise quantization for OPT-125M
# with per-layer 4-bit code-space entropy + dequantization + in-place weight replacement
# Requirements: pip install torch transformers
import math
from typing import Dict, Tuple, List

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM


# -----------------------
# Core helpers
# -----------------------
@torch.no_grad()
def affine_minmax_4bit_params(x: torch.Tensor):
    """
    For a 1D group vector x, compute 4-bit min-max params:
      scale = (x_max - x_min)/15, zero-point implicit via x_min.
    Returns: (x_min, scale) as scalars (tensors).
    """
    assert x.dim() == 1, "Expected 1D group vector"
    x_min = x.min()
    x_max = x.max()
    if x_max <= x_min:
        scale = torch.tensor(0.0, dtype=x.dtype, device=x.device)
    else:
        scale = (x_max - x_min) / 15.0
    return x_min, scale


@torch.no_grad()
def quantize4_to_int_affine(x: torch.Tensor, x_min: torch.Tensor, scale: torch.Tensor):
    """
    4-bit affine quant to [0..15] with min-max params.
    Code-space: u = (x - x_min)/scale, q = round(u).
    """
    if float(scale) == 0.0:
        return torch.zeros_like(x, dtype=torch.int32)
    u = (x - x_min) / scale
    q = torch.round(u).clamp(0, 15)
    return q.to(torch.int32)


@torch.no_grad()
def dequantize4_from_int_affine(q: torch.Tensor, x_min: torch.Tensor, scale: torch.Tensor):
    """Dequantize with min-max params: x = x_min + scale * q."""
    return (x_min.to(torch.float32) + scale.to(torch.float32) * q.to(torch.float32))


@torch.no_grad()
def shannon_entropy_base2(int_codes: torch.Tensor, num_bins: int = 16) -> float:
    hist = torch.bincount(int_codes, minlength=num_bins).float()
    total = hist.sum()
    if total == 0:
        return 0.0
    probs = hist / total
    nz = probs[probs > 0]
    return float(-(nz * torch.log2(nz)).sum().item())


# -----------------------
# Cosine snapping (2-bit in 4-bit grid)
# -----------------------
@torch.no_grad()
def two_bit_in_four_target_codes(x: torch.Tensor, s4: torch.Tensor, z4: torch.Tensor, b: int = 0):
    """
    Given 4-bit scale/zero-point (s4, z4), compute the 2-bit-in-4-bit target codes:
      grid = {b, b+4, b+8, b+12}, b in {0,2}
    u = x/s4 + z4   (real-valued code space)
    """
    u = x / s4 + z4
    k = torch.round((u - b) / 4.0)
    q2 = torch.clamp(b + 4.0 * k, 0.0, 15.0)
    return q2.to(torch.int32)


@torch.no_grad()
def cosine_snap_alpha(
    x: torch.Tensor,
    s4: torch.Tensor,
    z4: torch.Tensor,
    b: int = 0,
    snap_strength: float = 1.0,
    eps: float = 1e-6,
):
    """
    Compute alpha so that 4-bit quantize(x + alpha) == 2-bit-in-4-bit target codes.

    Uses a cosine nudge (period 4 in code-space) plus a minimal corrective shift
    that GUARANTEES exact landing on the target code cell.
    """
    # Target 2-bit-in-4-bit codes
    q2 = two_bit_in_four_target_codes(x, s4, z4, b=b).to(torch.float32)

    # Real-valued codes
    u = x / s4 + z4

    # Cosine nudge in code-space (period=4)
    phase = (u - b) - 4.0 * torch.floor((u - b) / 4.0)  # in [0,4)
    cos_term = torch.cos(2.0 * torch.pi * phase / 4.0)  # = cos(pi/2 * phase)
    alpha_code_cos = 0.5 * snap_strength * cos_term     # ≤ 0.5 step

    # After nudge, which code would rounding give?
    u_snapped = u + alpha_code_cos
    q_after = torch.round(u_snapped)

    # Minimal fix to force rounding to q2
    need_fix = (q_after != q2)
    delta = q2 - u_snapped
    outside = torch.clamp(torch.abs(delta) - 0.5, min=0.0)
    alpha_code_fix = torch.sign(delta) * (outside + eps)

    alpha_code = torch.where(need_fix, alpha_code_cos + alpha_code_fix, alpha_code_cos)
    return alpha_code * s4  # convert back to value-space


# -----------------------
# Sensitivity -> per-weight mask (0 for top sensitive)
# -----------------------
@torch.no_grad()
def build_random_sensitivity(W: torch.Tensor, seed: int = 0, distribution: str = "uniform") -> torch.Tensor:
    gen = torch.Generator(device=W.device)
    gen.manual_seed(seed)
    if distribution == "uniform":
        return torch.rand(W.shape, generator=gen, device=W.device)
    elif distribution == "normal":
        return torch.abs(torch.randn(W.shape, generator=gen, device=W.device))
    else:
        raise ValueError("distribution must be 'uniform' or 'normal'")


@torch.no_grad()
def sensitivity_to_mask(sensitivity: torch.Tensor, top_pct_sensitive: float) -> torch.Tensor:
    """
    Return a per-weight mask (same shape as sensitivity):
      mask = 0 for top-X% most sensitive weights (NO snapping; keep 4-bit)
      mask = 1 for the rest (apply cosine snapping to 2-bit-in-4-bit).
    """
    if not (0.0 < top_pct_sensitive <= 1.0):
        raise ValueError("top_pct_sensitive must be in (0,1].")
    flat = sensitivity.view(-1)
    N = flat.numel()
    k = max(1, int(math.ceil(top_pct_sensitive * N)))
    vals, _ = torch.topk(flat, k, largest=True, sorted=True)
    thresh = vals.min()
    top_mask = (flat >= thresh)  # boolean: True for top sensitive
    mask = (~top_mask).to(sensitivity.dtype)  # 1 for non-top, 0 for top
    return mask.view_as(sensitivity)


# -----------------------
# Quantize with cosine snapping mask
# -----------------------
@torch.no_grad()
def quantize_tensor_cosine_snap_to_4bit_codes(
    W: torch.Tensor,
    group_size: int,
    top_pct_sensitive: float,
    seed: int = 0,
    sens_distribution: str = "uniform",
    b: int = 0,                 # sub-grid offset: 0 -> {0,4,8,12}, 2 -> {2,6,10,14}
    snap_strength: float = 1.0, # cosine nudge strength [0,1]
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Group-wise 4-bit quantization with a per-weight cosine snapping mask:
      - mask=0 (most sensitive): no snapping, plain 4-bit
      - mask=1 (less sensitive): snap so codes lie on 2-bit-in-4-bit sub-grid

    Returns:
      q4_codes: flattened 1D int32 in [0..15]
      mask: per-weight {0,1} tensor (same shape as W)
      x_mins: per-group x_min (float32)
      scales: per-group scale (float32)
    """
    flat = W.view(-1).contiguous()
    N = flat.numel()
    num_groups = (N + group_size - 1) // group_size

    # Per-weight sensitivity → per-weight mask
    S = build_random_sensitivity(W, seed=seed, distribution=sens_distribution)
    mask = sensitivity_to_mask(S, top_pct_sensitive=top_pct_sensitive).contiguous().view(-1)

    q4_out = torch.empty(N, dtype=torch.int32, device=flat.device)
    x_mins = torch.empty(num_groups, dtype=torch.float32, device=flat.device)
    scales = torch.empty(num_groups, dtype=torch.float32, device=flat.device)

    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        grp = flat[s:e]

        # 4-bit min-max params per group
        x_min, scale = affine_minmax_4bit_params(grp)
        x_mins[g] = x_min
        scales[g] = scale

        if float(scale) == 0.0:
            q4_out[s:e] = 0
            continue

        # Convert (x_min, scale) to (s4, z4) so that u = x/s4 + z4 == (x - x_min)/scale
        s4 = scale
        z4 = -x_min / scale

        # Compute alpha only where mask==1 (less sensitive)
        m = mask[s:e].to(grp.dtype)  # 1 where we snap, 0 where we don't
        alpha_all = cosine_snap_alpha(
            grp, s4=s4, z4=z4, b=b, snap_strength=snap_strength, eps=1e-6
        )
        alpha = alpha_all * m

        # Final 4-bit integer codes
        u = (grp + alpha) / s4 + z4
        q = torch.round(u).clamp(0, 15).to(torch.int32)

        q4_out[s:e] = q

    return q4_out, mask.view_as(W), x_mins, scales


@torch.no_grad()
def dequantize_from_4bit_codes(
    q4_codes: torch.Tensor,
    x_mins: torch.Tensor,
    scales: torch.Tensor,
    group_size: int,
    out_shape: torch.Size,
) -> torch.Tensor:
    """
    Dequantize merged 4-bit codes with saved min-max params (no group bits needed).
    """
    N = q4_codes.numel()
    num_groups = x_mins.numel()
    assert (N + group_size - 1) // group_size == num_groups

    out = torch.empty(N, dtype=torch.float32, device=q4_codes.device)
    for g in range(num_groups):
        s = g * group_size
        e = min(s + group_size, N)
        q = q4_codes[s:e].to(torch.float32)
        out[s:e] = x_mins[g].to(torch.float32) + scales[g].to(torch.float32) * q
    return out.view(out_shape)


# -----------------------
# Orchestration on OPT-125M
# -----------------------
@torch.no_grad()
def quantize_opt125m_and_report_entropy(
    model_name: str = "facebook/opt-125m",
    group_size: int = 128,
    top_pct_sensitive: float = 0.25,  # top X% kept unsnapped (mask=0)
    seed: int = 0,
    device: str = "cpu",
    sens_distribution: str = "uniform",  # or "normal"
    b: int = 0,                          # choose {0,2} for the 2-bit sub-grid
    snap_strength: float = 1.0,
    replace_weights: bool = True,
) -> Dict[str, Dict]:
    """
    Per Linear layer:
      - Build per-weight mask from sensitivity (mask=0 for top-X%).
      - Group-wise min-max params (4-bit).
      - Apply cosine snapping for mask==1 to force 2-bit-in-4-bit codes.
      - Compute per-layer entropy over the 4-bit code stream.
      - Dequantize and (optionally) replace weights in-place.
    """
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=None)
    model.to(device)
    model.eval()

    results = {}

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            W = module.weight.detach().to(device)

            # Quantize → merged 4-bit codes with cosine snapping mask
            q4_codes, mask, x_mins, scales = quantize_tensor_cosine_snap_to_4bit_codes(
                W,
                group_size=group_size,
                top_pct_sensitive=top_pct_sensitive,
                seed=seed,
                sens_distribution=sens_distribution,
                b=b,
                snap_strength=snap_strength,
            )

            # Layer entropy on merged 4-bit codes
            H = shannon_entropy_base2(q4_codes, num_bins=16)
            num_params = W.numel()
            snapped = int(mask.sum().item())
            unsnapped = num_params - snapped  # top sensitive (kept 4-bit)

            # Dequantize and (optionally) replace weights
            W_deq = dequantize_from_4bit_codes(
                q4_codes=q4_codes,
                x_mins=x_mins,
                scales=scales,
                group_size=group_size,
                out_shape=W.shape,
            )
            if replace_weights:
                module.weight.data.copy_(W_deq)
            print(f"{name:55s} | H={H:.4f} bits | params={num_params:,} | snapped={snapped:,} | 4b_plain={unsnapped:,} | top%={top_pct_sensitive*100:.1f} | b={b} | g={group_size}")
            results[name] = {
                "entropy_bits": H,
                "num_params": num_params,
                "snapped_to_2b_in_4b": snapped,
                "kept_plain_4b": unsnapped,
                "top_pct_sensitive": top_pct_sensitive,
                "b_subgrid": b,
                "snap_strength": snap_strength,
                "group_size": group_size,
            }

    return results


if __name__ == "__main__":
    TOP_PCT_SENSITIVE = 0.10  # top 30% = no snapping (mask=0); rest snapped to 2b-in-4b

    summary = quantize_opt125m_and_report_entropy(
        model_name="facebook/opt-125m",
        group_size=128,
        top_pct_sensitive=TOP_PCT_SENSITIVE,
        seed=42,
        device="cpu",
        sens_distribution="uniform",
        b=2,                # try 0 or 2; 2 → {2,6,10,14}
        snap_strength=1.0,  # 0..1; minimal fix still ensures exact landing
        replace_weights=True,
    )

    print("Per-layer entropy of merged 4-bit codes (OPT-125M) with cosine snapping:")
    for layer, info in summary.items():
        print(f"{layer:55s} | H={info['entropy_bits']:.4f} bits | params={info['num_params']:,} "
              f"| snapped={info['snapped_to_2b_in_4b']:,} | 4b_plain={info['kept_plain_4b']:,} "
              f"| top%={info['top_pct_sensitive']*100:.1f} | b={info['b_subgrid']} | g={info['group_size']}")

results

