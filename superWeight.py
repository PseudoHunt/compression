# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ssslliwdJKA6dEvza_bFneuM-gzqRKEC
"""

import os
import torch
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForCausalLM

# -------------------------
# CONFIG
# -------------------------
model_name = "allenai/OLMo-1B-0724-hf"   # or "meta-llama/Llama-3-8b" etc.
device = "cuda" if torch.cuda.is_available() else "cpu"

TARGET_LAYER = 1          # super weight is in layer 2 (for LLaMA 7B in the paper)
SUPER_ROW = 1764          # row index in down_proj.weight (activation channel)
SUPER_COL = 1710          # col index in down_proj.weight (not needed for grads here)

text = "My favorite condiment is"  # any prompt; super activation is almost prompt-independent

# where to store heatmaps
HEATMAP_DIR = "grad_heatmaps"
os.makedirs(HEATMAP_DIR, exist_ok=True)

# -------------------------
# LOAD TOKENIZER & MODEL
# -------------------------
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map={"": device},   # put whole model on single device
)

model.train()  # we want gradients

base = model.model  # this is LlamaModel inside LlamaForCausalLM

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.masking_utils import create_causal_mask
import matplotlib.pyplot as plt

# -------------------------
# CONFIG
# -------------------------
model_name = "allenai/OLMo-1B-0724-hf"
device = "cuda" if torch.cuda.is_available() else "cpu"

# Layer index where the super weight row lives (0-based)
TARGET_LAYER = 1

# --- Handling two super weights ---
# If both super weights share the same output row, you only
# need that single row index here.
# If they are in *different* rows, list both:
#   SUPER_ROWS = [row_a, row_b]
SUPER_ROWS = [1764]    # example: single row containing the super weights

text = "My favorite condiment is"

# -------------------------
# LOAD TOKENIZER & MODEL
# -------------------------
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map={"": device},
)
model.train()
base = model.model  # this is OlmoModel

# -------------------------
# PREP INPUTS
# -------------------------
enc = tokenizer(text, return_tensors="pt")
input_ids = enc["input_ids"].to(device)
attention_mask = enc["attention_mask"].to(device)

# -------------------------
# HOOK ON TARGET LAYER MLP.DOWN_PROJ
# -------------------------
super_act_container = {"act": None}

def downproj_hook(module, inp, out):
    """
    out: [batch, seq_len, hidden_size]
    SUPER_ROWS are indices in the hidden dimension.
    If there are multiple super rows, we sum them to get a single
    scalar per token (you can change aggregation if needed).
    """
    # shape: [batch, seq_len, len(SUPER_ROWS)]
    super_channels = out[..., SUPER_ROWS]
    # Aggregate across channels -> [batch, seq_len]
    super_act_container["act"] = super_channels.sum(dim=-1)

handle = base.layers[TARGET_LAYER].mlp.down_proj.register_forward_hook(downproj_hook)

# -------------------------
# TRUNCATED FORWARD TO LAYER k
# -------------------------
def forward_to_layer_k(input_ids, attention_mask, k: int):
    """
    Manual truncated forward for OlmoModel up to and including layer k.
    Mirrors modeling_olmo.OlmoModel.forward but stops early.
    """
    if attention_mask is None:
        attention_mask_local = torch.ones_like(input_ids, device=input_ids.device)
    else:
        attention_mask_local = attention_mask

    # 1) Embeddings
    inputs_embeds = base.embed_tokens(input_ids)
    batch_size, seq_len, _ = inputs_embeds.shape

    # 2) Causal mask + positions (same logic as OlmoModel.forward)
    cache_position = torch.arange(seq_len, device=input_ids.device)
    position_ids = cache_position.unsqueeze(0)  # [1, seq_len]

    causal_mask = create_causal_mask(
        config=base.config,
        input_embeds=inputs_embeds,
        attention_mask=attention_mask_local,
        cache_position=cache_position,
        past_key_values=None,
        position_ids=position_ids,
    )

    hidden_states = inputs_embeds
    position_embeddings = base.rotary_emb(hidden_states, position_ids=position_ids)

    # 3) Run layers 0..k
    for idx, decoder_layer in enumerate(base.layers):
        hidden_states = decoder_layer(
            hidden_states,
            attention_mask=causal_mask,
            position_ids=position_ids,
            position_embeddings=position_embeddings,
            past_key_values=None,
            use_cache=False,
            cache_position=cache_position,
        )
        if idx == k:
            break

    hidden_states = base.norm(hidden_states)
    return hidden_states

# -------------------------
# GRADIENT SETUP
# -------------------------
# Only layers up to TARGET_LAYER (and embeddings) need grads
for i, layer in enumerate(base.layers):
    requires = (i <= TARGET_LAYER)
    for p in layer.parameters():
        p.requires_grad_(requires)

for p in base.embed_tokens.parameters():
    p.requires_grad_(True)

# -------------------------
# RUN FORWARD + BACKWARD
# -------------------------
model.zero_grad(set_to_none=True)

hidden_states = forward_to_layer_k(input_ids, attention_mask, TARGET_LAYER)

super_act = super_act_container["act"]      # [batch, seq_len]
assert super_act is not None, "Hook did not run!"

# Scalar objective from the super activations
# (you can use sum(), max() or focus on last token if you want)
objective = super_act.mean()

objective.backward()
handle.remove()

# -------------------------
# RANK PARAMETERS BY GRAD NORM (SUPER-PATH CANDIDATES)
# -------------------------
def collect_grad_norms(prefix_module, module_name):
    results = []
    for name, param in prefix_module.named_parameters():
        if param.grad is None:
            continue
        grad_norm = param.grad.detach().abs().mean().item()
        results.append((f"{module_name}.{name}", grad_norm))
    return results

contrib = []
contrib.extend(collect_grad_norms(base.embed_tokens, "embed_tokens"))
for idx in range(TARGET_LAYER + 1):
    layer = base.layers[idx]
    contrib.extend(collect_grad_norms(layer, f"layers.{idx}"))

contrib.sort(key=lambda x: x[1], reverse=True)

print("\nTop 20 parameters by |grad| mean (super-path candidates):")
for name, score in contrib[:20]:
    print(f"{name:80s}  grad_mean={score:.4e}")

# -------------------------
# HEATMAPS OF WEIGHT GRADIENTS
# -------------------------
def plot_param_grad_heatmap(param, title=None, vmax_percentile=99.0):
    """
    Plot a heatmap of the 2D gradient tensor for a weight matrix.
    """
    if param.grad is None:
        print(f"[skip] No grad for {title}")
        return

    g = param.grad.detach().float().cpu()
    if g.ndim != 2:
        print(f"[skip] Grad for {title} is not 2D (shape={tuple(g.shape)})")
        return

    # Clip extremes for nicer visualization
    flat = g.abs().flatten()
    if flat.numel() == 0:
        print(f"[skip] Empty grad for {title}")
        return
    vmax = torch.quantile(flat, vmax_percentile / 100.0).item()

    plt.figure(figsize=(6, 4))
    plt.imshow(g, aspect="auto", origin="lower", vmin=-vmax, vmax=vmax)
    plt.colorbar()
    plt.xlabel("input dimension")
    plt.ylabel("output dimension")
    plt.title(title or "grad heatmap")
    plt.tight_layout()
    plt.show()

# Example: visualize MLP down_proj gradients along the path
for idx in range(TARGET_LAYER + 1):
    layer = base.layers[idx]
    plot_param_grad_heatmap(
        layer.mlp.down_proj.weight,
        title=f"layers.{idx}.mlp.down_proj.weight grad"
    )

# You can add more heatmaps if you want, e.g. attention projections:
# for idx in range(TARGET_LAYER + 1):
#     attn = base.layers[idx].self_attn
#     plot_param_grad_heatmap(attn.q_proj.weight,  title=f\"layers.{idx}.self_attn.q_proj.weight grad\")
#     plot_param_grad_heatmap(attn.k_proj.weight,  title=f\"layers.{idx}.self_attn.k_proj.weight grad\")
#     plot_param_grad_heatmap(attn.v_proj.weight,  title=f\"layers.{idx}.self_attn.v_proj.weight grad\")

def plot_layer_all_grad_hists(layer, layer_idx, max_plots=None):
    """
    For a given transformer block (layer), plot grad histograms for all parameters
    that have gradients.

    max_plots: optional cap on how many params to plot (to avoid 100+ figures).
    """
    count = 0
    for name, param in layer.named_parameters(recurse=True):
        if param.grad is None:
            continue

        full_name = f"layers.{layer_idx}.{name}"
        print(f"Plotting grad hist for: {full_name}")
        plot_param_grad_hist(param, title=full_name)

        count += 1
        if max_plots is not None and count >= max_plots:
            print(f"[info] Reached max_plots={max_plots} for layer {layer_idx}")
            break

import torch
import matplotlib.pyplot as plt

def plot_param_grad_hist(param, title="", bins=100, logy=True):
    """
    Plot a histogram of gradients for a given parameter tensor.
    """
    if param.grad is None:
        print(f"[skip] {title}: grad is None")
        return

    grad = param.grad.detach().float().flatten().cpu()

    plt.figure(figsize=(6, 4))
    plt.hist(grad.numpy(), bins=bins)
    if logy:
        plt.yscale("log")
    plt.title(title)
    plt.xlabel("grad value")
    plt.ylabel("count")
    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm, PowerNorm

def plot_param_grad_heatmap(
    param,
    title="",
    mode="log",              # "log" | "square" | "power" | "grad_weight"
    gamma=0.3,               # for power norm
    clip_percentile=90,
    cmap="magma"
):
    if param.grad is None:
        return

    # ---- base signal ----
    grad = param.grad.detach().float().abs()

    if mode == "square":
        signal = grad ** 2
    elif mode == "grad_weight":
        signal = grad * param.detach().abs()
    else:
        signal = grad

    # ---- reshape ----
    if signal.ndim == 1:
        signal = signal.unsqueeze(0)
    elif signal.ndim > 2:
        signal = signal.view(signal.shape[0], -1)

    signal = signal.cpu().numpy()

    # ---- percentile clipping ----
    vmax = np.percentile(signal, clip_percentile)
    vmin = np.percentile(signal[signal > 0], 1) if np.any(signal > 0) else 1e-12
    signal = np.clip(signal, vmin, vmax)



    # ---- normalization ----
    if mode in ["log", "square", "grad_weight"]:
        norm = LogNorm(vmin=vmin, vmax=vmax)
    elif mode == "power":
        norm = PowerNorm(gamma=gamma, vmin=vmin, vmax=vmax)
    else:
        norm = None

    plt.figure(figsize=(6, 4))
    im = plt.imshow(
        signal,
        aspect="auto",
        cmap=cmap,
        norm=norm,
        interpolation="nearest"
    )
    plt.colorbar(im)
    plt.title(f"{title}  [{mode}]")
    plt.tight_layout()
    plt.show()

def plot_param_grad_heatmap_old(param, title=""):
    """
    2D heatmap of |grad| for a weight tensor.
    For 1D params, we show them as a single row.
    For >2D, we flatten all but last two dims.
    """
    if param.grad is None:
        return
    g = param.grad.detach()

    if g.ndim == 1:
        g2 = g.unsqueeze(0)  # [1, N]
    elif g.ndim == 2:
        g2 = g
    else:
        # Flatten leading dims: [d0, d1, ..., dk, M, N] -> [D, M*N? or D, N]
        # simplest: view as [D, -1]
        g2 = g.view(g.shape[0], -1)

    g2 = g2.float().abs().cpu()
    outlier_percentile = 90  # tune this
    threshold = np.percentile(g2, outlier_percentile)

    g2 = np.where(g2 >= threshold, g2, 0.0)
    # if g2.size() == 0:
    #     return

    plt.figure(figsize=(5, 4))
    plt.imshow(g2, aspect="auto", interpolation="nearest",cmap="magma")
    plt.colorbar(label="|grad|")
    plt.title(title)
    plt.xlabel("column")
    plt.ylabel("row")
    plt.tight_layout()
    plt.show()

def plot_layer_all_grad_heatmaps(layer, layer_idx=0, max_plots=20):
    """
    For a given decoder layer, plot |grad| heatmaps for all 2D+ parameters,
    up to max_plots.
    """
    count = 0
    for name, param in layer.named_parameters():
        if param.grad is None:
            continue
        # Only meaningful for 2D+ weight-like tensors
        if param.ndim < 2:
            continue
        count += 1
        if count > max_plots:
            break
        title = f"layer {layer_idx} :: {name} |grad| heatmap"
        plot_param_grad_heatmap_old(param, title=title)

# -------------------------
# RUN VIZ FOR ALL LAYERS UP TO TARGET_LAYER
# -------------------------
for idx in range(TARGET_LAYER + 1):
    layer = base.layers[idx]
    print(f"\n==================== LAYER {idx} ====================")
    # Histograms of all sub-layers' params
    #plot_layer_all_grad_hists(layer, layer_idx=idx, max_plots=20)
    # Heatmaps for all 2D params in the layer
    plot_layer_all_grad_heatmaps(layer, layer_idx=idx, max_plots=20)
