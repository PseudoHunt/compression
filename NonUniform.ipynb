{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0920887fd87c48d8a9abca0111fac1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e41a65f00df4cc085f6c35d991c84a5",
              "IPY_MODEL_bde8113e6de4498bacedd3ccaf830743",
              "IPY_MODEL_63b7eb45d3ae44e29f404844ee4c05b4"
            ],
            "layout": "IPY_MODEL_96d821f4e37b41d799a39ff90198b177"
          }
        },
        "1e41a65f00df4cc085f6c35d991c84a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b9bfa9ccc31445f879a22652a48d98c",
            "placeholder": "​",
            "style": "IPY_MODEL_5772edd8d43c4c4b8e39f883b457e408",
            "value": "config.json: 100%"
          }
        },
        "bde8113e6de4498bacedd3ccaf830743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00be46014d294fc09aaf9cb0a7555a54",
            "max": 651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_437fbd634f99462da834ae960a177cd2",
            "value": 651
          }
        },
        "63b7eb45d3ae44e29f404844ee4c05b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee36262605745d18ad9d7646b05d79e",
            "placeholder": "​",
            "style": "IPY_MODEL_04fab5378c43494d82b5869a44bab39d",
            "value": " 651/651 [00:00&lt;00:00, 78.2kB/s]"
          }
        },
        "96d821f4e37b41d799a39ff90198b177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b9bfa9ccc31445f879a22652a48d98c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5772edd8d43c4c4b8e39f883b457e408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00be46014d294fc09aaf9cb0a7555a54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "437fbd634f99462da834ae960a177cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ee36262605745d18ad9d7646b05d79e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04fab5378c43494d82b5869a44bab39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d592bea7f6b24d29b192a1e1b9df9568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fcfec647edd415595d52a69454d8f41",
              "IPY_MODEL_3c9098593a0546b5a642dce7be59351b",
              "IPY_MODEL_6a572cdc350f470e86a3b5b83b66670f"
            ],
            "layout": "IPY_MODEL_709c919a26c34f17ae4190d9da731fe9"
          }
        },
        "7fcfec647edd415595d52a69454d8f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc07189a85dd4c0c9951713d9ba8ccb1",
            "placeholder": "​",
            "style": "IPY_MODEL_df8244874ecc43c6a0d701d14378c6f3",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "3c9098593a0546b5a642dce7be59351b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08d851c047e4c9db6bd78fc8107712a",
            "max": 250540281,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffb1bd30e973430facf0f16098ef772d",
            "value": 250540281
          }
        },
        "6a572cdc350f470e86a3b5b83b66670f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00510c4cf09e4b3d9c8571bed2405649",
            "placeholder": "​",
            "style": "IPY_MODEL_5cf6feb8a17b49ca86ee446405866cc2",
            "value": " 251M/251M [00:01&lt;00:00, 238MB/s]"
          }
        },
        "709c919a26c34f17ae4190d9da731fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc07189a85dd4c0c9951713d9ba8ccb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df8244874ecc43c6a0d701d14378c6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a08d851c047e4c9db6bd78fc8107712a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffb1bd30e973430facf0f16098ef772d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00510c4cf09e4b3d9c8571bed2405649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cf6feb8a17b49ca86ee446405866cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7fc3c30f81646d29964e068e649afdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0af8d07ec8df4cb7a71ceacbdd314a76",
              "IPY_MODEL_64874d8830384dd0899b0d24d15bacb6",
              "IPY_MODEL_0768aebf3f784cddb4264ee9f1c6cc78"
            ],
            "layout": "IPY_MODEL_ad6fc702ee524a3a9ec2c72c6aba5e92"
          }
        },
        "0af8d07ec8df4cb7a71ceacbdd314a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2148b5254c224caf940edf075f8117db",
            "placeholder": "​",
            "style": "IPY_MODEL_8a5b590f736d4998b554a687e0ec65ff",
            "value": "generation_config.json: 100%"
          }
        },
        "64874d8830384dd0899b0d24d15bacb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c14492c285f401ea0b4e74fd026ec64",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f25b70eb45614f7f87b99f43f36302b3",
            "value": 137
          }
        },
        "0768aebf3f784cddb4264ee9f1c6cc78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bed637f0e354e5eb6f3a54b999e619c",
            "placeholder": "​",
            "style": "IPY_MODEL_0e7f43d215f94ddb932b9cc47a1dcd63",
            "value": " 137/137 [00:00&lt;00:00, 18.2kB/s]"
          }
        },
        "ad6fc702ee524a3a9ec2c72c6aba5e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2148b5254c224caf940edf075f8117db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5b590f736d4998b554a687e0ec65ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c14492c285f401ea0b4e74fd026ec64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25b70eb45614f7f87b99f43f36302b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bed637f0e354e5eb6f3a54b999e619c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7f43d215f94ddb932b9cc47a1dcd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32efe18b89dc4b85870fda616b7c24c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1168dc77ad004638b6c883888f0fbcde",
              "IPY_MODEL_117a4dcaf95749918442e388b3f77dc6",
              "IPY_MODEL_ea58145ee1284976b372a38256973ae6"
            ],
            "layout": "IPY_MODEL_1b83f6c742e149bab5003539c79d5aa3"
          }
        },
        "1168dc77ad004638b6c883888f0fbcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56efd00302284e3482b666d1cb28e68c",
            "placeholder": "​",
            "style": "IPY_MODEL_82f8d537312842efb64b861ca5c7f074",
            "value": "model.safetensors: 100%"
          }
        },
        "117a4dcaf95749918442e388b3f77dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22f8e75e2182489e83b25c692763d9dc",
            "max": 250501024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01e41b0b618e46f9877181a7f0780ece",
            "value": 250501024
          }
        },
        "ea58145ee1284976b372a38256973ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1053343ecf54452aa9473cd95fa40341",
            "placeholder": "​",
            "style": "IPY_MODEL_cb74a26685a34fe2a514c6d986c7961b",
            "value": " 251M/251M [00:01&lt;00:00, 251MB/s]"
          }
        },
        "1b83f6c742e149bab5003539c79d5aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56efd00302284e3482b666d1cb28e68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f8d537312842efb64b861ca5c7f074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22f8e75e2182489e83b25c692763d9dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01e41b0b618e46f9877181a7f0780ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1053343ecf54452aa9473cd95fa40341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb74a26685a34fe2a514c6d986c7961b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "974ba326ae014525981fd4199b57739a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea874d49aefd42cfaf91859d52e84b8d",
              "IPY_MODEL_411e624a201647fc8f12034fbaaa774f",
              "IPY_MODEL_909d6fc9d7e94ea28433e68c6e65516e"
            ],
            "layout": "IPY_MODEL_a555fda8abab4110984b07ef074f3f9c"
          }
        },
        "ea874d49aefd42cfaf91859d52e84b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b5e5c5691e4085a3ed7281f4255f35",
            "placeholder": "​",
            "style": "IPY_MODEL_7dbedee90d9d492d880c4b7b211bceea",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "411e624a201647fc8f12034fbaaa774f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb62e81fba5c40b7aed8eb771a8302ef",
            "max": 685,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a5fc401579a4180b1b889e802afa54d",
            "value": 685
          }
        },
        "909d6fc9d7e94ea28433e68c6e65516e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c00dd647eb3347f387dfdb05d5c035de",
            "placeholder": "​",
            "style": "IPY_MODEL_d15a166ddf5d454c8151c32a12791aed",
            "value": " 685/685 [00:00&lt;00:00, 76.3kB/s]"
          }
        },
        "a555fda8abab4110984b07ef074f3f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b5e5c5691e4085a3ed7281f4255f35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbedee90d9d492d880c4b7b211bceea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb62e81fba5c40b7aed8eb771a8302ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a5fc401579a4180b1b889e802afa54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c00dd647eb3347f387dfdb05d5c035de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d15a166ddf5d454c8151c32a12791aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c35e0555f4ad4c49ad5d67584a9226b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ecada196d394a5b90f63ad4190b3f03",
              "IPY_MODEL_500c3ef2fbdf42f0a111fd7521f343ee",
              "IPY_MODEL_903252a050a94536a1d204c6242ce2f0"
            ],
            "layout": "IPY_MODEL_e548eb09dec14df49e4ab73b65c482c5"
          }
        },
        "0ecada196d394a5b90f63ad4190b3f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b84eb65bc0c4e93b1e6f6c45d675124",
            "placeholder": "​",
            "style": "IPY_MODEL_e122a074c9624d29be3d6be1023d223e",
            "value": "vocab.json: 100%"
          }
        },
        "500c3ef2fbdf42f0a111fd7521f343ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e847b7e6a80d4f1f9720442f71edf1b1",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbb57c882d8e47779433d82c216d35ea",
            "value": 898822
          }
        },
        "903252a050a94536a1d204c6242ce2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42dfc547b0cd48fa870fa6a81b076e8d",
            "placeholder": "​",
            "style": "IPY_MODEL_0ad4749ab0b147828c3fedf61617c861",
            "value": " 899k/899k [00:00&lt;00:00, 4.39MB/s]"
          }
        },
        "e548eb09dec14df49e4ab73b65c482c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b84eb65bc0c4e93b1e6f6c45d675124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e122a074c9624d29be3d6be1023d223e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e847b7e6a80d4f1f9720442f71edf1b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb57c882d8e47779433d82c216d35ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42dfc547b0cd48fa870fa6a81b076e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad4749ab0b147828c3fedf61617c861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7834d43e7bb344ff8a8340f296683d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0010bb7d26a846f28dc036346cfb00f1",
              "IPY_MODEL_bbc6c991f48e420bbd45eaa35ad647f6",
              "IPY_MODEL_aa258fc9cda441a08f844407663a437b"
            ],
            "layout": "IPY_MODEL_dacf88e4a1ba48bbb1fa0659493fd8bb"
          }
        },
        "0010bb7d26a846f28dc036346cfb00f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fc48c484f4742989226bc15e6064d56",
            "placeholder": "​",
            "style": "IPY_MODEL_5da244f118bf4d0a929b196d033aad0f",
            "value": "merges.txt: 100%"
          }
        },
        "bbc6c991f48e420bbd45eaa35ad647f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95cbcfd1d13b4b64b7b3a06932e8adfb",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8887faa4cc640a7b3c581b050291933",
            "value": 456318
          }
        },
        "aa258fc9cda441a08f844407663a437b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef2aaea1676e4831946b9f7140c21bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_e2e11304d449428a93f1425fa5959492",
            "value": " 456k/456k [00:00&lt;00:00, 15.7MB/s]"
          }
        },
        "dacf88e4a1ba48bbb1fa0659493fd8bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc48c484f4742989226bc15e6064d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da244f118bf4d0a929b196d033aad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95cbcfd1d13b4b64b7b3a06932e8adfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8887faa4cc640a7b3c581b050291933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef2aaea1676e4831946b9f7140c21bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e11304d449428a93f1425fa5959492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27c491f6d2b640f791527d8ee29f3825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b88d72309874fdba016df69d93d80ff",
              "IPY_MODEL_63432fbab19f45d386740e04f678023a",
              "IPY_MODEL_e04ce6ce2d3b49a8a9f70c51d15055e6"
            ],
            "layout": "IPY_MODEL_4f75043a6240493684e8a3943c1017ec"
          }
        },
        "4b88d72309874fdba016df69d93d80ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002274d8f4a64c9d9770da6ebfdca6d9",
            "placeholder": "​",
            "style": "IPY_MODEL_6b95e94e87a14c01aae25f4c22ff08ce",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "63432fbab19f45d386740e04f678023a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88f524f56ca54c7196fa7040bc5de33c",
            "max": 441,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_219634ef1b814128b7e7428e7b096134",
            "value": 441
          }
        },
        "e04ce6ce2d3b49a8a9f70c51d15055e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a25cefc14d646169ccd44297e4f0676",
            "placeholder": "​",
            "style": "IPY_MODEL_8e4062b7d8d24e9dafa15ab4f8bb8c5f",
            "value": " 441/441 [00:00&lt;00:00, 52.2kB/s]"
          }
        },
        "4f75043a6240493684e8a3943c1017ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "002274d8f4a64c9d9770da6ebfdca6d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b95e94e87a14c01aae25f4c22ff08ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f524f56ca54c7196fa7040bc5de33c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "219634ef1b814128b7e7428e7b096134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a25cefc14d646169ccd44297e4f0676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4062b7d8d24e9dafa15ab4f8bb8c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiLJqpgswELp",
        "outputId": "5a03e699-5673-47af-b2a9-ee58fa631b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Loading TinyStories from CSV...\n",
            "🔠 Tokenizing TinyStories for calibration...\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.k_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/50, Entropy: 265.4024, Loss: 0.00612533\n",
            "Iteration 1/50, Entropy: 265.4069, Loss: 0.00573260\n",
            "Iteration 1/50, Entropy: 265.4255, Loss: 0.00571611\n",
            "Iteration 1/50, Entropy: 265.4567, Loss: 0.00554545\n",
            "Iteration 1/50, Entropy: 265.4746, Loss: 0.00559397\n",
            "Iteration 2/50, Entropy: 265.4818, Loss: 0.00585293\n",
            "Iteration 2/50, Entropy: 265.4867, Loss: 0.00550320\n",
            "Iteration 2/50, Entropy: 265.4975, Loss: 0.00556068\n",
            "Iteration 2/50, Entropy: 265.5141, Loss: 0.00537909\n",
            "Iteration 2/50, Entropy: 265.5258, Loss: 0.00546224\n",
            "Iteration 3/50, Entropy: 265.5404, Loss: 0.00576066\n",
            "Iteration 3/50, Entropy: 265.5574, Loss: 0.00543055\n",
            "Iteration 3/50, Entropy: 265.5699, Loss: 0.00548551\n",
            "Iteration 3/50, Entropy: 265.5745, Loss: 0.00532124\n",
            "Iteration 3/50, Entropy: 265.5700, Loss: 0.00540359\n",
            "Iteration 4/50, Entropy: 265.5634, Loss: 0.00570538\n",
            "Iteration 4/50, Entropy: 265.5558, Loss: 0.00533508\n",
            "Iteration 4/50, Entropy: 265.5473, Loss: 0.00537638\n",
            "Iteration 4/50, Entropy: 265.5363, Loss: 0.00523933\n",
            "Iteration 4/50, Entropy: 265.5257, Loss: 0.00532992\n",
            "Iteration 5/50, Entropy: 265.5199, Loss: 0.00568435\n",
            "Iteration 5/50, Entropy: 265.5203, Loss: 0.00530318\n",
            "Iteration 5/50, Entropy: 265.5283, Loss: 0.00533648\n",
            "Iteration 5/50, Entropy: 265.5376, Loss: 0.00521601\n",
            "Iteration 5/50, Entropy: 265.5450, Loss: 0.00529442\n",
            "Iteration 6/50, Entropy: 265.5511, Loss: 0.00565496\n",
            "Iteration 6/50, Entropy: 265.5600, Loss: 0.00527663\n",
            "Iteration 6/50, Entropy: 265.5663, Loss: 0.00532497\n",
            "Iteration 6/50, Entropy: 265.5688, Loss: 0.00520318\n",
            "Iteration 6/50, Entropy: 265.5651, Loss: 0.00527454\n",
            "Iteration 7/50, Entropy: 265.5587, Loss: 0.00563667\n",
            "Iteration 7/50, Entropy: 265.5547, Loss: 0.00524719\n",
            "Iteration 7/50, Entropy: 265.5495, Loss: 0.00530834\n",
            "Iteration 7/50, Entropy: 265.5441, Loss: 0.00518272\n",
            "Iteration 7/50, Entropy: 265.5381, Loss: 0.00526247\n",
            "Iteration 8/50, Entropy: 265.5346, Loss: 0.00562209\n",
            "Iteration 8/50, Entropy: 265.5367, Loss: 0.00523425\n",
            "Iteration 8/50, Entropy: 265.5406, Loss: 0.00529612\n",
            "Iteration 8/50, Entropy: 265.5433, Loss: 0.00518142\n",
            "Iteration 8/50, Entropy: 265.5457, Loss: 0.00524712\n",
            "Iteration 9/50, Entropy: 265.5492, Loss: 0.00561681\n",
            "Iteration 9/50, Entropy: 265.5558, Loss: 0.00522413\n",
            "Iteration 9/50, Entropy: 265.5599, Loss: 0.00528363\n",
            "Iteration 9/50, Entropy: 265.5611, Loss: 0.00517043\n",
            "Iteration 9/50, Entropy: 265.5580, Loss: 0.00523353\n",
            "Iteration 10/50, Entropy: 265.5537, Loss: 0.00560455\n",
            "Iteration 10/50, Entropy: 265.5520, Loss: 0.00521324\n",
            "Iteration 10/50, Entropy: 265.5481, Loss: 0.00527488\n",
            "Iteration 10/50, Entropy: 265.5432, Loss: 0.00516283\n",
            "Iteration 10/50, Entropy: 265.5384, Loss: 0.00523326\n",
            "Iteration 11/50, Entropy: 265.5352, Loss: 0.00559968\n",
            "Iteration 11/50, Entropy: 265.5360, Loss: 0.00520290\n",
            "Iteration 11/50, Entropy: 265.5349, Loss: 0.00526351\n",
            "Iteration 11/50, Entropy: 265.5315, Loss: 0.00514526\n",
            "Iteration 11/50, Entropy: 265.5271, Loss: 0.00521834\n",
            "Iteration 12/50, Entropy: 265.5242, Loss: 0.00558459\n",
            "Iteration 12/50, Entropy: 265.5255, Loss: 0.00519301\n",
            "Iteration 12/50, Entropy: 265.5247, Loss: 0.00525430\n",
            "Iteration 12/50, Entropy: 265.5213, Loss: 0.00513512\n",
            "Iteration 12/50, Entropy: 265.5150, Loss: 0.00520558\n",
            "Iteration 13/50, Entropy: 265.5080, Loss: 0.00557555\n",
            "Iteration 13/50, Entropy: 265.5040, Loss: 0.00517322\n",
            "Iteration 13/50, Entropy: 265.4962, Loss: 0.00523228\n",
            "Iteration 13/50, Entropy: 265.4862, Loss: 0.00511298\n",
            "Iteration 13/50, Entropy: 265.4763, Loss: 0.00517717\n",
            "Iteration 14/50, Entropy: 265.4686, Loss: 0.00554730\n",
            "Iteration 14/50, Entropy: 265.4658, Loss: 0.00513768\n",
            "Iteration 14/50, Entropy: 265.4619, Loss: 0.00520635\n",
            "Iteration 14/50, Entropy: 265.4571, Loss: 0.00509234\n",
            "Iteration 14/50, Entropy: 265.4519, Loss: 0.00515003\n",
            "Iteration 15/50, Entropy: 265.4492, Loss: 0.00552465\n",
            "Iteration 15/50, Entropy: 265.4517, Loss: 0.00511780\n",
            "Iteration 15/50, Entropy: 265.4527, Loss: 0.00518694\n",
            "Iteration 15/50, Entropy: 265.4528, Loss: 0.00507657\n",
            "Iteration 15/50, Entropy: 265.4511, Loss: 0.00514044\n",
            "Iteration 16/50, Entropy: 265.4483, Loss: 0.00551337\n",
            "Iteration 16/50, Entropy: 265.4469, Loss: 0.00510820\n",
            "Iteration 16/50, Entropy: 265.4425, Loss: 0.00518118\n",
            "Iteration 16/50, Entropy: 265.4373, Loss: 0.00507668\n",
            "Iteration 16/50, Entropy: 265.4324, Loss: 0.00513853\n",
            "Iteration 17/50, Entropy: 265.4309, Loss: 0.00550510\n",
            "Iteration 17/50, Entropy: 265.4332, Loss: 0.00511268\n",
            "Iteration 17/50, Entropy: 265.4325, Loss: 0.00517898\n",
            "Iteration 17/50, Entropy: 265.4292, Loss: 0.00507485\n",
            "Iteration 17/50, Entropy: 265.4234, Loss: 0.00514212\n",
            "Iteration 18/50, Entropy: 265.4194, Loss: 0.00550436\n",
            "Iteration 18/50, Entropy: 265.4196, Loss: 0.00510980\n",
            "Iteration 18/50, Entropy: 265.4178, Loss: 0.00517456\n",
            "Iteration 18/50, Entropy: 265.4148, Loss: 0.00506518\n",
            "Iteration 18/50, Entropy: 265.4092, Loss: 0.00513556\n",
            "Iteration 19/50, Entropy: 265.4059, Loss: 0.00549568\n",
            "Iteration 19/50, Entropy: 265.4064, Loss: 0.00509768\n",
            "Iteration 19/50, Entropy: 265.4053, Loss: 0.00516500\n",
            "Iteration 19/50, Entropy: 265.4034, Loss: 0.00506491\n",
            "Iteration 19/50, Entropy: 265.4007, Loss: 0.00513779\n",
            "Iteration 20/50, Entropy: 265.4012, Loss: 0.00549254\n",
            "Iteration 20/50, Entropy: 265.4056, Loss: 0.00509737\n",
            "Iteration 20/50, Entropy: 265.4065, Loss: 0.00516509\n",
            "Iteration 20/50, Entropy: 265.4048, Loss: 0.00506098\n",
            "Iteration 20/50, Entropy: 265.4002, Loss: 0.00513508\n",
            "Iteration 21/50, Entropy: 265.3981, Loss: 0.00549396\n",
            "Iteration 21/50, Entropy: 265.4006, Loss: 0.00509528\n",
            "Iteration 21/50, Entropy: 265.4004, Loss: 0.00516188\n",
            "Iteration 21/50, Entropy: 265.3986, Loss: 0.00506013\n",
            "Iteration 21/50, Entropy: 265.3940, Loss: 0.00513891\n",
            "Iteration 22/50, Entropy: 265.3924, Loss: 0.00549141\n",
            "Iteration 22/50, Entropy: 265.3952, Loss: 0.00509444\n",
            "Iteration 22/50, Entropy: 265.3964, Loss: 0.00516135\n",
            "Iteration 22/50, Entropy: 265.3964, Loss: 0.00505978\n",
            "Iteration 22/50, Entropy: 265.3944, Loss: 0.00513591\n",
            "Iteration 23/50, Entropy: 265.3951, Loss: 0.00549178\n",
            "Iteration 23/50, Entropy: 265.3994, Loss: 0.00509416\n",
            "Iteration 23/50, Entropy: 265.4006, Loss: 0.00516173\n",
            "Iteration 23/50, Entropy: 265.3995, Loss: 0.00506000\n",
            "Iteration 23/50, Entropy: 265.3953, Loss: 0.00513490\n",
            "Iteration 24/50, Entropy: 265.3938, Loss: 0.00549154\n",
            "Iteration 24/50, Entropy: 265.3966, Loss: 0.00509295\n",
            "Iteration 24/50, Entropy: 265.3972, Loss: 0.00516014\n",
            "Iteration 24/50, Entropy: 265.3962, Loss: 0.00506075\n",
            "Iteration 24/50, Entropy: 265.3932, Loss: 0.00513621\n",
            "Iteration 25/50, Entropy: 265.3928, Loss: 0.00549125\n",
            "Iteration 25/50, Entropy: 265.3962, Loss: 0.00509273\n",
            "Iteration 25/50, Entropy: 265.3970, Loss: 0.00515981\n",
            "Iteration 25/50, Entropy: 265.3962, Loss: 0.00506042\n",
            "Iteration 25/50, Entropy: 265.3929, Loss: 0.00513428\n",
            "Iteration 26/50, Entropy: 265.3926, Loss: 0.00549151\n",
            "Iteration 26/50, Entropy: 265.3960, Loss: 0.00509246\n",
            "Iteration 26/50, Entropy: 265.3966, Loss: 0.00515957\n",
            "Iteration 26/50, Entropy: 265.3953, Loss: 0.00506104\n",
            "Iteration 26/50, Entropy: 265.3914, Loss: 0.00513449\n",
            "Iteration 27/50, Entropy: 265.3908, Loss: 0.00549086\n",
            "Iteration 27/50, Entropy: 265.3937, Loss: 0.00509176\n",
            "Iteration 27/50, Entropy: 265.3945, Loss: 0.00515871\n",
            "Iteration 27/50, Entropy: 265.3935, Loss: 0.00506177\n",
            "Iteration 27/50, Entropy: 265.3900, Loss: 0.00513481\n",
            "Iteration 28/50, Entropy: 265.3898, Loss: 0.00549074\n",
            "Iteration 28/50, Entropy: 265.3930, Loss: 0.00509143\n",
            "Iteration 28/50, Entropy: 265.3940, Loss: 0.00515911\n",
            "Iteration 28/50, Entropy: 265.3931, Loss: 0.00506160\n",
            "Iteration 28/50, Entropy: 265.3896, Loss: 0.00513419\n",
            "Iteration 29/50, Entropy: 265.3895, Loss: 0.00549096\n",
            "Iteration 29/50, Entropy: 265.3928, Loss: 0.00509132\n",
            "Iteration 29/50, Entropy: 265.3936, Loss: 0.00515883\n",
            "Iteration 29/50, Entropy: 265.3924, Loss: 0.00506187\n",
            "Iteration 29/50, Entropy: 265.3886, Loss: 0.00513446\n",
            "Iteration 30/50, Entropy: 265.3884, Loss: 0.00549089\n",
            "Iteration 30/50, Entropy: 265.3918, Loss: 0.00509110\n",
            "Iteration 30/50, Entropy: 265.3929, Loss: 0.00515876\n",
            "Iteration 30/50, Entropy: 265.3921, Loss: 0.00506195\n",
            "Iteration 30/50, Entropy: 265.3888, Loss: 0.00513431\n",
            "Iteration 31/50, Entropy: 265.3889, Loss: 0.00549106\n",
            "Iteration 31/50, Entropy: 265.3924, Loss: 0.00509134\n",
            "Iteration 31/50, Entropy: 265.3936, Loss: 0.00515904\n",
            "Iteration 31/50, Entropy: 265.3927, Loss: 0.00506188\n",
            "Iteration 31/50, Entropy: 265.3891, Loss: 0.00513406\n",
            "Iteration 32/50, Entropy: 265.3889, Loss: 0.00549114\n",
            "Iteration 32/50, Entropy: 265.3922, Loss: 0.00509116\n",
            "Iteration 32/50, Entropy: 265.3933, Loss: 0.00515891\n",
            "Iteration 32/50, Entropy: 265.3924, Loss: 0.00506197\n",
            "Iteration 32/50, Entropy: 265.3891, Loss: 0.00513434\n",
            "Iteration 33/50, Entropy: 265.3893, Loss: 0.00549110\n",
            "Iteration 33/50, Entropy: 265.3929, Loss: 0.00509127\n",
            "Iteration 33/50, Entropy: 265.3941, Loss: 0.00515890\n",
            "Iteration 33/50, Entropy: 265.3932, Loss: 0.00506198\n",
            "Iteration 33/50, Entropy: 265.3897, Loss: 0.00513408\n",
            "Iteration 34/50, Entropy: 265.3896, Loss: 0.00549113\n",
            "Iteration 34/50, Entropy: 265.3930, Loss: 0.00509129\n",
            "Iteration 34/50, Entropy: 265.3940, Loss: 0.00515898\n",
            "Iteration 34/50, Entropy: 265.3932, Loss: 0.00506201\n",
            "Iteration 34/50, Entropy: 265.3897, Loss: 0.00513427\n",
            "Iteration 35/50, Entropy: 265.3897, Loss: 0.00549104\n",
            "Iteration 35/50, Entropy: 265.3933, Loss: 0.00509136\n",
            "Iteration 35/50, Entropy: 265.3945, Loss: 0.00515887\n",
            "Iteration 35/50, Entropy: 265.3936, Loss: 0.00506207\n",
            "Iteration 35/50, Entropy: 265.3900, Loss: 0.00513426\n",
            "Iteration 36/50, Entropy: 265.3900, Loss: 0.00549106\n",
            "Iteration 36/50, Entropy: 265.3933, Loss: 0.00509135\n",
            "Iteration 36/50, Entropy: 265.3944, Loss: 0.00515900\n",
            "Iteration 36/50, Entropy: 265.3936, Loss: 0.00506200\n",
            "Iteration 36/50, Entropy: 265.3902, Loss: 0.00513422\n",
            "Iteration 37/50, Entropy: 265.3901, Loss: 0.00549105\n",
            "Iteration 37/50, Entropy: 265.3937, Loss: 0.00509147\n",
            "Iteration 37/50, Entropy: 265.3948, Loss: 0.00515893\n",
            "Iteration 37/50, Entropy: 265.3939, Loss: 0.00506207\n",
            "Iteration 37/50, Entropy: 265.3903, Loss: 0.00513430\n",
            "Iteration 38/50, Entropy: 265.3901, Loss: 0.00549103\n",
            "Iteration 38/50, Entropy: 265.3935, Loss: 0.00509141\n",
            "Iteration 38/50, Entropy: 265.3947, Loss: 0.00515902\n",
            "Iteration 38/50, Entropy: 265.3939, Loss: 0.00506205\n",
            "Iteration 38/50, Entropy: 265.3904, Loss: 0.00513423\n",
            "Iteration 39/50, Entropy: 265.3905, Loss: 0.00549107\n",
            "Iteration 39/50, Entropy: 265.3940, Loss: 0.00509154\n",
            "Iteration 39/50, Entropy: 265.3951, Loss: 0.00515901\n",
            "Iteration 39/50, Entropy: 265.3942, Loss: 0.00506211\n",
            "Iteration 39/50, Entropy: 265.3906, Loss: 0.00513423\n",
            "Iteration 40/50, Entropy: 265.3904, Loss: 0.00549107\n",
            "Iteration 40/50, Entropy: 265.3938, Loss: 0.00509145\n",
            "Iteration 40/50, Entropy: 265.3950, Loss: 0.00515901\n",
            "Iteration 40/50, Entropy: 265.3942, Loss: 0.00506216\n",
            "Iteration 40/50, Entropy: 265.3907, Loss: 0.00513421\n",
            "Iteration 41/50, Entropy: 265.3908, Loss: 0.00549109\n",
            "Iteration 41/50, Entropy: 265.3943, Loss: 0.00509155\n",
            "Iteration 41/50, Entropy: 265.3954, Loss: 0.00515901\n",
            "Iteration 41/50, Entropy: 265.3946, Loss: 0.00506222\n",
            "Iteration 41/50, Entropy: 265.3909, Loss: 0.00513416\n",
            "Iteration 42/50, Entropy: 265.3908, Loss: 0.00549110\n",
            "Iteration 42/50, Entropy: 265.3942, Loss: 0.00509149\n",
            "Iteration 42/50, Entropy: 265.3954, Loss: 0.00515902\n",
            "Iteration 42/50, Entropy: 265.3946, Loss: 0.00506228\n",
            "Iteration 42/50, Entropy: 265.3911, Loss: 0.00513416\n",
            "Iteration 43/50, Entropy: 265.3911, Loss: 0.00549111\n",
            "Iteration 43/50, Entropy: 265.3947, Loss: 0.00509155\n",
            "Iteration 43/50, Entropy: 265.3958, Loss: 0.00515901\n",
            "Iteration 43/50, Entropy: 265.3950, Loss: 0.00506234\n",
            "Iteration 43/50, Entropy: 265.3913, Loss: 0.00513409\n",
            "Iteration 44/50, Entropy: 265.3913, Loss: 0.00549113\n",
            "Iteration 44/50, Entropy: 265.3947, Loss: 0.00509153\n",
            "Iteration 44/50, Entropy: 265.3958, Loss: 0.00515901\n",
            "Iteration 44/50, Entropy: 265.3951, Loss: 0.00506239\n",
            "Iteration 44/50, Entropy: 265.3917, Loss: 0.00513406\n",
            "Iteration 45/50, Entropy: 265.3917, Loss: 0.00549113\n",
            "Iteration 45/50, Entropy: 265.3952, Loss: 0.00509158\n",
            "Iteration 45/50, Entropy: 265.3963, Loss: 0.00515900\n",
            "Iteration 45/50, Entropy: 265.3955, Loss: 0.00506246\n",
            "Iteration 45/50, Entropy: 265.3919, Loss: 0.00513400\n",
            "Iteration 46/50, Entropy: 265.3919, Loss: 0.00549116\n",
            "Iteration 46/50, Entropy: 265.3954, Loss: 0.00509157\n",
            "Iteration 46/50, Entropy: 265.3965, Loss: 0.00515899\n",
            "Iteration 46/50, Entropy: 265.3958, Loss: 0.00506253\n",
            "Iteration 46/50, Entropy: 265.3923, Loss: 0.00513394\n",
            "Iteration 47/50, Entropy: 265.3924, Loss: 0.00549117\n",
            "Iteration 47/50, Entropy: 265.3959, Loss: 0.00509161\n",
            "Iteration 47/50, Entropy: 265.3971, Loss: 0.00515897\n",
            "Iteration 47/50, Entropy: 265.3963, Loss: 0.00506260\n",
            "Iteration 47/50, Entropy: 265.3927, Loss: 0.00513387\n",
            "Iteration 48/50, Entropy: 265.3927, Loss: 0.00549118\n",
            "Iteration 48/50, Entropy: 265.3961, Loss: 0.00509160\n",
            "Iteration 48/50, Entropy: 265.3973, Loss: 0.00515895\n",
            "Iteration 48/50, Entropy: 265.3966, Loss: 0.00506267\n",
            "Iteration 48/50, Entropy: 265.3932, Loss: 0.00513382\n",
            "Iteration 49/50, Entropy: 265.3932, Loss: 0.00549119\n",
            "Iteration 49/50, Entropy: 265.3967, Loss: 0.00509162\n",
            "Iteration 49/50, Entropy: 265.3978, Loss: 0.00515894\n",
            "Iteration 49/50, Entropy: 265.3971, Loss: 0.00506275\n",
            "Iteration 49/50, Entropy: 265.3935, Loss: 0.00513375\n",
            "Iteration 50/50, Entropy: 265.3935, Loss: 0.00549122\n",
            "Iteration 50/50, Entropy: 265.3970, Loss: 0.00509161\n",
            "Iteration 50/50, Entropy: 265.3981, Loss: 0.00515891\n",
            "Iteration 50/50, Entropy: 265.3974, Loss: 0.00506282\n",
            "Iteration 50/50, Entropy: 265.3940, Loss: 0.00513372\n",
            "weight diff tensor(0.0010, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.v_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/50, Entropy: 267.5085, Loss: 0.00018565\n",
            "Iteration 1/50, Entropy: 267.4770, Loss: 0.00017464\n",
            "Iteration 1/50, Entropy: 267.4366, Loss: 0.00017373\n",
            "Iteration 1/50, Entropy: 267.4251, Loss: 0.00017154\n",
            "Iteration 1/50, Entropy: 267.4825, Loss: 0.00017023\n",
            "Iteration 2/50, Entropy: 267.5173, Loss: 0.00017923\n",
            "Iteration 2/50, Entropy: 267.5829, Loss: 0.00016646\n",
            "Iteration 2/50, Entropy: 267.6385, Loss: 0.00016587\n",
            "Iteration 2/50, Entropy: 267.6609, Loss: 0.00016118\n",
            "Iteration 2/50, Entropy: 267.7124, Loss: 0.00016114\n",
            "Iteration 3/50, Entropy: 267.7929, Loss: 0.00017088\n",
            "Iteration 3/50, Entropy: 267.8635, Loss: 0.00016263\n",
            "Iteration 3/50, Entropy: 267.9351, Loss: 0.00016198\n",
            "Iteration 3/50, Entropy: 268.0007, Loss: 0.00015583\n",
            "Iteration 3/50, Entropy: 268.0453, Loss: 0.00015770\n",
            "Iteration 4/50, Entropy: 268.0824, Loss: 0.00016435\n",
            "Iteration 4/50, Entropy: 268.1231, Loss: 0.00015561\n",
            "Iteration 4/50, Entropy: 268.1514, Loss: 0.00015819\n",
            "Iteration 4/50, Entropy: 268.1799, Loss: 0.00015192\n",
            "Iteration 4/50, Entropy: 268.2219, Loss: 0.00015316\n",
            "Iteration 5/50, Entropy: 268.2535, Loss: 0.00016145\n",
            "Iteration 5/50, Entropy: 268.2625, Loss: 0.00015242\n",
            "Iteration 5/50, Entropy: 268.2709, Loss: 0.00015481\n",
            "Iteration 5/50, Entropy: 268.2909, Loss: 0.00014806\n",
            "Iteration 5/50, Entropy: 268.3174, Loss: 0.00015055\n",
            "Iteration 6/50, Entropy: 268.3456, Loss: 0.00015863\n",
            "Iteration 6/50, Entropy: 268.3684, Loss: 0.00014802\n",
            "Iteration 6/50, Entropy: 268.4042, Loss: 0.00014910\n",
            "Iteration 6/50, Entropy: 268.4287, Loss: 0.00014444\n",
            "Iteration 6/50, Entropy: 268.4391, Loss: 0.00014797\n",
            "Iteration 7/50, Entropy: 268.4515, Loss: 0.00015639\n",
            "Iteration 7/50, Entropy: 268.4698, Loss: 0.00014538\n",
            "Iteration 7/50, Entropy: 268.4885, Loss: 0.00014670\n",
            "Iteration 7/50, Entropy: 268.4932, Loss: 0.00014279\n",
            "Iteration 7/50, Entropy: 268.4713, Loss: 0.00014524\n",
            "Iteration 8/50, Entropy: 268.4442, Loss: 0.00015337\n",
            "Iteration 8/50, Entropy: 268.4191, Loss: 0.00014339\n",
            "Iteration 8/50, Entropy: 268.4084, Loss: 0.00014511\n",
            "Iteration 8/50, Entropy: 268.4135, Loss: 0.00014097\n",
            "Iteration 8/50, Entropy: 268.4290, Loss: 0.00014275\n",
            "Iteration 9/50, Entropy: 268.4492, Loss: 0.00015150\n",
            "Iteration 9/50, Entropy: 268.4595, Loss: 0.00014157\n",
            "Iteration 9/50, Entropy: 268.4556, Loss: 0.00014264\n",
            "Iteration 9/50, Entropy: 268.4400, Loss: 0.00013958\n",
            "Iteration 9/50, Entropy: 268.4175, Loss: 0.00014205\n",
            "Iteration 10/50, Entropy: 268.4211, Loss: 0.00015053\n",
            "Iteration 10/50, Entropy: 268.4313, Loss: 0.00014069\n",
            "Iteration 10/50, Entropy: 268.4413, Loss: 0.00014250\n",
            "Iteration 10/50, Entropy: 268.4300, Loss: 0.00013897\n",
            "Iteration 10/50, Entropy: 268.4099, Loss: 0.00014101\n",
            "Iteration 11/50, Entropy: 268.4098, Loss: 0.00014883\n",
            "Iteration 11/50, Entropy: 268.3988, Loss: 0.00013974\n",
            "Iteration 11/50, Entropy: 268.3924, Loss: 0.00014072\n",
            "Iteration 11/50, Entropy: 268.3924, Loss: 0.00013723\n",
            "Iteration 11/50, Entropy: 268.4046, Loss: 0.00013979\n",
            "Iteration 12/50, Entropy: 268.4273, Loss: 0.00014865\n",
            "Iteration 12/50, Entropy: 268.4228, Loss: 0.00013862\n",
            "Iteration 12/50, Entropy: 268.4063, Loss: 0.00014053\n",
            "Iteration 12/50, Entropy: 268.3896, Loss: 0.00013740\n",
            "Iteration 12/50, Entropy: 268.3840, Loss: 0.00013949\n",
            "Iteration 13/50, Entropy: 268.4131, Loss: 0.00014666\n",
            "Iteration 13/50, Entropy: 268.4539, Loss: 0.00013759\n",
            "Iteration 13/50, Entropy: 268.4928, Loss: 0.00013830\n",
            "Iteration 13/50, Entropy: 268.5281, Loss: 0.00013496\n",
            "Iteration 13/50, Entropy: 268.5655, Loss: 0.00013757\n",
            "Iteration 14/50, Entropy: 268.5845, Loss: 0.00014669\n",
            "Iteration 14/50, Entropy: 268.5832, Loss: 0.00013760\n",
            "Iteration 14/50, Entropy: 268.5528, Loss: 0.00013877\n",
            "Iteration 14/50, Entropy: 268.5242, Loss: 0.00013545\n",
            "Iteration 14/50, Entropy: 268.5249, Loss: 0.00013718\n",
            "Iteration 15/50, Entropy: 268.5375, Loss: 0.00014572\n",
            "Iteration 15/50, Entropy: 268.5420, Loss: 0.00013555\n",
            "Iteration 15/50, Entropy: 268.5375, Loss: 0.00013707\n",
            "Iteration 15/50, Entropy: 268.5217, Loss: 0.00013443\n",
            "Iteration 15/50, Entropy: 268.5137, Loss: 0.00013649\n",
            "Iteration 16/50, Entropy: 268.5170, Loss: 0.00014549\n",
            "Iteration 16/50, Entropy: 268.5139, Loss: 0.00013601\n",
            "Iteration 16/50, Entropy: 268.5038, Loss: 0.00013686\n",
            "Iteration 16/50, Entropy: 268.4997, Loss: 0.00013332\n",
            "Iteration 16/50, Entropy: 268.4964, Loss: 0.00013590\n",
            "Iteration 17/50, Entropy: 268.4958, Loss: 0.00014455\n",
            "Iteration 17/50, Entropy: 268.4996, Loss: 0.00013578\n",
            "Iteration 17/50, Entropy: 268.5154, Loss: 0.00013628\n",
            "Iteration 17/50, Entropy: 268.5273, Loss: 0.00013319\n",
            "Iteration 17/50, Entropy: 268.5355, Loss: 0.00013539\n",
            "Iteration 18/50, Entropy: 268.5382, Loss: 0.00014462\n",
            "Iteration 18/50, Entropy: 268.5302, Loss: 0.00013502\n",
            "Iteration 18/50, Entropy: 268.5171, Loss: 0.00013646\n",
            "Iteration 18/50, Entropy: 268.5175, Loss: 0.00013204\n",
            "Iteration 18/50, Entropy: 268.5335, Loss: 0.00013493\n",
            "Iteration 19/50, Entropy: 268.5476, Loss: 0.00014460\n",
            "Iteration 19/50, Entropy: 268.5463, Loss: 0.00013400\n",
            "Iteration 19/50, Entropy: 268.5415, Loss: 0.00013538\n",
            "Iteration 19/50, Entropy: 268.5406, Loss: 0.00013144\n",
            "Iteration 19/50, Entropy: 268.5479, Loss: 0.00013378\n",
            "Iteration 20/50, Entropy: 268.5557, Loss: 0.00014395\n",
            "Iteration 20/50, Entropy: 268.5498, Loss: 0.00013361\n",
            "Iteration 20/50, Entropy: 268.5620, Loss: 0.00013527\n",
            "Iteration 20/50, Entropy: 268.5880, Loss: 0.00013111\n",
            "Iteration 20/50, Entropy: 268.6130, Loss: 0.00013416\n",
            "Iteration 21/50, Entropy: 268.6185, Loss: 0.00014295\n",
            "Iteration 21/50, Entropy: 268.6176, Loss: 0.00013325\n",
            "Iteration 21/50, Entropy: 268.6075, Loss: 0.00013471\n",
            "Iteration 21/50, Entropy: 268.6109, Loss: 0.00013126\n",
            "Iteration 21/50, Entropy: 268.6227, Loss: 0.00013357\n",
            "Iteration 22/50, Entropy: 268.6263, Loss: 0.00014277\n",
            "Iteration 22/50, Entropy: 268.6129, Loss: 0.00013324\n",
            "Iteration 22/50, Entropy: 268.5958, Loss: 0.00013475\n",
            "Iteration 22/50, Entropy: 268.5916, Loss: 0.00013105\n",
            "Iteration 22/50, Entropy: 268.6031, Loss: 0.00013347\n",
            "Iteration 23/50, Entropy: 268.6238, Loss: 0.00014250\n",
            "Iteration 23/50, Entropy: 268.6353, Loss: 0.00013283\n",
            "Iteration 23/50, Entropy: 268.6386, Loss: 0.00013430\n",
            "Iteration 23/50, Entropy: 268.6414, Loss: 0.00013027\n",
            "Iteration 23/50, Entropy: 268.6450, Loss: 0.00013346\n",
            "Iteration 24/50, Entropy: 268.6484, Loss: 0.00014225\n",
            "Iteration 24/50, Entropy: 268.6320, Loss: 0.00013288\n",
            "Iteration 24/50, Entropy: 268.6063, Loss: 0.00013448\n",
            "Iteration 24/50, Entropy: 268.5970, Loss: 0.00013112\n",
            "Iteration 24/50, Entropy: 268.6062, Loss: 0.00013344\n",
            "Iteration 25/50, Entropy: 268.6199, Loss: 0.00014218\n",
            "Iteration 25/50, Entropy: 268.6360, Loss: 0.00013336\n",
            "Iteration 25/50, Entropy: 268.6364, Loss: 0.00013437\n",
            "Iteration 25/50, Entropy: 268.6390, Loss: 0.00013014\n",
            "Iteration 25/50, Entropy: 268.6333, Loss: 0.00013313\n",
            "Iteration 26/50, Entropy: 268.6241, Loss: 0.00014212\n",
            "Iteration 26/50, Entropy: 268.6056, Loss: 0.00013247\n",
            "Iteration 26/50, Entropy: 268.5886, Loss: 0.00013406\n",
            "Iteration 26/50, Entropy: 268.5854, Loss: 0.00013092\n",
            "Iteration 26/50, Entropy: 268.5950, Loss: 0.00013318\n",
            "Iteration 27/50, Entropy: 268.6122, Loss: 0.00014216\n",
            "Iteration 27/50, Entropy: 268.6215, Loss: 0.00013325\n",
            "Iteration 27/50, Entropy: 268.6192, Loss: 0.00013436\n",
            "Iteration 27/50, Entropy: 268.6179, Loss: 0.00013075\n",
            "Iteration 27/50, Entropy: 268.6142, Loss: 0.00013345\n",
            "Iteration 28/50, Entropy: 268.6108, Loss: 0.00014178\n",
            "Iteration 28/50, Entropy: 268.5968, Loss: 0.00013290\n",
            "Iteration 28/50, Entropy: 268.5858, Loss: 0.00013423\n",
            "Iteration 28/50, Entropy: 268.5903, Loss: 0.00013055\n",
            "Iteration 28/50, Entropy: 268.6031, Loss: 0.00013329\n",
            "Iteration 29/50, Entropy: 268.6158, Loss: 0.00014168\n",
            "Iteration 29/50, Entropy: 268.6189, Loss: 0.00013233\n",
            "Iteration 29/50, Entropy: 268.6212, Loss: 0.00013404\n",
            "Iteration 29/50, Entropy: 268.6331, Loss: 0.00013025\n",
            "Iteration 29/50, Entropy: 268.6395, Loss: 0.00013325\n",
            "Iteration 30/50, Entropy: 268.6354, Loss: 0.00014193\n",
            "Iteration 30/50, Entropy: 268.6131, Loss: 0.00013274\n",
            "Iteration 30/50, Entropy: 268.5913, Loss: 0.00013392\n",
            "Iteration 30/50, Entropy: 268.5938, Loss: 0.00013040\n",
            "Iteration 30/50, Entropy: 268.6054, Loss: 0.00013339\n",
            "Iteration 31/50, Entropy: 268.6157, Loss: 0.00014179\n",
            "Iteration 31/50, Entropy: 268.6143, Loss: 0.00013240\n",
            "Iteration 31/50, Entropy: 268.6061, Loss: 0.00013387\n",
            "Iteration 31/50, Entropy: 268.6108, Loss: 0.00013029\n",
            "Iteration 31/50, Entropy: 268.6225, Loss: 0.00013288\n",
            "Iteration 32/50, Entropy: 268.6290, Loss: 0.00014217\n",
            "Iteration 32/50, Entropy: 268.6117, Loss: 0.00013254\n",
            "Iteration 32/50, Entropy: 268.5900, Loss: 0.00013401\n",
            "Iteration 32/50, Entropy: 268.5867, Loss: 0.00013037\n",
            "Iteration 32/50, Entropy: 268.5952, Loss: 0.00013301\n",
            "Iteration 33/50, Entropy: 268.6126, Loss: 0.00014169\n",
            "Iteration 33/50, Entropy: 268.6180, Loss: 0.00013254\n",
            "Iteration 33/50, Entropy: 268.6154, Loss: 0.00013387\n",
            "Iteration 33/50, Entropy: 268.6241, Loss: 0.00013009\n",
            "Iteration 33/50, Entropy: 268.6247, Loss: 0.00013316\n",
            "Iteration 34/50, Entropy: 268.6179, Loss: 0.00014209\n",
            "Iteration 34/50, Entropy: 268.5986, Loss: 0.00013247\n",
            "Iteration 34/50, Entropy: 268.5826, Loss: 0.00013366\n",
            "Iteration 34/50, Entropy: 268.5859, Loss: 0.00013046\n",
            "Iteration 34/50, Entropy: 268.5959, Loss: 0.00013297\n",
            "Iteration 35/50, Entropy: 268.6060, Loss: 0.00014144\n",
            "Iteration 35/50, Entropy: 268.6102, Loss: 0.00013276\n",
            "Iteration 35/50, Entropy: 268.6098, Loss: 0.00013387\n",
            "Iteration 35/50, Entropy: 268.6171, Loss: 0.00013014\n",
            "Iteration 35/50, Entropy: 268.6172, Loss: 0.00013340\n",
            "Iteration 36/50, Entropy: 268.6114, Loss: 0.00014191\n",
            "Iteration 36/50, Entropy: 268.6013, Loss: 0.00013256\n",
            "Iteration 36/50, Entropy: 268.5886, Loss: 0.00013398\n",
            "Iteration 36/50, Entropy: 268.5852, Loss: 0.00013030\n",
            "Iteration 36/50, Entropy: 268.5893, Loss: 0.00013311\n",
            "Iteration 37/50, Entropy: 268.5998, Loss: 0.00014161\n",
            "Iteration 37/50, Entropy: 268.6110, Loss: 0.00013233\n",
            "Iteration 37/50, Entropy: 268.6148, Loss: 0.00013387\n",
            "Iteration 37/50, Entropy: 268.6222, Loss: 0.00013032\n",
            "Iteration 37/50, Entropy: 268.6185, Loss: 0.00013283\n",
            "Iteration 38/50, Entropy: 268.6108, Loss: 0.00014172\n",
            "Iteration 38/50, Entropy: 268.6031, Loss: 0.00013237\n",
            "Iteration 38/50, Entropy: 268.5905, Loss: 0.00013387\n",
            "Iteration 38/50, Entropy: 268.5917, Loss: 0.00013018\n",
            "Iteration 38/50, Entropy: 268.5967, Loss: 0.00013302\n",
            "Iteration 39/50, Entropy: 268.6003, Loss: 0.00014153\n",
            "Iteration 39/50, Entropy: 268.6024, Loss: 0.00013233\n",
            "Iteration 39/50, Entropy: 268.6038, Loss: 0.00013353\n",
            "Iteration 39/50, Entropy: 268.6082, Loss: 0.00012989\n",
            "Iteration 39/50, Entropy: 268.6158, Loss: 0.00013295\n",
            "Iteration 40/50, Entropy: 268.6158, Loss: 0.00014157\n",
            "Iteration 40/50, Entropy: 268.6012, Loss: 0.00013226\n",
            "Iteration 40/50, Entropy: 268.5864, Loss: 0.00013357\n",
            "Iteration 40/50, Entropy: 268.5868, Loss: 0.00013026\n",
            "Iteration 40/50, Entropy: 268.5940, Loss: 0.00013300\n",
            "Iteration 41/50, Entropy: 268.6032, Loss: 0.00014161\n",
            "Iteration 41/50, Entropy: 268.6031, Loss: 0.00013247\n",
            "Iteration 41/50, Entropy: 268.5985, Loss: 0.00013384\n",
            "Iteration 41/50, Entropy: 268.6063, Loss: 0.00013041\n",
            "Iteration 41/50, Entropy: 268.6075, Loss: 0.00013324\n",
            "Iteration 42/50, Entropy: 268.6038, Loss: 0.00014216\n",
            "Iteration 42/50, Entropy: 268.5954, Loss: 0.00013265\n",
            "Iteration 42/50, Entropy: 268.5834, Loss: 0.00013363\n",
            "Iteration 42/50, Entropy: 268.5866, Loss: 0.00013028\n",
            "Iteration 42/50, Entropy: 268.5948, Loss: 0.00013294\n",
            "Iteration 43/50, Entropy: 268.6046, Loss: 0.00014134\n",
            "Iteration 43/50, Entropy: 268.6036, Loss: 0.00013265\n",
            "Iteration 43/50, Entropy: 268.5978, Loss: 0.00013366\n",
            "Iteration 43/50, Entropy: 268.6033, Loss: 0.00013007\n",
            "Iteration 43/50, Entropy: 268.6088, Loss: 0.00013294\n",
            "Iteration 44/50, Entropy: 268.6096, Loss: 0.00014182\n",
            "Iteration 44/50, Entropy: 268.5971, Loss: 0.00013241\n",
            "Iteration 44/50, Entropy: 268.5855, Loss: 0.00013350\n",
            "Iteration 44/50, Entropy: 268.5905, Loss: 0.00013017\n",
            "Iteration 44/50, Entropy: 268.6018, Loss: 0.00013289\n",
            "Iteration 45/50, Entropy: 268.6131, Loss: 0.00014131\n",
            "Iteration 45/50, Entropy: 268.6109, Loss: 0.00013251\n",
            "Iteration 45/50, Entropy: 268.6017, Loss: 0.00013386\n",
            "Iteration 45/50, Entropy: 268.6050, Loss: 0.00013018\n",
            "Iteration 45/50, Entropy: 268.6091, Loss: 0.00013337\n",
            "Iteration 46/50, Entropy: 268.6069, Loss: 0.00014157\n",
            "Iteration 46/50, Entropy: 268.5952, Loss: 0.00013254\n",
            "Iteration 46/50, Entropy: 268.5851, Loss: 0.00013385\n",
            "Iteration 46/50, Entropy: 268.5898, Loss: 0.00013027\n",
            "Iteration 46/50, Entropy: 268.5976, Loss: 0.00013299\n",
            "Iteration 47/50, Entropy: 268.6049, Loss: 0.00014142\n",
            "Iteration 47/50, Entropy: 268.6053, Loss: 0.00013226\n",
            "Iteration 47/50, Entropy: 268.6039, Loss: 0.00013368\n",
            "Iteration 47/50, Entropy: 268.6113, Loss: 0.00012976\n",
            "Iteration 47/50, Entropy: 268.6164, Loss: 0.00013276\n",
            "Iteration 48/50, Entropy: 268.6199, Loss: 0.00014122\n",
            "Iteration 48/50, Entropy: 268.6068, Loss: 0.00013221\n",
            "Iteration 48/50, Entropy: 268.5883, Loss: 0.00013359\n",
            "Iteration 48/50, Entropy: 268.5849, Loss: 0.00013017\n",
            "Iteration 48/50, Entropy: 268.5938, Loss: 0.00013286\n",
            "Iteration 49/50, Entropy: 268.6104, Loss: 0.00014147\n",
            "Iteration 49/50, Entropy: 268.6158, Loss: 0.00013217\n",
            "Iteration 49/50, Entropy: 268.6116, Loss: 0.00013382\n",
            "Iteration 49/50, Entropy: 268.6126, Loss: 0.00012999\n",
            "Iteration 49/50, Entropy: 268.6131, Loss: 0.00013303\n",
            "Iteration 50/50, Entropy: 268.6142, Loss: 0.00014141\n",
            "Iteration 50/50, Entropy: 268.5996, Loss: 0.00013248\n",
            "Iteration 50/50, Entropy: 268.5856, Loss: 0.00013387\n",
            "Iteration 50/50, Entropy: 268.5848, Loss: 0.00013022\n",
            "Iteration 50/50, Entropy: 268.5875, Loss: 0.00013315\n",
            "weight diff tensor(3.0533e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.q_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/50, Entropy: 264.1232, Loss: 0.00609490\n",
            "Iteration 1/50, Entropy: 264.1527, Loss: 0.00562515\n",
            "Iteration 1/50, Entropy: 264.1859, Loss: 0.00562955\n",
            "Iteration 1/50, Entropy: 264.2297, Loss: 0.00551716\n",
            "Iteration 1/50, Entropy: 264.2953, Loss: 0.00553263\n",
            "Iteration 2/50, Entropy: 264.3599, Loss: 0.00581069\n",
            "Iteration 2/50, Entropy: 264.4126, Loss: 0.00539578\n",
            "Iteration 2/50, Entropy: 264.4612, Loss: 0.00545884\n",
            "Iteration 2/50, Entropy: 264.5037, Loss: 0.00536863\n",
            "Iteration 2/50, Entropy: 264.5379, Loss: 0.00543768\n",
            "Iteration 3/50, Entropy: 264.5564, Loss: 0.00572156\n",
            "Iteration 3/50, Entropy: 264.5619, Loss: 0.00533282\n",
            "Iteration 3/50, Entropy: 264.5572, Loss: 0.00539035\n",
            "Iteration 3/50, Entropy: 264.5482, Loss: 0.00529609\n",
            "Iteration 3/50, Entropy: 264.5343, Loss: 0.00535198\n",
            "Iteration 4/50, Entropy: 264.5257, Loss: 0.00562514\n",
            "Iteration 4/50, Entropy: 264.5205, Loss: 0.00523090\n",
            "Iteration 4/50, Entropy: 264.5240, Loss: 0.00532034\n",
            "Iteration 4/50, Entropy: 264.5356, Loss: 0.00523469\n",
            "Iteration 4/50, Entropy: 264.5499, Loss: 0.00528349\n",
            "Iteration 5/50, Entropy: 264.5691, Loss: 0.00558429\n",
            "Iteration 5/50, Entropy: 264.5862, Loss: 0.00517625\n",
            "Iteration 5/50, Entropy: 264.6010, Loss: 0.00526910\n",
            "Iteration 5/50, Entropy: 264.6088, Loss: 0.00518131\n",
            "Iteration 5/50, Entropy: 264.6100, Loss: 0.00526198\n",
            "Iteration 6/50, Entropy: 264.6086, Loss: 0.00556362\n",
            "Iteration 6/50, Entropy: 264.6088, Loss: 0.00516990\n",
            "Iteration 6/50, Entropy: 264.6070, Loss: 0.00524299\n",
            "Iteration 6/50, Entropy: 264.6091, Loss: 0.00515466\n",
            "Iteration 6/50, Entropy: 264.6156, Loss: 0.00523790\n",
            "Iteration 7/50, Entropy: 264.6274, Loss: 0.00551633\n",
            "Iteration 7/50, Entropy: 264.6425, Loss: 0.00514084\n",
            "Iteration 7/50, Entropy: 264.6576, Loss: 0.00522749\n",
            "Iteration 7/50, Entropy: 264.6745, Loss: 0.00512590\n",
            "Iteration 7/50, Entropy: 264.6892, Loss: 0.00520492\n",
            "Iteration 8/50, Entropy: 264.7054, Loss: 0.00547192\n",
            "Iteration 8/50, Entropy: 264.7187, Loss: 0.00510866\n",
            "Iteration 8/50, Entropy: 264.7286, Loss: 0.00519591\n",
            "Iteration 8/50, Entropy: 264.7356, Loss: 0.00509912\n",
            "Iteration 8/50, Entropy: 264.7397, Loss: 0.00517984\n",
            "Iteration 9/50, Entropy: 264.7460, Loss: 0.00544926\n",
            "Iteration 9/50, Entropy: 264.7506, Loss: 0.00509086\n",
            "Iteration 9/50, Entropy: 264.7525, Loss: 0.00517650\n",
            "Iteration 9/50, Entropy: 264.7551, Loss: 0.00508660\n",
            "Iteration 9/50, Entropy: 264.7571, Loss: 0.00516216\n",
            "Iteration 10/50, Entropy: 264.7620, Loss: 0.00544148\n",
            "Iteration 10/50, Entropy: 264.7648, Loss: 0.00508120\n",
            "Iteration 10/50, Entropy: 264.7645, Loss: 0.00517185\n",
            "Iteration 10/50, Entropy: 264.7632, Loss: 0.00507097\n",
            "Iteration 10/50, Entropy: 264.7597, Loss: 0.00514813\n",
            "Iteration 11/50, Entropy: 264.7596, Loss: 0.00542206\n",
            "Iteration 11/50, Entropy: 264.7576, Loss: 0.00506820\n",
            "Iteration 11/50, Entropy: 264.7537, Loss: 0.00515302\n",
            "Iteration 11/50, Entropy: 264.7506, Loss: 0.00506204\n",
            "Iteration 11/50, Entropy: 264.7475, Loss: 0.00514994\n",
            "Iteration 12/50, Entropy: 264.7484, Loss: 0.00541820\n",
            "Iteration 12/50, Entropy: 264.7470, Loss: 0.00505984\n",
            "Iteration 12/50, Entropy: 264.7439, Loss: 0.00514424\n",
            "Iteration 12/50, Entropy: 264.7411, Loss: 0.00505780\n",
            "Iteration 12/50, Entropy: 264.7383, Loss: 0.00514903\n",
            "Iteration 13/50, Entropy: 264.7417, Loss: 0.00541156\n",
            "Iteration 13/50, Entropy: 264.7445, Loss: 0.00505153\n",
            "Iteration 13/50, Entropy: 264.7463, Loss: 0.00514152\n",
            "Iteration 13/50, Entropy: 264.7489, Loss: 0.00505014\n",
            "Iteration 13/50, Entropy: 264.7513, Loss: 0.00514552\n",
            "Iteration 14/50, Entropy: 264.7583, Loss: 0.00540739\n",
            "Iteration 14/50, Entropy: 264.7621, Loss: 0.00504134\n",
            "Iteration 14/50, Entropy: 264.7622, Loss: 0.00513268\n",
            "Iteration 14/50, Entropy: 264.7611, Loss: 0.00503667\n",
            "Iteration 14/50, Entropy: 264.7593, Loss: 0.00514264\n",
            "Iteration 15/50, Entropy: 264.7628, Loss: 0.00541020\n",
            "Iteration 15/50, Entropy: 264.7639, Loss: 0.00503680\n",
            "Iteration 15/50, Entropy: 264.7625, Loss: 0.00512771\n",
            "Iteration 15/50, Entropy: 264.7606, Loss: 0.00502789\n",
            "Iteration 15/50, Entropy: 264.7583, Loss: 0.00513939\n",
            "Iteration 16/50, Entropy: 264.7619, Loss: 0.00540205\n",
            "Iteration 16/50, Entropy: 264.7646, Loss: 0.00503264\n",
            "Iteration 16/50, Entropy: 264.7660, Loss: 0.00512255\n",
            "Iteration 16/50, Entropy: 264.7679, Loss: 0.00501929\n",
            "Iteration 16/50, Entropy: 264.7691, Loss: 0.00513500\n",
            "Iteration 17/50, Entropy: 264.7762, Loss: 0.00539546\n",
            "Iteration 17/50, Entropy: 264.7813, Loss: 0.00502929\n",
            "Iteration 17/50, Entropy: 264.7843, Loss: 0.00511223\n",
            "Iteration 17/50, Entropy: 264.7870, Loss: 0.00500832\n",
            "Iteration 17/50, Entropy: 264.7882, Loss: 0.00512691\n",
            "Iteration 18/50, Entropy: 264.7944, Loss: 0.00539540\n",
            "Iteration 18/50, Entropy: 264.7985, Loss: 0.00502699\n",
            "Iteration 18/50, Entropy: 264.8004, Loss: 0.00510397\n",
            "Iteration 18/50, Entropy: 264.8013, Loss: 0.00500478\n",
            "Iteration 18/50, Entropy: 264.7997, Loss: 0.00511713\n",
            "Iteration 19/50, Entropy: 264.8035, Loss: 0.00539147\n",
            "Iteration 19/50, Entropy: 264.8054, Loss: 0.00501365\n",
            "Iteration 19/50, Entropy: 264.8060, Loss: 0.00510270\n",
            "Iteration 19/50, Entropy: 264.8065, Loss: 0.00500068\n",
            "Iteration 19/50, Entropy: 264.8057, Loss: 0.00509738\n",
            "Iteration 20/50, Entropy: 264.8112, Loss: 0.00536890\n",
            "Iteration 20/50, Entropy: 264.8148, Loss: 0.00499289\n",
            "Iteration 20/50, Entropy: 264.8170, Loss: 0.00509516\n",
            "Iteration 20/50, Entropy: 264.8183, Loss: 0.00499385\n",
            "Iteration 20/50, Entropy: 264.8183, Loss: 0.00508875\n",
            "Iteration 21/50, Entropy: 264.8238, Loss: 0.00535771\n",
            "Iteration 21/50, Entropy: 264.8270, Loss: 0.00498426\n",
            "Iteration 21/50, Entropy: 264.8281, Loss: 0.00508588\n",
            "Iteration 21/50, Entropy: 264.8281, Loss: 0.00498683\n",
            "Iteration 21/50, Entropy: 264.8270, Loss: 0.00509402\n",
            "Iteration 22/50, Entropy: 264.8315, Loss: 0.00535868\n",
            "Iteration 22/50, Entropy: 264.8341, Loss: 0.00498578\n",
            "Iteration 22/50, Entropy: 264.8350, Loss: 0.00507925\n",
            "Iteration 22/50, Entropy: 264.8338, Loss: 0.00498192\n",
            "Iteration 22/50, Entropy: 264.8311, Loss: 0.00508552\n",
            "Iteration 23/50, Entropy: 264.8341, Loss: 0.00535363\n",
            "Iteration 23/50, Entropy: 264.8353, Loss: 0.00497439\n",
            "Iteration 23/50, Entropy: 264.8357, Loss: 0.00507452\n",
            "Iteration 23/50, Entropy: 264.8356, Loss: 0.00498245\n",
            "Iteration 23/50, Entropy: 264.8343, Loss: 0.00507618\n",
            "Iteration 24/50, Entropy: 264.8392, Loss: 0.00534811\n",
            "Iteration 24/50, Entropy: 264.8422, Loss: 0.00497171\n",
            "Iteration 24/50, Entropy: 264.8437, Loss: 0.00507165\n",
            "Iteration 24/50, Entropy: 264.8433, Loss: 0.00498120\n",
            "Iteration 24/50, Entropy: 264.8403, Loss: 0.00507276\n",
            "Iteration 25/50, Entropy: 264.8433, Loss: 0.00534711\n",
            "Iteration 25/50, Entropy: 264.8437, Loss: 0.00496766\n",
            "Iteration 25/50, Entropy: 264.8431, Loss: 0.00506872\n",
            "Iteration 25/50, Entropy: 264.8416, Loss: 0.00497692\n",
            "Iteration 25/50, Entropy: 264.8382, Loss: 0.00507040\n",
            "Iteration 26/50, Entropy: 264.8412, Loss: 0.00534601\n",
            "Iteration 26/50, Entropy: 264.8423, Loss: 0.00496528\n",
            "Iteration 26/50, Entropy: 264.8429, Loss: 0.00506709\n",
            "Iteration 26/50, Entropy: 264.8418, Loss: 0.00497446\n",
            "Iteration 26/50, Entropy: 264.8383, Loss: 0.00506735\n",
            "Iteration 27/50, Entropy: 264.8414, Loss: 0.00534607\n",
            "Iteration 27/50, Entropy: 264.8421, Loss: 0.00496444\n",
            "Iteration 27/50, Entropy: 264.8423, Loss: 0.00506453\n",
            "Iteration 27/50, Entropy: 264.8419, Loss: 0.00497335\n",
            "Iteration 27/50, Entropy: 264.8398, Loss: 0.00506672\n",
            "Iteration 28/50, Entropy: 264.8443, Loss: 0.00534554\n",
            "Iteration 28/50, Entropy: 264.8464, Loss: 0.00496134\n",
            "Iteration 28/50, Entropy: 264.8473, Loss: 0.00505903\n",
            "Iteration 28/50, Entropy: 264.8460, Loss: 0.00496855\n",
            "Iteration 28/50, Entropy: 264.8419, Loss: 0.00505932\n",
            "Iteration 29/50, Entropy: 264.8445, Loss: 0.00534072\n",
            "Iteration 29/50, Entropy: 264.8445, Loss: 0.00495884\n",
            "Iteration 29/50, Entropy: 264.8444, Loss: 0.00505297\n",
            "Iteration 29/50, Entropy: 264.8440, Loss: 0.00496634\n",
            "Iteration 29/50, Entropy: 264.8416, Loss: 0.00505481\n",
            "Iteration 30/50, Entropy: 264.8457, Loss: 0.00533457\n",
            "Iteration 30/50, Entropy: 264.8469, Loss: 0.00495421\n",
            "Iteration 30/50, Entropy: 264.8470, Loss: 0.00504519\n",
            "Iteration 30/50, Entropy: 264.8444, Loss: 0.00496182\n",
            "Iteration 30/50, Entropy: 264.8386, Loss: 0.00504815\n",
            "Iteration 31/50, Entropy: 264.8401, Loss: 0.00533046\n",
            "Iteration 31/50, Entropy: 264.8395, Loss: 0.00495636\n",
            "Iteration 31/50, Entropy: 264.8402, Loss: 0.00504258\n",
            "Iteration 31/50, Entropy: 264.8416, Loss: 0.00495521\n",
            "Iteration 31/50, Entropy: 264.8415, Loss: 0.00504295\n",
            "Iteration 32/50, Entropy: 264.8483, Loss: 0.00531430\n",
            "Iteration 32/50, Entropy: 264.8529, Loss: 0.00495533\n",
            "Iteration 32/50, Entropy: 264.8560, Loss: 0.00504538\n",
            "Iteration 32/50, Entropy: 264.8553, Loss: 0.00495537\n",
            "Iteration 32/50, Entropy: 264.8503, Loss: 0.00505069\n",
            "Iteration 33/50, Entropy: 264.8512, Loss: 0.00531322\n",
            "Iteration 33/50, Entropy: 264.8492, Loss: 0.00495659\n",
            "Iteration 33/50, Entropy: 264.8481, Loss: 0.00504533\n",
            "Iteration 33/50, Entropy: 264.8476, Loss: 0.00495435\n",
            "Iteration 33/50, Entropy: 264.8457, Loss: 0.00504956\n",
            "Iteration 34/50, Entropy: 264.8511, Loss: 0.00531265\n",
            "Iteration 34/50, Entropy: 264.8545, Loss: 0.00495817\n",
            "Iteration 34/50, Entropy: 264.8572, Loss: 0.00504246\n",
            "Iteration 34/50, Entropy: 264.8558, Loss: 0.00495407\n",
            "Iteration 34/50, Entropy: 264.8502, Loss: 0.00504488\n",
            "Iteration 35/50, Entropy: 264.8506, Loss: 0.00531442\n",
            "Iteration 35/50, Entropy: 264.8481, Loss: 0.00495317\n",
            "Iteration 35/50, Entropy: 264.8468, Loss: 0.00503818\n",
            "Iteration 35/50, Entropy: 264.8464, Loss: 0.00495323\n",
            "Iteration 35/50, Entropy: 264.8453, Loss: 0.00504242\n",
            "Iteration 36/50, Entropy: 264.8518, Loss: 0.00531864\n",
            "Iteration 36/50, Entropy: 264.8560, Loss: 0.00495288\n",
            "Iteration 36/50, Entropy: 264.8594, Loss: 0.00503876\n",
            "Iteration 36/50, Entropy: 264.8587, Loss: 0.00495272\n",
            "Iteration 36/50, Entropy: 264.8531, Loss: 0.00504165\n",
            "Iteration 37/50, Entropy: 264.8532, Loss: 0.00531312\n",
            "Iteration 37/50, Entropy: 264.8500, Loss: 0.00495371\n",
            "Iteration 37/50, Entropy: 264.8478, Loss: 0.00503893\n",
            "Iteration 37/50, Entropy: 264.8463, Loss: 0.00495162\n",
            "Iteration 37/50, Entropy: 264.8442, Loss: 0.00504145\n",
            "Iteration 38/50, Entropy: 264.8499, Loss: 0.00531328\n",
            "Iteration 38/50, Entropy: 264.8537, Loss: 0.00495387\n",
            "Iteration 38/50, Entropy: 264.8576, Loss: 0.00503704\n",
            "Iteration 38/50, Entropy: 264.8576, Loss: 0.00495142\n",
            "Iteration 38/50, Entropy: 264.8530, Loss: 0.00503934\n",
            "Iteration 39/50, Entropy: 264.8541, Loss: 0.00531458\n",
            "Iteration 39/50, Entropy: 264.8517, Loss: 0.00495298\n",
            "Iteration 39/50, Entropy: 264.8501, Loss: 0.00503602\n",
            "Iteration 39/50, Entropy: 264.8484, Loss: 0.00495067\n",
            "Iteration 39/50, Entropy: 264.8455, Loss: 0.00503928\n",
            "Iteration 40/50, Entropy: 264.8505, Loss: 0.00531221\n",
            "Iteration 40/50, Entropy: 264.8537, Loss: 0.00495396\n",
            "Iteration 40/50, Entropy: 264.8573, Loss: 0.00503582\n",
            "Iteration 40/50, Entropy: 264.8577, Loss: 0.00494980\n",
            "Iteration 40/50, Entropy: 264.8535, Loss: 0.00503921\n",
            "Iteration 41/50, Entropy: 264.8550, Loss: 0.00531154\n",
            "Iteration 41/50, Entropy: 264.8528, Loss: 0.00495321\n",
            "Iteration 41/50, Entropy: 264.8514, Loss: 0.00503310\n",
            "Iteration 41/50, Entropy: 264.8494, Loss: 0.00494909\n",
            "Iteration 41/50, Entropy: 264.8457, Loss: 0.00503838\n",
            "Iteration 42/50, Entropy: 264.8502, Loss: 0.00530953\n",
            "Iteration 42/50, Entropy: 264.8530, Loss: 0.00495234\n",
            "Iteration 42/50, Entropy: 264.8569, Loss: 0.00503360\n",
            "Iteration 42/50, Entropy: 264.8581, Loss: 0.00494858\n",
            "Iteration 42/50, Entropy: 264.8550, Loss: 0.00503823\n",
            "Iteration 43/50, Entropy: 264.8575, Loss: 0.00530941\n",
            "Iteration 43/50, Entropy: 264.8558, Loss: 0.00495171\n",
            "Iteration 43/50, Entropy: 264.8549, Loss: 0.00503320\n",
            "Iteration 43/50, Entropy: 264.8530, Loss: 0.00494851\n",
            "Iteration 43/50, Entropy: 264.8487, Loss: 0.00503929\n",
            "Iteration 44/50, Entropy: 264.8524, Loss: 0.00530767\n",
            "Iteration 44/50, Entropy: 264.8544, Loss: 0.00495236\n",
            "Iteration 44/50, Entropy: 264.8579, Loss: 0.00503377\n",
            "Iteration 44/50, Entropy: 264.8591, Loss: 0.00494846\n",
            "Iteration 44/50, Entropy: 264.8564, Loss: 0.00503830\n",
            "Iteration 45/50, Entropy: 264.8595, Loss: 0.00530888\n",
            "Iteration 45/50, Entropy: 264.8585, Loss: 0.00495148\n",
            "Iteration 45/50, Entropy: 264.8582, Loss: 0.00503285\n",
            "Iteration 45/50, Entropy: 264.8565, Loss: 0.00494810\n",
            "Iteration 45/50, Entropy: 264.8523, Loss: 0.00503851\n",
            "Iteration 46/50, Entropy: 264.8557, Loss: 0.00530834\n",
            "Iteration 46/50, Entropy: 264.8574, Loss: 0.00495203\n",
            "Iteration 46/50, Entropy: 264.8604, Loss: 0.00503308\n",
            "Iteration 46/50, Entropy: 264.8613, Loss: 0.00494763\n",
            "Iteration 46/50, Entropy: 264.8583, Loss: 0.00503773\n",
            "Iteration 47/50, Entropy: 264.8613, Loss: 0.00530940\n",
            "Iteration 47/50, Entropy: 264.8605, Loss: 0.00495167\n",
            "Iteration 47/50, Entropy: 264.8607, Loss: 0.00503247\n",
            "Iteration 47/50, Entropy: 264.8594, Loss: 0.00494776\n",
            "Iteration 47/50, Entropy: 264.8554, Loss: 0.00503803\n",
            "Iteration 48/50, Entropy: 264.8588, Loss: 0.00530904\n",
            "Iteration 48/50, Entropy: 264.8603, Loss: 0.00495160\n",
            "Iteration 48/50, Entropy: 264.8631, Loss: 0.00503294\n",
            "Iteration 48/50, Entropy: 264.8636, Loss: 0.00494754\n",
            "Iteration 48/50, Entropy: 264.8605, Loss: 0.00503751\n",
            "Iteration 49/50, Entropy: 264.8632, Loss: 0.00530915\n",
            "Iteration 49/50, Entropy: 264.8624, Loss: 0.00495126\n",
            "Iteration 49/50, Entropy: 264.8627, Loss: 0.00503255\n",
            "Iteration 49/50, Entropy: 264.8618, Loss: 0.00494779\n",
            "Iteration 49/50, Entropy: 264.8578, Loss: 0.00503792\n",
            "Iteration 50/50, Entropy: 264.8613, Loss: 0.00530861\n",
            "Iteration 50/50, Entropy: 264.8627, Loss: 0.00495136\n",
            "Iteration 50/50, Entropy: 264.8652, Loss: 0.00503300\n",
            "Iteration 50/50, Entropy: 264.8657, Loss: 0.00494747\n",
            "Iteration 50/50, Entropy: 264.8624, Loss: 0.00503739\n",
            "weight diff tensor(0.0011, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.out_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/50, Entropy: 213.7561, Loss: 0.00058705\n",
            "Iteration 1/50, Entropy: 214.1808, Loss: 0.00058452\n",
            "Iteration 1/50, Entropy: 214.4271, Loss: 0.00058239\n",
            "Iteration 1/50, Entropy: 214.5282, Loss: 0.00058185\n",
            "Iteration 1/50, Entropy: 214.5992, Loss: 0.00058234\n",
            "Iteration 2/50, Entropy: 214.6674, Loss: 0.00058416\n",
            "Iteration 2/50, Entropy: 214.7507, Loss: 0.00058285\n",
            "Iteration 2/50, Entropy: 214.8342, Loss: 0.00058037\n",
            "Iteration 2/50, Entropy: 214.9016, Loss: 0.00057991\n",
            "Iteration 2/50, Entropy: 214.9722, Loss: 0.00058019\n",
            "Iteration 3/50, Entropy: 215.0171, Loss: 0.00058261\n",
            "Iteration 3/50, Entropy: 215.0401, Loss: 0.00058165\n",
            "Iteration 3/50, Entropy: 215.0436, Loss: 0.00057945\n",
            "Iteration 3/50, Entropy: 215.0398, Loss: 0.00057902\n",
            "Iteration 3/50, Entropy: 215.0488, Loss: 0.00057960\n",
            "Iteration 4/50, Entropy: 215.0390, Loss: 0.00058185\n",
            "Iteration 4/50, Entropy: 215.0279, Loss: 0.00058041\n",
            "Iteration 4/50, Entropy: 215.0177, Loss: 0.00057836\n",
            "Iteration 4/50, Entropy: 215.0244, Loss: 0.00057818\n",
            "Iteration 4/50, Entropy: 215.0572, Loss: 0.00057883\n",
            "Iteration 5/50, Entropy: 215.0790, Loss: 0.00058140\n",
            "Iteration 5/50, Entropy: 215.1089, Loss: 0.00057981\n",
            "Iteration 5/50, Entropy: 215.1378, Loss: 0.00057830\n",
            "Iteration 5/50, Entropy: 215.1715, Loss: 0.00057789\n",
            "Iteration 5/50, Entropy: 215.2205, Loss: 0.00057849\n",
            "Iteration 6/50, Entropy: 215.2509, Loss: 0.00058090\n",
            "Iteration 6/50, Entropy: 215.2733, Loss: 0.00057946\n",
            "Iteration 6/50, Entropy: 215.2794, Loss: 0.00057773\n",
            "Iteration 6/50, Entropy: 215.2836, Loss: 0.00057748\n",
            "Iteration 6/50, Entropy: 215.2980, Loss: 0.00057828\n",
            "Iteration 7/50, Entropy: 215.2977, Loss: 0.00058059\n",
            "Iteration 7/50, Entropy: 215.2910, Loss: 0.00057914\n",
            "Iteration 7/50, Entropy: 215.2776, Loss: 0.00057739\n",
            "Iteration 7/50, Entropy: 215.2744, Loss: 0.00057712\n",
            "Iteration 7/50, Entropy: 215.3008, Loss: 0.00057754\n",
            "Iteration 8/50, Entropy: 215.3286, Loss: 0.00058016\n",
            "Iteration 8/50, Entropy: 215.3658, Loss: 0.00057856\n",
            "Iteration 8/50, Entropy: 215.3996, Loss: 0.00057693\n",
            "Iteration 8/50, Entropy: 215.4381, Loss: 0.00057665\n",
            "Iteration 8/50, Entropy: 215.4924, Loss: 0.00057716\n",
            "Iteration 9/50, Entropy: 215.5365, Loss: 0.00057986\n",
            "Iteration 9/50, Entropy: 215.5752, Loss: 0.00057824\n",
            "Iteration 9/50, Entropy: 215.5966, Loss: 0.00057660\n",
            "Iteration 9/50, Entropy: 215.6146, Loss: 0.00057633\n",
            "Iteration 9/50, Entropy: 215.6428, Loss: 0.00057689\n",
            "Iteration 10/50, Entropy: 215.6615, Loss: 0.00057955\n",
            "Iteration 10/50, Entropy: 215.6731, Loss: 0.00057798\n",
            "Iteration 10/50, Entropy: 215.6757, Loss: 0.00057638\n",
            "Iteration 10/50, Entropy: 215.6812, Loss: 0.00057622\n",
            "Iteration 10/50, Entropy: 215.7045, Loss: 0.00057672\n",
            "Iteration 11/50, Entropy: 215.7249, Loss: 0.00057939\n",
            "Iteration 11/50, Entropy: 215.7506, Loss: 0.00057783\n",
            "Iteration 11/50, Entropy: 215.7742, Loss: 0.00057601\n",
            "Iteration 11/50, Entropy: 215.8061, Loss: 0.00057580\n",
            "Iteration 11/50, Entropy: 215.8555, Loss: 0.00057630\n",
            "Iteration 12/50, Entropy: 215.8897, Loss: 0.00057890\n",
            "Iteration 12/50, Entropy: 215.9198, Loss: 0.00057742\n",
            "Iteration 12/50, Entropy: 215.9311, Loss: 0.00057568\n",
            "Iteration 12/50, Entropy: 215.9343, Loss: 0.00057546\n",
            "Iteration 12/50, Entropy: 215.9452, Loss: 0.00057593\n",
            "Iteration 13/50, Entropy: 215.9427, Loss: 0.00057850\n",
            "Iteration 13/50, Entropy: 215.9480, Loss: 0.00057702\n",
            "Iteration 13/50, Entropy: 215.9507, Loss: 0.00057549\n",
            "Iteration 13/50, Entropy: 215.9529, Loss: 0.00057538\n",
            "Iteration 13/50, Entropy: 215.9669, Loss: 0.00057576\n",
            "Iteration 14/50, Entropy: 215.9733, Loss: 0.00057829\n",
            "Iteration 14/50, Entropy: 215.9811, Loss: 0.00057689\n",
            "Iteration 14/50, Entropy: 215.9867, Loss: 0.00057540\n",
            "Iteration 14/50, Entropy: 215.9909, Loss: 0.00057519\n",
            "Iteration 14/50, Entropy: 216.0041, Loss: 0.00057558\n",
            "Iteration 15/50, Entropy: 216.0097, Loss: 0.00057811\n",
            "Iteration 15/50, Entropy: 216.0189, Loss: 0.00057670\n",
            "Iteration 15/50, Entropy: 216.0208, Loss: 0.00057515\n",
            "Iteration 15/50, Entropy: 216.0172, Loss: 0.00057500\n",
            "Iteration 15/50, Entropy: 216.0228, Loss: 0.00057539\n",
            "Iteration 16/50, Entropy: 216.0205, Loss: 0.00057789\n",
            "Iteration 16/50, Entropy: 216.0229, Loss: 0.00057653\n",
            "Iteration 16/50, Entropy: 216.0267, Loss: 0.00057499\n",
            "Iteration 16/50, Entropy: 216.0382, Loss: 0.00057492\n",
            "Iteration 16/50, Entropy: 216.0683, Loss: 0.00057515\n",
            "Iteration 17/50, Entropy: 216.0947, Loss: 0.00057769\n",
            "Iteration 17/50, Entropy: 216.1262, Loss: 0.00057619\n",
            "Iteration 17/50, Entropy: 216.1538, Loss: 0.00057468\n",
            "Iteration 17/50, Entropy: 216.1783, Loss: 0.00057450\n",
            "Iteration 17/50, Entropy: 216.2132, Loss: 0.00057483\n",
            "Iteration 18/50, Entropy: 216.2320, Loss: 0.00057747\n",
            "Iteration 18/50, Entropy: 216.2553, Loss: 0.00057595\n",
            "Iteration 18/50, Entropy: 216.2679, Loss: 0.00057450\n",
            "Iteration 18/50, Entropy: 216.2761, Loss: 0.00057437\n",
            "Iteration 18/50, Entropy: 216.2982, Loss: 0.00057475\n",
            "Iteration 19/50, Entropy: 216.3112, Loss: 0.00057738\n",
            "Iteration 19/50, Entropy: 216.3360, Loss: 0.00057583\n",
            "Iteration 19/50, Entropy: 216.3546, Loss: 0.00057437\n",
            "Iteration 19/50, Entropy: 216.3686, Loss: 0.00057427\n",
            "Iteration 19/50, Entropy: 216.3916, Loss: 0.00057464\n",
            "Iteration 20/50, Entropy: 216.3995, Loss: 0.00057714\n",
            "Iteration 20/50, Entropy: 216.4137, Loss: 0.00057574\n",
            "Iteration 20/50, Entropy: 216.4180, Loss: 0.00057430\n",
            "Iteration 20/50, Entropy: 216.4172, Loss: 0.00057418\n",
            "Iteration 20/50, Entropy: 216.4269, Loss: 0.00057453\n",
            "Iteration 21/50, Entropy: 216.4234, Loss: 0.00057702\n",
            "Iteration 21/50, Entropy: 216.4269, Loss: 0.00057558\n",
            "Iteration 21/50, Entropy: 216.4232, Loss: 0.00057415\n",
            "Iteration 21/50, Entropy: 216.4183, Loss: 0.00057404\n",
            "Iteration 21/50, Entropy: 216.4276, Loss: 0.00057440\n",
            "Iteration 22/50, Entropy: 216.4259, Loss: 0.00057691\n",
            "Iteration 22/50, Entropy: 216.4341, Loss: 0.00057545\n",
            "Iteration 22/50, Entropy: 216.4356, Loss: 0.00057403\n",
            "Iteration 22/50, Entropy: 216.4358, Loss: 0.00057390\n",
            "Iteration 22/50, Entropy: 216.4498, Loss: 0.00057427\n",
            "Iteration 23/50, Entropy: 216.4515, Loss: 0.00057678\n",
            "Iteration 23/50, Entropy: 216.4613, Loss: 0.00057535\n",
            "Iteration 23/50, Entropy: 216.4625, Loss: 0.00057390\n",
            "Iteration 23/50, Entropy: 216.4602, Loss: 0.00057380\n",
            "Iteration 23/50, Entropy: 216.4719, Loss: 0.00057419\n",
            "Iteration 24/50, Entropy: 216.4714, Loss: 0.00057668\n",
            "Iteration 24/50, Entropy: 216.4794, Loss: 0.00057526\n",
            "Iteration 24/50, Entropy: 216.4798, Loss: 0.00057381\n",
            "Iteration 24/50, Entropy: 216.4785, Loss: 0.00057370\n",
            "Iteration 24/50, Entropy: 216.4928, Loss: 0.00057411\n",
            "Iteration 25/50, Entropy: 216.4950, Loss: 0.00057658\n",
            "Iteration 25/50, Entropy: 216.5076, Loss: 0.00057518\n",
            "Iteration 25/50, Entropy: 216.5106, Loss: 0.00057371\n",
            "Iteration 25/50, Entropy: 216.5111, Loss: 0.00057358\n",
            "Iteration 25/50, Entropy: 216.5268, Loss: 0.00057401\n",
            "Iteration 26/50, Entropy: 216.5255, Loss: 0.00057647\n",
            "Iteration 26/50, Entropy: 216.5305, Loss: 0.00057513\n",
            "Iteration 26/50, Entropy: 216.5257, Loss: 0.00057362\n",
            "Iteration 26/50, Entropy: 216.5212, Loss: 0.00057352\n",
            "Iteration 26/50, Entropy: 216.5342, Loss: 0.00057394\n",
            "Iteration 27/50, Entropy: 216.5368, Loss: 0.00057641\n",
            "Iteration 27/50, Entropy: 216.5526, Loss: 0.00057501\n",
            "Iteration 27/50, Entropy: 216.5606, Loss: 0.00057354\n",
            "Iteration 27/50, Entropy: 216.5674, Loss: 0.00057342\n",
            "Iteration 27/50, Entropy: 216.5901, Loss: 0.00057385\n",
            "Iteration 28/50, Entropy: 216.5951, Loss: 0.00057628\n",
            "Iteration 28/50, Entropy: 216.6047, Loss: 0.00057498\n",
            "Iteration 28/50, Entropy: 216.6042, Loss: 0.00057348\n",
            "Iteration 28/50, Entropy: 216.6025, Loss: 0.00057338\n",
            "Iteration 28/50, Entropy: 216.6169, Loss: 0.00057380\n",
            "Iteration 29/50, Entropy: 216.6225, Loss: 0.00057620\n",
            "Iteration 29/50, Entropy: 216.6427, Loss: 0.00057487\n",
            "Iteration 29/50, Entropy: 216.6571, Loss: 0.00057340\n",
            "Iteration 29/50, Entropy: 216.6697, Loss: 0.00057326\n",
            "Iteration 29/50, Entropy: 216.6942, Loss: 0.00057369\n",
            "Iteration 30/50, Entropy: 216.7032, Loss: 0.00057600\n",
            "Iteration 30/50, Entropy: 216.7187, Loss: 0.00057481\n",
            "Iteration 30/50, Entropy: 216.7262, Loss: 0.00057335\n",
            "Iteration 30/50, Entropy: 216.7312, Loss: 0.00057325\n",
            "Iteration 30/50, Entropy: 216.7471, Loss: 0.00057362\n",
            "Iteration 31/50, Entropy: 216.7478, Loss: 0.00057592\n",
            "Iteration 31/50, Entropy: 216.7525, Loss: 0.00057475\n",
            "Iteration 31/50, Entropy: 216.7476, Loss: 0.00057324\n",
            "Iteration 31/50, Entropy: 216.7424, Loss: 0.00057312\n",
            "Iteration 31/50, Entropy: 216.7540, Loss: 0.00057355\n",
            "Iteration 32/50, Entropy: 216.7565, Loss: 0.00057586\n",
            "Iteration 32/50, Entropy: 216.7691, Loss: 0.00057466\n",
            "Iteration 32/50, Entropy: 216.7737, Loss: 0.00057319\n",
            "Iteration 32/50, Entropy: 216.7746, Loss: 0.00057307\n",
            "Iteration 32/50, Entropy: 216.7879, Loss: 0.00057349\n",
            "Iteration 33/50, Entropy: 216.7879, Loss: 0.00057578\n",
            "Iteration 33/50, Entropy: 216.7980, Loss: 0.00057463\n",
            "Iteration 33/50, Entropy: 216.8028, Loss: 0.00057312\n",
            "Iteration 33/50, Entropy: 216.8066, Loss: 0.00057300\n",
            "Iteration 33/50, Entropy: 216.8224, Loss: 0.00057343\n",
            "Iteration 34/50, Entropy: 216.8219, Loss: 0.00057570\n",
            "Iteration 34/50, Entropy: 216.8268, Loss: 0.00057455\n",
            "Iteration 34/50, Entropy: 216.8244, Loss: 0.00057306\n",
            "Iteration 34/50, Entropy: 216.8202, Loss: 0.00057293\n",
            "Iteration 34/50, Entropy: 216.8310, Loss: 0.00057337\n",
            "Iteration 35/50, Entropy: 216.8296, Loss: 0.00057566\n",
            "Iteration 35/50, Entropy: 216.8380, Loss: 0.00057449\n",
            "Iteration 35/50, Entropy: 216.8416, Loss: 0.00057299\n",
            "Iteration 35/50, Entropy: 216.8429, Loss: 0.00057287\n",
            "Iteration 35/50, Entropy: 216.8569, Loss: 0.00057331\n",
            "Iteration 36/50, Entropy: 216.8564, Loss: 0.00057557\n",
            "Iteration 36/50, Entropy: 216.8643, Loss: 0.00057445\n",
            "Iteration 36/50, Entropy: 216.8665, Loss: 0.00057293\n",
            "Iteration 36/50, Entropy: 216.8654, Loss: 0.00057280\n",
            "Iteration 36/50, Entropy: 216.8767, Loss: 0.00057326\n",
            "Iteration 37/50, Entropy: 216.8738, Loss: 0.00057553\n",
            "Iteration 37/50, Entropy: 216.8802, Loss: 0.00057437\n",
            "Iteration 37/50, Entropy: 216.8831, Loss: 0.00057287\n",
            "Iteration 37/50, Entropy: 216.8843, Loss: 0.00057274\n",
            "Iteration 37/50, Entropy: 216.8991, Loss: 0.00057320\n",
            "Iteration 38/50, Entropy: 216.8993, Loss: 0.00057546\n",
            "Iteration 38/50, Entropy: 216.9076, Loss: 0.00057433\n",
            "Iteration 38/50, Entropy: 216.9114, Loss: 0.00057280\n",
            "Iteration 38/50, Entropy: 216.9117, Loss: 0.00057267\n",
            "Iteration 38/50, Entropy: 216.9244, Loss: 0.00057315\n",
            "Iteration 39/50, Entropy: 216.9221, Loss: 0.00057540\n",
            "Iteration 39/50, Entropy: 216.9289, Loss: 0.00057427\n",
            "Iteration 39/50, Entropy: 216.9324, Loss: 0.00057275\n",
            "Iteration 39/50, Entropy: 216.9331, Loss: 0.00057261\n",
            "Iteration 39/50, Entropy: 216.9467, Loss: 0.00057309\n",
            "Iteration 40/50, Entropy: 216.9453, Loss: 0.00057534\n",
            "Iteration 40/50, Entropy: 216.9530, Loss: 0.00057421\n",
            "Iteration 40/50, Entropy: 216.9573, Loss: 0.00057268\n",
            "Iteration 40/50, Entropy: 216.9582, Loss: 0.00057255\n",
            "Iteration 40/50, Entropy: 216.9713, Loss: 0.00057304\n",
            "Iteration 41/50, Entropy: 216.9692, Loss: 0.00057527\n",
            "Iteration 41/50, Entropy: 216.9764, Loss: 0.00057416\n",
            "Iteration 41/50, Entropy: 216.9814, Loss: 0.00057262\n",
            "Iteration 41/50, Entropy: 216.9828, Loss: 0.00057249\n",
            "Iteration 41/50, Entropy: 216.9959, Loss: 0.00057299\n",
            "Iteration 42/50, Entropy: 216.9930, Loss: 0.00057520\n",
            "Iteration 42/50, Entropy: 216.9994, Loss: 0.00057410\n",
            "Iteration 42/50, Entropy: 217.0038, Loss: 0.00057256\n",
            "Iteration 42/50, Entropy: 217.0044, Loss: 0.00057243\n",
            "Iteration 42/50, Entropy: 217.0172, Loss: 0.00057293\n",
            "Iteration 43/50, Entropy: 217.0146, Loss: 0.00057514\n",
            "Iteration 43/50, Entropy: 217.0215, Loss: 0.00057404\n",
            "Iteration 43/50, Entropy: 217.0274, Loss: 0.00057250\n",
            "Iteration 43/50, Entropy: 217.0290, Loss: 0.00057236\n",
            "Iteration 43/50, Entropy: 217.0420, Loss: 0.00057288\n",
            "Iteration 44/50, Entropy: 217.0392, Loss: 0.00057507\n",
            "Iteration 44/50, Entropy: 217.0457, Loss: 0.00057398\n",
            "Iteration 44/50, Entropy: 217.0515, Loss: 0.00057243\n",
            "Iteration 44/50, Entropy: 217.0530, Loss: 0.00057230\n",
            "Iteration 44/50, Entropy: 217.0659, Loss: 0.00057283\n",
            "Iteration 45/50, Entropy: 217.0636, Loss: 0.00057499\n",
            "Iteration 45/50, Entropy: 217.0711, Loss: 0.00057392\n",
            "Iteration 45/50, Entropy: 217.0789, Loss: 0.00057237\n",
            "Iteration 45/50, Entropy: 217.0816, Loss: 0.00057223\n",
            "Iteration 45/50, Entropy: 217.0954, Loss: 0.00057278\n",
            "Iteration 46/50, Entropy: 217.0940, Loss: 0.00057490\n",
            "Iteration 46/50, Entropy: 217.1035, Loss: 0.00057384\n",
            "Iteration 46/50, Entropy: 217.1149, Loss: 0.00057231\n",
            "Iteration 46/50, Entropy: 217.1199, Loss: 0.00057216\n",
            "Iteration 46/50, Entropy: 217.1347, Loss: 0.00057273\n",
            "Iteration 47/50, Entropy: 217.1339, Loss: 0.00057480\n",
            "Iteration 47/50, Entropy: 217.1413, Loss: 0.00057375\n",
            "Iteration 47/50, Entropy: 217.1501, Loss: 0.00057225\n",
            "Iteration 47/50, Entropy: 217.1501, Loss: 0.00057210\n",
            "Iteration 47/50, Entropy: 217.1585, Loss: 0.00057266\n",
            "Iteration 48/50, Entropy: 217.1521, Loss: 0.00057474\n",
            "Iteration 48/50, Entropy: 217.1559, Loss: 0.00057369\n",
            "Iteration 48/50, Entropy: 217.1625, Loss: 0.00057216\n",
            "Iteration 48/50, Entropy: 217.1619, Loss: 0.00057202\n",
            "Iteration 48/50, Entropy: 217.1716, Loss: 0.00057259\n",
            "Iteration 49/50, Entropy: 217.1662, Loss: 0.00057467\n",
            "Iteration 49/50, Entropy: 217.1722, Loss: 0.00057363\n",
            "Iteration 49/50, Entropy: 217.1811, Loss: 0.00057209\n",
            "Iteration 49/50, Entropy: 217.1833, Loss: 0.00057195\n",
            "Iteration 49/50, Entropy: 217.1959, Loss: 0.00057253\n",
            "Iteration 50/50, Entropy: 217.1930, Loss: 0.00057458\n",
            "Iteration 50/50, Entropy: 217.2003, Loss: 0.00057355\n",
            "Iteration 50/50, Entropy: 217.2102, Loss: 0.00057202\n",
            "Iteration 50/50, Entropy: 217.2123, Loss: 0.00057188\n",
            "Iteration 50/50, Entropy: 217.2238, Loss: 0.00057247\n",
            "weight diff tensor(0.0005, device='cuda:0')\n",
            "\n",
            "✅ Finished GPTQ-initialized blockwise quantization for all layers.\n"
          ]
        }
      ],
      "source": [
        "# === Setup ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from safetensors.torch import save_file\n",
        "from gptq import GPTQ\n",
        "import math\n",
        "\n",
        "# === CONFIG ===\n",
        "MODEL_NAME = \"facebook/opt-125m\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 2\n",
        "N_BATCHES = 5\n",
        "SEQ_LEN = 32\n",
        "NUM_BITS = 4\n",
        "BLOCK_SIZE = 128\n",
        "FIXED_T = 1000.0\n",
        "LR = 0.001\n",
        "NUM_ITERATIONS = 50\n",
        "\n",
        "\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# === Calibration Setup using TinyStories CSV ===\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"validation.csv\"        # Path to your TinyStories CSV\n",
        "TEXT_COLUMN = \"text\"               # Column containing stories\n",
        "N_CALIB_SAMPLES = 1000              # Number of samples to use\n",
        "\n",
        "# Load and preprocess CSV\n",
        "print(\"📖 Loading TinyStories from CSV...\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "assert TEXT_COLUMN in df.columns, f\"'{TEXT_COLUMN}' column not found in CSV.\"\n",
        "texts = df[TEXT_COLUMN].dropna().tolist()[:BATCH_SIZE * N_BATCHES]\n",
        "\n",
        "# Tokenize\n",
        "print(\"🔠 Tokenizing TinyStories for calibration...\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=SEQ_LEN,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "input_ids = encodings[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = encodings[\"attention_mask\"].to(DEVICE)\n",
        "input_batches = input_ids.split(BATCH_SIZE)\n",
        "mask_batches = attention_mask.split(BATCH_SIZE)\n",
        "\n",
        "\n",
        "def get_power_bins(a=0.5, num_bits=4, device=\"cpu\"):\n",
        "    q_levels = 2 ** num_bits\n",
        "    lin = torch.linspace(0, 1, q_levels, device=device)\n",
        "    scaled = (lin ** (1 / a)) * 0.5\n",
        "    bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "    return bins\n",
        "\n",
        "\n",
        "# === Define BlockwiseQuantizationOptim with GPTQ weight ===\n",
        "class BlockwiseQuantizationOptim(nn.Module):\n",
        "    def __init__(self, weight, block_size=128, num_bits=4, fixed_T=100.0,\n",
        "                 gptq_scale=None, gptq_zero=None, gptq_g_idx=None):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.num_bits = num_bits\n",
        "        self.fixed_T = fixed_T\n",
        "        self.original_shape = weight.shape\n",
        "        self.num_levels = 2 ** num_bits\n",
        "\n",
        "        padded_rows = math.ceil(weight.size(0) / block_size) * block_size\n",
        "        padded_cols = math.ceil(weight.size(1) / block_size) * block_size\n",
        "        self.padded_weight = torch.zeros((padded_rows, padded_cols), device=weight.device)\n",
        "        self.padded_weight[:weight.size(0), :weight.size(1)] = weight\n",
        "\n",
        "        self.blocks = []\n",
        "        self.block_metadata = []\n",
        "        for i in range(0, padded_rows, block_size):\n",
        "            for j in range(0, padded_cols, block_size):\n",
        "                block = self.padded_weight[i:i+block_size, j:j+block_size]\n",
        "                self.blocks.append(block)\n",
        "                self.block_metadata.append((i, j))\n",
        "\n",
        "        self.w_min = nn.ParameterList()\n",
        "        self.w_max = nn.ParameterList()\n",
        "\n",
        "        for _, (i, j) in enumerate(self.block_metadata):\n",
        "            if gptq_scale is not None and gptq_zero is not None and gptq_g_idx is not None:\n",
        "                # Compute group indices for this block of columns\n",
        "                col_start = j\n",
        "                col_end = min(j + block_size, gptq_g_idx.shape[0])\n",
        "                block_g_idx = gptq_g_idx[col_start:col_end]  # Shape: [block_cols]\n",
        "\n",
        "                # Take the mean scale and zero for this block's group mapping\n",
        "                scale_block = gptq_scale[0, block_g_idx].mean().detach()\n",
        "                zero_block = gptq_zero[0, block_g_idx].mean().detach()\n",
        "\n",
        "                # Derive min and max from scale/zero\n",
        "                w_min = (-zero_block * scale_block)\n",
        "                w_max = ((2 ** self.num_bits - 1 - zero_block) * scale_block)\n",
        "            else:\n",
        "                # Fallback to naive initialization\n",
        "                block = self.padded_weight[i:i+block_size, j:j+block_size]\n",
        "                w_min = block.min().detach()\n",
        "                w_max = block.max().detach()\n",
        "\n",
        "            self.w_min.append(nn.Parameter(w_min.view(1)))\n",
        "            self.w_max.append(nn.Parameter(w_max.view(1)))\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "        total_entropy = 0.0\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            w_min = self.w_min[idx].clamp(max=self.w_max[idx].item() - eps)\n",
        "            w_max = self.w_max[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "            #q_levels = torch.linspace(0, 1, self.num_levels, device=block.device)\n",
        "            q_levels = get_power_bins(a=0.5, num_bits=self.num_bits, device=block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - q_levels)\n",
        "            soft_probs = torch.softmax(dists * self.fixed_T, dim=-1)\n",
        "            w_q = (soft_probs * q_levels).sum(dim=-1)\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "            bin_mass = soft_probs.sum(dim=0)\n",
        "            bin_probs = bin_mass / (bin_mass.sum() + eps)\n",
        "            entropy = -(bin_probs * (bin_probs + eps).log()).sum()\n",
        "            total_entropy += entropy\n",
        "\n",
        "        padded_out = torch.zeros_like(self.padded_weight)\n",
        "        for idx, (i, j) in enumerate(self.block_metadata):\n",
        "            padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "        return padded_out[:self.original_shape[0], :self.original_shape[1]], total_entropy\n",
        "\n",
        "    def export(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            w_min = self.w_min[idx].clamp(max=self.w_max[idx].item() - eps)\n",
        "            w_max = self.w_max[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "            #q_levels = torch.linspace(0, 1, self.num_levels, device=block.device)\n",
        "            q_levels = get_power_bins(a=0.5, num_bits=self.num_bits, device=block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - q_levels)\n",
        "            q_idx = torch.argmax(dists, dim=-1).to(torch.int32)\n",
        "            w_q = q_levels[q_idx]\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "        padded_out = torch.zeros_like(self.padded_weight)\n",
        "        for idx, (i, j) in enumerate(self.block_metadata):\n",
        "            padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "        return padded_out[:self.original_shape[0], :self.original_shape[1]].cpu()\n",
        "\n",
        "# === Quantization Loop for all Linear Layers ===\n",
        "safetensor_dict = {}\n",
        "flag = 0\n",
        "for name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "    if flag == 4:\n",
        "      break\n",
        "    flag += 1\n",
        "    print(f\"\\n🔧 GPTQ + Blockwise Quantizing Layer: {name} | Shape: {module.weight.shape}\")\n",
        "    original_weight = module.weight.data.clone()\n",
        "    activation_batches = []\n",
        "    def hook_fn(mod, inp, out):\n",
        "        activation_batches.append(inp[0].detach())\n",
        "    hook = module.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, m in zip(input_batches, mask_batches):\n",
        "            model(input_ids=x, attention_mask=m)\n",
        "    hook.remove()\n",
        "\n",
        "    if not activation_batches:\n",
        "        continue\n",
        "\n",
        "    gptq = GPTQ(module)\n",
        "    for act in activation_batches:\n",
        "        gptq.add_batch(act, module(act))\n",
        "    scale, zero, g_idx = gptq.fasterquant(\n",
        "        blocksize=BLOCK_SIZE,\n",
        "        percdamp=0.01,\n",
        "        group_size=128,\n",
        "        actorder=True,\n",
        "    )\n",
        "    q_weight = module.weight.data.clone()\n",
        "\n",
        "    # Init BlockwiseQuantizationOptim using GPTQ parameters\n",
        "    quant_layer = BlockwiseQuantizationOptim(\n",
        "        weight=original_weight,\n",
        "        block_size=BLOCK_SIZE,\n",
        "        num_bits=NUM_BITS,\n",
        "        fixed_T=FIXED_T,\n",
        "        gptq_scale=scale,\n",
        "        gptq_zero=zero,\n",
        "        gptq_g_idx=g_idx\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(quant_layer.parameters(), lr=LR)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    #original_weight = module.weight.data.clone()\n",
        "\n",
        "    for it in range(NUM_ITERATIONS):\n",
        "        for act in activation_batches:\n",
        "            optimizer.zero_grad()\n",
        "            w_q, entropy = quant_layer()\n",
        "            recon = F.linear(act.to(DEVICE), w_q)\n",
        "            target = F.linear(act.to(DEVICE), q_weight)\n",
        "            loss = mse_loss(recon, target) + mse_loss(q_weight, w_q)\n",
        "            print(f\"Iteration {it + 1}/{NUM_ITERATIONS}, Entropy: {entropy.item():.4f}, Loss: {loss.item():.8f}\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        final_weight = quant_layer.export().to(module.weight.device)\n",
        "        loss = mse_loss(q_weight, final_weight)\n",
        "        print(\"weight diff\",loss)\n",
        "        module.weight.copy_(final_weight)\n",
        "        safetensor_dict[name.replace(\".\", \"_\") + \".dequant\"] = final_weight\n",
        "\n",
        "# === Save Final Weights ===\n",
        "#save_file(safetensor_dict, \"quantized_blockwise_gptq.safetensors\")\n",
        "print(\"\\n✅ Finished GPTQ-initialized blockwise quantization for all layers.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "# ==== Test quantized model ====\n",
        "prompt = \"travel while\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=10)\n",
        "\n",
        "print(\"Sample Output:\", tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN0mZwjOyks7",
        "outputId": "9c0fa032-81e0-4b85-8fd2-52eaab52433f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Output: travel while you were sleeping.\n",
            "\n",
            "The following\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from safetensors.torch import save_file\n",
        "from gptq import GPTQ\n",
        "import math\n",
        "\n",
        "# === CONFIG ===\n",
        "MODEL_NAME = \"facebook/opt-125m\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 2\n",
        "N_BATCHES = 5\n",
        "SEQ_LEN = 32\n",
        "NUM_BITS = 4\n",
        "BLOCK_SIZE = 128\n",
        "FIXED_T = 1000.0\n",
        "LR = 0.001\n",
        "NUM_ITERATIONS = 10\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# === Calibration Setup using TinyStories CSV ===\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"validation.csv\"        # Path to your TinyStories CSV\n",
        "TEXT_COLUMN = \"text\"               # Column containing stories\n",
        "N_CALIB_SAMPLES = 1000              # Number of samples to use\n",
        "\n",
        "# Load and preprocess CSV\n",
        "print(\"📖 Loading TinyStories from CSV...\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "assert TEXT_COLUMN in df.columns, f\"'{TEXT_COLUMN}' column not found in CSV.\"\n",
        "texts = df[TEXT_COLUMN].dropna().tolist()[:BATCH_SIZE * N_BATCHES]\n",
        "\n",
        "# Tokenize\n",
        "print(\"🔠 Tokenizing TinyStories for calibration...\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=SEQ_LEN,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "input_ids = encodings[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = encodings[\"attention_mask\"].to(DEVICE)\n",
        "input_batches = input_ids.split(BATCH_SIZE)\n",
        "mask_batches = attention_mask.split(BATCH_SIZE)\n",
        "def get_power_bins(a=0.5, num_bits=4, device=\"cpu\"):\n",
        "    q_levels = 2 ** num_bits\n",
        "    lin = torch.linspace(0, 1, q_levels, device=device)\n",
        "    scaled = (lin ** (1 / a)) * 0.5\n",
        "    bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "    return bins\n",
        "# === Define BlockwiseQuantizationOptim with GPTQ weight ===\n",
        "class BlockwiseQuantizationOptim(nn.Module):\n",
        "    def __init__(self, gptq_weight, block_size=BLOCK_SIZE, num_bits=NUM_BITS, fixed_T=FIXED_T):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.num_bits = num_bits\n",
        "        self.fixed_T = fixed_T\n",
        "        self.original_shape = gptq_weight.shape\n",
        "        self.num_levels = 2 ** num_bits\n",
        "\n",
        "        padded_rows = math.ceil(gptq_weight.size(0) / block_size) * block_size\n",
        "        padded_cols = math.ceil(gptq_weight.size(1) / block_size) * block_size\n",
        "        self.padded_weight = torch.zeros((padded_rows, padded_cols), device=gptq_weight.device)\n",
        "        self.padded_weight[:gptq_weight.size(0), :gptq_weight.size(1)] = gptq_weight\n",
        "\n",
        "        self.blocks = []\n",
        "        self.block_metadata = []\n",
        "        self.w_min = nn.ParameterList()\n",
        "        self.w_max = nn.ParameterList()\n",
        "\n",
        "        for i in range(0, padded_rows, block_size):\n",
        "            for j in range(0, padded_cols, block_size):\n",
        "                block = self.padded_weight[i:i+block_size, j:j+block_size]\n",
        "                self.blocks.append(block)\n",
        "                self.block_metadata.append((i, j))\n",
        "                w_min = block.min().detach()\n",
        "                w_max = block.max().detach()\n",
        "                self.w_min.append(nn.Parameter(w_min.view(1)))\n",
        "                self.w_max.append(nn.Parameter(w_max.view(1)))\n",
        "\n",
        "    def forward(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "        total_entropy = 0.0\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            w_min = self.w_min[idx].clamp(max=self.w_max[idx].item() - eps)\n",
        "            w_max = self.w_max[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "            #q_levels = torch.linspace(0, 1, self.num_levels, device=block.device)\n",
        "            q_levels = get_power_bins(a=0.55, num_bits=self.num_bits, device=block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - q_levels)\n",
        "            soft_probs = torch.softmax(dists * self.fixed_T, dim=-1)\n",
        "            w_q = (soft_probs * q_levels).sum(dim=-1)\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "            bin_mass = soft_probs.sum(dim=0)\n",
        "            bin_probs = bin_mass / (bin_mass.sum() + eps)\n",
        "            entropy = -(bin_probs * (bin_probs + eps).log()).sum()\n",
        "            total_entropy += entropy\n",
        "\n",
        "        padded_out = torch.zeros_like(self.padded_weight)\n",
        "        for idx, (i, j) in enumerate(self.block_metadata):\n",
        "            padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "        return padded_out[:self.original_shape[0], :self.original_shape[1]], total_entropy\n",
        "\n",
        "    def export(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            w_min = self.w_min[idx].clamp(max=self.w_max[idx].item() - eps)\n",
        "            w_max = self.w_max[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "            #q_levels = torch.linspace(0, 1, self.num_levels, device=block.device)\n",
        "            q_levels = get_power_bins(a=0.55, num_bits=self.num_bits, device=block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - q_levels)\n",
        "            q_idx = torch.argmax(dists, dim=-1).to(torch.int32)\n",
        "            w_q = q_levels[q_idx]\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "        padded_out = torch.zeros_like(self.padded_weight)\n",
        "        for idx, (i, j) in enumerate(self.block_metadata):\n",
        "            padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "        return padded_out[:self.original_shape[0], :self.original_shape[1]].cpu()\n",
        "\n",
        "# === Quantization Loop for all Linear Layers ===\n",
        "safetensor_dict = {}\n",
        "flag = 0\n",
        "for name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "    if \"lm_head\" in name:\n",
        "        continue\n",
        "    if \"fc2\" in name:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n🔧 GPTQ + Blockwise Quantizing Layer: {name} | Shape: {module.weight.shape}\")\n",
        "\n",
        "    activation_batches = []\n",
        "    def hook_fn(mod, inp, out):\n",
        "        activation_batches.append(inp[0].detach())\n",
        "    hook = module.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, m in zip(input_batches, mask_batches):\n",
        "            model(input_ids=x, attention_mask=m)\n",
        "    hook.remove()\n",
        "\n",
        "    if not activation_batches:\n",
        "        continue\n",
        "    original_weight = module.weight.data.clone()\n",
        "    gptq = GPTQ(module)\n",
        "    for act in activation_batches:\n",
        "        gptq.add_batch(act, module(act))\n",
        "    gptq.fasterquant(\n",
        "        blocksize=BLOCK_SIZE,\n",
        "        percdamp=0.01,\n",
        "        group_size=128,\n",
        "        actorder=True,\n",
        "    )\n",
        "    q_weight = module.weight.data.clone()\n",
        "\n",
        "    quant_layer = BlockwiseQuantizationOptim(original_weight).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(quant_layer.parameters(), lr=LR)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    #original_weight = module.weight.data.clone()\n",
        "\n",
        "    for it in range(NUM_ITERATIONS):\n",
        "        for act in activation_batches:\n",
        "            optimizer.zero_grad()\n",
        "            w_q, entropy = quant_layer()\n",
        "            recon = F.linear(act.to(DEVICE), w_q)\n",
        "            target = F.linear(act.to(DEVICE), original_weight)\n",
        "            loss = mse_loss(recon, target) + mse_loss(original_weight, w_q)\n",
        "            print(f\"Iteration {it + 1}/{NUM_ITERATIONS}, Entropy: {entropy.item():.4f}, Loss: {loss.item():.8f}\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        final_weight = quant_layer.export().to(module.weight.device)\n",
        "        loss = mse_loss(original_weight, final_weight)\n",
        "        print(\"weight diff\",loss)\n",
        "        module.weight.copy_(final_weight)\n",
        "        safetensor_dict[name.replace(\".\", \"_\") + \".dequant\"] = final_weight\n",
        "\n",
        "# === Save Final Weights ===\n",
        "#save_file(safetensor_dict, \"quantized_blockwise_gptq.safetensors\")\n",
        "print(\"\\n✅ Finished GPTQ-initialized blockwise quantization for all layers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0920887fd87c48d8a9abca0111fac1af",
            "1e41a65f00df4cc085f6c35d991c84a5",
            "bde8113e6de4498bacedd3ccaf830743",
            "63b7eb45d3ae44e29f404844ee4c05b4",
            "96d821f4e37b41d799a39ff90198b177",
            "7b9bfa9ccc31445f879a22652a48d98c",
            "5772edd8d43c4c4b8e39f883b457e408",
            "00be46014d294fc09aaf9cb0a7555a54",
            "437fbd634f99462da834ae960a177cd2",
            "6ee36262605745d18ad9d7646b05d79e",
            "04fab5378c43494d82b5869a44bab39d",
            "d592bea7f6b24d29b192a1e1b9df9568",
            "7fcfec647edd415595d52a69454d8f41",
            "3c9098593a0546b5a642dce7be59351b",
            "6a572cdc350f470e86a3b5b83b66670f",
            "709c919a26c34f17ae4190d9da731fe9",
            "dc07189a85dd4c0c9951713d9ba8ccb1",
            "df8244874ecc43c6a0d701d14378c6f3",
            "a08d851c047e4c9db6bd78fc8107712a",
            "ffb1bd30e973430facf0f16098ef772d",
            "00510c4cf09e4b3d9c8571bed2405649",
            "5cf6feb8a17b49ca86ee446405866cc2",
            "d7fc3c30f81646d29964e068e649afdb",
            "0af8d07ec8df4cb7a71ceacbdd314a76",
            "64874d8830384dd0899b0d24d15bacb6",
            "0768aebf3f784cddb4264ee9f1c6cc78",
            "ad6fc702ee524a3a9ec2c72c6aba5e92",
            "2148b5254c224caf940edf075f8117db",
            "8a5b590f736d4998b554a687e0ec65ff",
            "6c14492c285f401ea0b4e74fd026ec64",
            "f25b70eb45614f7f87b99f43f36302b3",
            "4bed637f0e354e5eb6f3a54b999e619c",
            "0e7f43d215f94ddb932b9cc47a1dcd63",
            "32efe18b89dc4b85870fda616b7c24c5",
            "1168dc77ad004638b6c883888f0fbcde",
            "117a4dcaf95749918442e388b3f77dc6",
            "ea58145ee1284976b372a38256973ae6",
            "1b83f6c742e149bab5003539c79d5aa3",
            "56efd00302284e3482b666d1cb28e68c",
            "82f8d537312842efb64b861ca5c7f074",
            "22f8e75e2182489e83b25c692763d9dc",
            "01e41b0b618e46f9877181a7f0780ece",
            "1053343ecf54452aa9473cd95fa40341",
            "cb74a26685a34fe2a514c6d986c7961b",
            "974ba326ae014525981fd4199b57739a",
            "ea874d49aefd42cfaf91859d52e84b8d",
            "411e624a201647fc8f12034fbaaa774f",
            "909d6fc9d7e94ea28433e68c6e65516e",
            "a555fda8abab4110984b07ef074f3f9c",
            "49b5e5c5691e4085a3ed7281f4255f35",
            "7dbedee90d9d492d880c4b7b211bceea",
            "fb62e81fba5c40b7aed8eb771a8302ef",
            "2a5fc401579a4180b1b889e802afa54d",
            "c00dd647eb3347f387dfdb05d5c035de",
            "d15a166ddf5d454c8151c32a12791aed",
            "c35e0555f4ad4c49ad5d67584a9226b0",
            "0ecada196d394a5b90f63ad4190b3f03",
            "500c3ef2fbdf42f0a111fd7521f343ee",
            "903252a050a94536a1d204c6242ce2f0",
            "e548eb09dec14df49e4ab73b65c482c5",
            "9b84eb65bc0c4e93b1e6f6c45d675124",
            "e122a074c9624d29be3d6be1023d223e",
            "e847b7e6a80d4f1f9720442f71edf1b1",
            "bbb57c882d8e47779433d82c216d35ea",
            "42dfc547b0cd48fa870fa6a81b076e8d",
            "0ad4749ab0b147828c3fedf61617c861",
            "7834d43e7bb344ff8a8340f296683d7c",
            "0010bb7d26a846f28dc036346cfb00f1",
            "bbc6c991f48e420bbd45eaa35ad647f6",
            "aa258fc9cda441a08f844407663a437b",
            "dacf88e4a1ba48bbb1fa0659493fd8bb",
            "6fc48c484f4742989226bc15e6064d56",
            "5da244f118bf4d0a929b196d033aad0f",
            "95cbcfd1d13b4b64b7b3a06932e8adfb",
            "d8887faa4cc640a7b3c581b050291933",
            "ef2aaea1676e4831946b9f7140c21bfc",
            "e2e11304d449428a93f1425fa5959492",
            "27c491f6d2b640f791527d8ee29f3825",
            "4b88d72309874fdba016df69d93d80ff",
            "63432fbab19f45d386740e04f678023a",
            "e04ce6ce2d3b49a8a9f70c51d15055e6",
            "4f75043a6240493684e8a3943c1017ec",
            "002274d8f4a64c9d9770da6ebfdca6d9",
            "6b95e94e87a14c01aae25f4c22ff08ce",
            "88f524f56ca54c7196fa7040bc5de33c",
            "219634ef1b814128b7e7428e7b096134",
            "0a25cefc14d646169ccd44297e4f0676",
            "8e4062b7d8d24e9dafa15ab4f8bb8c5f"
          ]
        },
        "id": "MhtM9BVazCmA",
        "outputId": "65eaa494-10b6-4622-9ed7-ca00ec3b3118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0920887fd87c48d8a9abca0111fac1af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d592bea7f6b24d29b192a1e1b9df9568"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7fc3c30f81646d29964e068e649afdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32efe18b89dc4b85870fda616b7c24c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "974ba326ae014525981fd4199b57739a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c35e0555f4ad4c49ad5d67584a9226b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7834d43e7bb344ff8a8340f296683d7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27c491f6d2b640f791527d8ee29f3825"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Loading TinyStories from CSV...\n",
            "🔠 Tokenizing TinyStories for calibration...\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.k_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 267.9043, Loss: 0.00306232\n",
            "Iteration 1/10, Entropy: 267.8928, Loss: 0.00295261\n",
            "Iteration 1/10, Entropy: 267.9056, Loss: 0.00294262\n",
            "Iteration 1/10, Entropy: 267.9290, Loss: 0.00297390\n",
            "Iteration 1/10, Entropy: 267.9485, Loss: 0.00291151\n",
            "Iteration 2/10, Entropy: 267.9654, Loss: 0.00291848\n",
            "Iteration 2/10, Entropy: 267.9790, Loss: 0.00284403\n",
            "Iteration 2/10, Entropy: 267.9899, Loss: 0.00286282\n",
            "Iteration 2/10, Entropy: 267.9994, Loss: 0.00291082\n",
            "Iteration 2/10, Entropy: 268.0043, Loss: 0.00284972\n",
            "Iteration 3/10, Entropy: 268.0102, Loss: 0.00285881\n",
            "Iteration 3/10, Entropy: 268.0182, Loss: 0.00281264\n",
            "Iteration 3/10, Entropy: 268.0255, Loss: 0.00282637\n",
            "Iteration 3/10, Entropy: 268.0319, Loss: 0.00286619\n",
            "Iteration 3/10, Entropy: 268.0364, Loss: 0.00282210\n",
            "Iteration 4/10, Entropy: 268.0407, Loss: 0.00283807\n",
            "Iteration 4/10, Entropy: 268.0452, Loss: 0.00278447\n",
            "Iteration 4/10, Entropy: 268.0488, Loss: 0.00279899\n",
            "Iteration 4/10, Entropy: 268.0512, Loss: 0.00283746\n",
            "Iteration 4/10, Entropy: 268.0522, Loss: 0.00280764\n",
            "Iteration 5/10, Entropy: 268.0524, Loss: 0.00282856\n",
            "Iteration 5/10, Entropy: 268.0518, Loss: 0.00276331\n",
            "Iteration 5/10, Entropy: 268.0502, Loss: 0.00277261\n",
            "Iteration 5/10, Entropy: 268.0490, Loss: 0.00281768\n",
            "Iteration 5/10, Entropy: 268.0466, Loss: 0.00278015\n",
            "Iteration 6/10, Entropy: 268.0435, Loss: 0.00279880\n",
            "Iteration 6/10, Entropy: 268.0394, Loss: 0.00274626\n",
            "Iteration 6/10, Entropy: 268.0338, Loss: 0.00275112\n",
            "Iteration 6/10, Entropy: 268.0292, Loss: 0.00279849\n",
            "Iteration 6/10, Entropy: 268.0264, Loss: 0.00277099\n",
            "Iteration 7/10, Entropy: 268.0247, Loss: 0.00278156\n",
            "Iteration 7/10, Entropy: 268.0231, Loss: 0.00272032\n",
            "Iteration 7/10, Entropy: 268.0214, Loss: 0.00272743\n",
            "Iteration 7/10, Entropy: 268.0231, Loss: 0.00278631\n",
            "Iteration 7/10, Entropy: 268.0291, Loss: 0.00275771\n",
            "Iteration 8/10, Entropy: 268.0373, Loss: 0.00276753\n",
            "Iteration 8/10, Entropy: 268.0433, Loss: 0.00270146\n",
            "Iteration 8/10, Entropy: 268.0458, Loss: 0.00272298\n",
            "Iteration 8/10, Entropy: 268.0476, Loss: 0.00276795\n",
            "Iteration 8/10, Entropy: 268.0499, Loss: 0.00275297\n",
            "Iteration 9/10, Entropy: 268.0522, Loss: 0.00276895\n",
            "Iteration 9/10, Entropy: 268.0533, Loss: 0.00270107\n",
            "Iteration 9/10, Entropy: 268.0535, Loss: 0.00271663\n",
            "Iteration 9/10, Entropy: 268.0573, Loss: 0.00276604\n",
            "Iteration 9/10, Entropy: 268.0641, Loss: 0.00275040\n",
            "Iteration 10/10, Entropy: 268.0717, Loss: 0.00276544\n",
            "Iteration 10/10, Entropy: 268.0739, Loss: 0.00269833\n",
            "Iteration 10/10, Entropy: 268.0712, Loss: 0.00271570\n",
            "Iteration 10/10, Entropy: 268.0679, Loss: 0.00276051\n",
            "Iteration 10/10, Entropy: 268.0648, Loss: 0.00274628\n",
            "weight diff tensor(4.1889e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.v_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 262.4628, Loss: 0.00013469\n",
            "Iteration 1/10, Entropy: 262.4068, Loss: 0.00013369\n",
            "Iteration 1/10, Entropy: 262.4740, Loss: 0.00013376\n",
            "Iteration 1/10, Entropy: 262.5239, Loss: 0.00013357\n",
            "Iteration 1/10, Entropy: 262.5223, Loss: 0.00013164\n",
            "Iteration 2/10, Entropy: 262.5672, Loss: 0.00013119\n",
            "Iteration 2/10, Entropy: 262.6147, Loss: 0.00012778\n",
            "Iteration 2/10, Entropy: 262.7095, Loss: 0.00012769\n",
            "Iteration 2/10, Entropy: 262.7482, Loss: 0.00012963\n",
            "Iteration 2/10, Entropy: 262.7820, Loss: 0.00012832\n",
            "Iteration 3/10, Entropy: 262.8289, Loss: 0.00012568\n",
            "Iteration 3/10, Entropy: 262.8846, Loss: 0.00012521\n",
            "Iteration 3/10, Entropy: 262.9087, Loss: 0.00012517\n",
            "Iteration 3/10, Entropy: 262.9225, Loss: 0.00012616\n",
            "Iteration 3/10, Entropy: 262.9480, Loss: 0.00012410\n",
            "Iteration 4/10, Entropy: 262.9900, Loss: 0.00012384\n",
            "Iteration 4/10, Entropy: 263.0362, Loss: 0.00012158\n",
            "Iteration 4/10, Entropy: 263.0877, Loss: 0.00012091\n",
            "Iteration 4/10, Entropy: 263.1391, Loss: 0.00012096\n",
            "Iteration 4/10, Entropy: 263.1861, Loss: 0.00012094\n",
            "Iteration 5/10, Entropy: 263.2417, Loss: 0.00012011\n",
            "Iteration 5/10, Entropy: 263.3094, Loss: 0.00011932\n",
            "Iteration 5/10, Entropy: 263.3741, Loss: 0.00011823\n",
            "Iteration 5/10, Entropy: 263.4228, Loss: 0.00011822\n",
            "Iteration 5/10, Entropy: 263.4747, Loss: 0.00011909\n",
            "Iteration 6/10, Entropy: 263.5226, Loss: 0.00011843\n",
            "Iteration 6/10, Entropy: 263.5555, Loss: 0.00011811\n",
            "Iteration 6/10, Entropy: 263.5808, Loss: 0.00011689\n",
            "Iteration 6/10, Entropy: 263.5962, Loss: 0.00011728\n",
            "Iteration 6/10, Entropy: 263.6013, Loss: 0.00011714\n",
            "Iteration 7/10, Entropy: 263.6190, Loss: 0.00011616\n",
            "Iteration 7/10, Entropy: 263.6538, Loss: 0.00011485\n",
            "Iteration 7/10, Entropy: 263.6986, Loss: 0.00011574\n",
            "Iteration 7/10, Entropy: 263.7186, Loss: 0.00011547\n",
            "Iteration 7/10, Entropy: 263.7314, Loss: 0.00011521\n",
            "Iteration 8/10, Entropy: 263.7503, Loss: 0.00011445\n",
            "Iteration 8/10, Entropy: 263.7804, Loss: 0.00011273\n",
            "Iteration 8/10, Entropy: 263.8009, Loss: 0.00011238\n",
            "Iteration 8/10, Entropy: 263.8011, Loss: 0.00011399\n",
            "Iteration 8/10, Entropy: 263.7955, Loss: 0.00011372\n",
            "Iteration 9/10, Entropy: 263.8045, Loss: 0.00011398\n",
            "Iteration 9/10, Entropy: 263.8236, Loss: 0.00011150\n",
            "Iteration 9/10, Entropy: 263.8512, Loss: 0.00011081\n",
            "Iteration 9/10, Entropy: 263.8710, Loss: 0.00011153\n",
            "Iteration 9/10, Entropy: 263.8858, Loss: 0.00011111\n",
            "Iteration 10/10, Entropy: 263.9034, Loss: 0.00011182\n",
            "Iteration 10/10, Entropy: 263.9272, Loss: 0.00010983\n",
            "Iteration 10/10, Entropy: 263.9511, Loss: 0.00011092\n",
            "Iteration 10/10, Entropy: 263.9722, Loss: 0.00011073\n",
            "Iteration 10/10, Entropy: 263.9933, Loss: 0.00011078\n",
            "weight diff tensor(2.2805e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.q_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 273.0677, Loss: 0.00267150\n",
            "Iteration 1/10, Entropy: 273.0700, Loss: 0.00255457\n",
            "Iteration 1/10, Entropy: 273.0828, Loss: 0.00254315\n",
            "Iteration 1/10, Entropy: 273.0864, Loss: 0.00254049\n",
            "Iteration 1/10, Entropy: 273.0850, Loss: 0.00253016\n",
            "Iteration 2/10, Entropy: 273.0848, Loss: 0.00254222\n",
            "Iteration 2/10, Entropy: 273.0812, Loss: 0.00246758\n",
            "Iteration 2/10, Entropy: 273.0822, Loss: 0.00246586\n",
            "Iteration 2/10, Entropy: 273.0843, Loss: 0.00248713\n",
            "Iteration 2/10, Entropy: 273.0838, Loss: 0.00247166\n",
            "Iteration 3/10, Entropy: 273.0845, Loss: 0.00248841\n",
            "Iteration 3/10, Entropy: 273.0865, Loss: 0.00242556\n",
            "Iteration 3/10, Entropy: 273.0916, Loss: 0.00242935\n",
            "Iteration 3/10, Entropy: 273.0969, Loss: 0.00243631\n",
            "Iteration 3/10, Entropy: 273.0988, Loss: 0.00244004\n",
            "Iteration 4/10, Entropy: 273.0976, Loss: 0.00246608\n",
            "Iteration 4/10, Entropy: 273.0945, Loss: 0.00240151\n",
            "Iteration 4/10, Entropy: 273.0912, Loss: 0.00240686\n",
            "Iteration 4/10, Entropy: 273.0886, Loss: 0.00242253\n",
            "Iteration 4/10, Entropy: 273.0875, Loss: 0.00242665\n",
            "Iteration 5/10, Entropy: 273.0890, Loss: 0.00244600\n",
            "Iteration 5/10, Entropy: 273.0902, Loss: 0.00237954\n",
            "Iteration 5/10, Entropy: 273.0928, Loss: 0.00238644\n",
            "Iteration 5/10, Entropy: 273.0952, Loss: 0.00239918\n",
            "Iteration 5/10, Entropy: 273.0960, Loss: 0.00240993\n",
            "Iteration 6/10, Entropy: 273.0971, Loss: 0.00244757\n",
            "Iteration 6/10, Entropy: 273.0966, Loss: 0.00237206\n",
            "Iteration 6/10, Entropy: 273.0978, Loss: 0.00237292\n",
            "Iteration 6/10, Entropy: 273.0992, Loss: 0.00238955\n",
            "Iteration 6/10, Entropy: 273.1004, Loss: 0.00239381\n",
            "Iteration 7/10, Entropy: 273.1034, Loss: 0.00243179\n",
            "Iteration 7/10, Entropy: 273.1065, Loss: 0.00236857\n",
            "Iteration 7/10, Entropy: 273.1114, Loss: 0.00236000\n",
            "Iteration 7/10, Entropy: 273.1152, Loss: 0.00237823\n",
            "Iteration 7/10, Entropy: 273.1174, Loss: 0.00238699\n",
            "Iteration 8/10, Entropy: 273.1190, Loss: 0.00242693\n",
            "Iteration 8/10, Entropy: 273.1200, Loss: 0.00235524\n",
            "Iteration 8/10, Entropy: 273.1223, Loss: 0.00236058\n",
            "Iteration 8/10, Entropy: 273.1235, Loss: 0.00237938\n",
            "Iteration 8/10, Entropy: 273.1239, Loss: 0.00238510\n",
            "Iteration 9/10, Entropy: 273.1239, Loss: 0.00242099\n",
            "Iteration 9/10, Entropy: 273.1241, Loss: 0.00234965\n",
            "Iteration 9/10, Entropy: 273.1273, Loss: 0.00235292\n",
            "Iteration 9/10, Entropy: 273.1301, Loss: 0.00237199\n",
            "Iteration 9/10, Entropy: 273.1313, Loss: 0.00238072\n",
            "Iteration 10/10, Entropy: 273.1312, Loss: 0.00242068\n",
            "Iteration 10/10, Entropy: 273.1311, Loss: 0.00233821\n",
            "Iteration 10/10, Entropy: 273.1357, Loss: 0.00234992\n",
            "Iteration 10/10, Entropy: 273.1411, Loss: 0.00236960\n",
            "Iteration 10/10, Entropy: 273.1443, Loss: 0.00237731\n",
            "weight diff tensor(4.1128e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.self_attn.out_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 237.1292, Loss: 0.00003444\n",
            "Iteration 1/10, Entropy: 237.1464, Loss: 0.00003290\n",
            "Iteration 1/10, Entropy: 236.9838, Loss: 0.00003221\n",
            "Iteration 1/10, Entropy: 236.9638, Loss: 0.00003092\n",
            "Iteration 1/10, Entropy: 237.0281, Loss: 0.00003087\n",
            "Iteration 2/10, Entropy: 237.0759, Loss: 0.00003161\n",
            "Iteration 2/10, Entropy: 237.1357, Loss: 0.00003044\n",
            "Iteration 2/10, Entropy: 237.1721, Loss: 0.00003028\n",
            "Iteration 2/10, Entropy: 237.2207, Loss: 0.00002990\n",
            "Iteration 2/10, Entropy: 237.2870, Loss: 0.00003002\n",
            "Iteration 3/10, Entropy: 237.3675, Loss: 0.00003104\n",
            "Iteration 3/10, Entropy: 237.4455, Loss: 0.00003004\n",
            "Iteration 3/10, Entropy: 237.5063, Loss: 0.00002993\n",
            "Iteration 3/10, Entropy: 237.5398, Loss: 0.00002927\n",
            "Iteration 3/10, Entropy: 237.5467, Loss: 0.00002961\n",
            "Iteration 4/10, Entropy: 237.5376, Loss: 0.00003062\n",
            "Iteration 4/10, Entropy: 237.5383, Loss: 0.00002955\n",
            "Iteration 4/10, Entropy: 237.5434, Loss: 0.00002944\n",
            "Iteration 4/10, Entropy: 237.5584, Loss: 0.00002879\n",
            "Iteration 4/10, Entropy: 237.5927, Loss: 0.00002907\n",
            "Iteration 5/10, Entropy: 237.6321, Loss: 0.00003018\n",
            "Iteration 5/10, Entropy: 237.6709, Loss: 0.00002902\n",
            "Iteration 5/10, Entropy: 237.6779, Loss: 0.00002894\n",
            "Iteration 5/10, Entropy: 237.6686, Loss: 0.00002819\n",
            "Iteration 5/10, Entropy: 237.6725, Loss: 0.00002862\n",
            "Iteration 6/10, Entropy: 237.6916, Loss: 0.00002967\n",
            "Iteration 6/10, Entropy: 237.7315, Loss: 0.00002864\n",
            "Iteration 6/10, Entropy: 237.7737, Loss: 0.00002861\n",
            "Iteration 6/10, Entropy: 237.8173, Loss: 0.00002789\n",
            "Iteration 6/10, Entropy: 237.8694, Loss: 0.00002829\n",
            "Iteration 7/10, Entropy: 237.9175, Loss: 0.00002920\n",
            "Iteration 7/10, Entropy: 237.9618, Loss: 0.00002842\n",
            "Iteration 7/10, Entropy: 237.9978, Loss: 0.00002837\n",
            "Iteration 7/10, Entropy: 238.0303, Loss: 0.00002773\n",
            "Iteration 7/10, Entropy: 238.0683, Loss: 0.00002829\n",
            "Iteration 8/10, Entropy: 238.0969, Loss: 0.00002901\n",
            "Iteration 8/10, Entropy: 238.1112, Loss: 0.00002820\n",
            "Iteration 8/10, Entropy: 238.1068, Loss: 0.00002815\n",
            "Iteration 8/10, Entropy: 238.1030, Loss: 0.00002745\n",
            "Iteration 8/10, Entropy: 238.1163, Loss: 0.00002799\n",
            "Iteration 9/10, Entropy: 238.1441, Loss: 0.00002880\n",
            "Iteration 9/10, Entropy: 238.1885, Loss: 0.00002799\n",
            "Iteration 9/10, Entropy: 238.2385, Loss: 0.00002789\n",
            "Iteration 9/10, Entropy: 238.2836, Loss: 0.00002730\n",
            "Iteration 9/10, Entropy: 238.3355, Loss: 0.00002786\n",
            "Iteration 10/10, Entropy: 238.3861, Loss: 0.00002861\n",
            "Iteration 10/10, Entropy: 238.4356, Loss: 0.00002781\n",
            "Iteration 10/10, Entropy: 238.4698, Loss: 0.00002781\n",
            "Iteration 10/10, Entropy: 238.4968, Loss: 0.00002714\n",
            "Iteration 10/10, Entropy: 238.5226, Loss: 0.00002767\n",
            "weight diff tensor(1.4333e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.0.fc1 | Shape: torch.Size([3072, 768])\n",
            "Iteration 1/10, Entropy: 1014.0167, Loss: 0.00035191\n",
            "Iteration 1/10, Entropy: 1014.1606, Loss: 0.00034266\n",
            "Iteration 1/10, Entropy: 1014.3368, Loss: 0.00033493\n",
            "Iteration 1/10, Entropy: 1014.4865, Loss: 0.00032508\n",
            "Iteration 1/10, Entropy: 1014.6319, Loss: 0.00032477\n",
            "Iteration 2/10, Entropy: 1014.7835, Loss: 0.00032401\n",
            "Iteration 2/10, Entropy: 1014.9139, Loss: 0.00032307\n",
            "Iteration 2/10, Entropy: 1015.0129, Loss: 0.00032276\n",
            "Iteration 2/10, Entropy: 1015.0828, Loss: 0.00031588\n",
            "Iteration 2/10, Entropy: 1015.1406, Loss: 0.00031718\n",
            "Iteration 3/10, Entropy: 1015.1888, Loss: 0.00031610\n",
            "Iteration 3/10, Entropy: 1015.2521, Loss: 0.00031490\n",
            "Iteration 3/10, Entropy: 1015.3267, Loss: 0.00031608\n",
            "Iteration 3/10, Entropy: 1015.3809, Loss: 0.00031068\n",
            "Iteration 3/10, Entropy: 1015.4187, Loss: 0.00031127\n",
            "Iteration 4/10, Entropy: 1015.4512, Loss: 0.00031081\n",
            "Iteration 4/10, Entropy: 1015.4866, Loss: 0.00031067\n",
            "Iteration 4/10, Entropy: 1015.5276, Loss: 0.00031026\n",
            "Iteration 4/10, Entropy: 1015.5646, Loss: 0.00030488\n",
            "Iteration 4/10, Entropy: 1015.6266, Loss: 0.00030696\n",
            "Iteration 5/10, Entropy: 1015.6952, Loss: 0.00030570\n",
            "Iteration 5/10, Entropy: 1015.7573, Loss: 0.00030668\n",
            "Iteration 5/10, Entropy: 1015.8050, Loss: 0.00030591\n",
            "Iteration 5/10, Entropy: 1015.8369, Loss: 0.00030142\n",
            "Iteration 5/10, Entropy: 1015.8721, Loss: 0.00030326\n",
            "Iteration 6/10, Entropy: 1015.9071, Loss: 0.00030298\n",
            "Iteration 6/10, Entropy: 1015.9464, Loss: 0.00030390\n",
            "Iteration 6/10, Entropy: 1015.9869, Loss: 0.00030302\n",
            "Iteration 6/10, Entropy: 1016.0121, Loss: 0.00029895\n",
            "Iteration 6/10, Entropy: 1016.0370, Loss: 0.00030085\n",
            "Iteration 7/10, Entropy: 1016.0637, Loss: 0.00029994\n",
            "Iteration 7/10, Entropy: 1016.0942, Loss: 0.00030103\n",
            "Iteration 7/10, Entropy: 1016.1326, Loss: 0.00030008\n",
            "Iteration 7/10, Entropy: 1016.1744, Loss: 0.00029716\n",
            "Iteration 7/10, Entropy: 1016.2218, Loss: 0.00029910\n",
            "Iteration 8/10, Entropy: 1016.2708, Loss: 0.00029816\n",
            "Iteration 8/10, Entropy: 1016.3240, Loss: 0.00029877\n",
            "Iteration 8/10, Entropy: 1016.3774, Loss: 0.00029826\n",
            "Iteration 8/10, Entropy: 1016.4185, Loss: 0.00029508\n",
            "Iteration 8/10, Entropy: 1016.4496, Loss: 0.00029682\n",
            "Iteration 9/10, Entropy: 1016.4720, Loss: 0.00029612\n",
            "Iteration 9/10, Entropy: 1016.5024, Loss: 0.00029685\n",
            "Iteration 9/10, Entropy: 1016.5241, Loss: 0.00029624\n",
            "Iteration 9/10, Entropy: 1016.5252, Loss: 0.00029287\n",
            "Iteration 9/10, Entropy: 1016.5336, Loss: 0.00029475\n",
            "Iteration 10/10, Entropy: 1016.5652, Loss: 0.00029388\n",
            "Iteration 10/10, Entropy: 1016.6122, Loss: 0.00029464\n",
            "Iteration 10/10, Entropy: 1016.6644, Loss: 0.00029427\n",
            "Iteration 10/10, Entropy: 1016.7119, Loss: 0.00029142\n",
            "Iteration 10/10, Entropy: 1016.7593, Loss: 0.00029338\n",
            "weight diff tensor(2.2377e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.1.self_attn.k_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 269.4810, Loss: 0.00310985\n",
            "Iteration 1/10, Entropy: 269.5295, Loss: 0.00289653\n",
            "Iteration 1/10, Entropy: 269.5620, Loss: 0.00283445\n",
            "Iteration 1/10, Entropy: 269.5814, Loss: 0.00288230\n",
            "Iteration 1/10, Entropy: 269.6082, Loss: 0.00279976\n",
            "Iteration 2/10, Entropy: 269.6384, Loss: 0.00284410\n",
            "Iteration 2/10, Entropy: 269.6676, Loss: 0.00270980\n",
            "Iteration 2/10, Entropy: 269.6921, Loss: 0.00274092\n",
            "Iteration 2/10, Entropy: 269.7091, Loss: 0.00279542\n",
            "Iteration 2/10, Entropy: 269.7207, Loss: 0.00272064\n",
            "Iteration 3/10, Entropy: 269.7260, Loss: 0.00278451\n",
            "Iteration 3/10, Entropy: 269.7287, Loss: 0.00266603\n",
            "Iteration 3/10, Entropy: 269.7281, Loss: 0.00269757\n",
            "Iteration 3/10, Entropy: 269.7294, Loss: 0.00274685\n",
            "Iteration 3/10, Entropy: 269.7345, Loss: 0.00268686\n",
            "Iteration 4/10, Entropy: 269.7390, Loss: 0.00274088\n",
            "Iteration 4/10, Entropy: 269.7449, Loss: 0.00263093\n",
            "Iteration 4/10, Entropy: 269.7496, Loss: 0.00267109\n",
            "Iteration 4/10, Entropy: 269.7500, Loss: 0.00272260\n",
            "Iteration 4/10, Entropy: 269.7500, Loss: 0.00266820\n",
            "Iteration 5/10, Entropy: 269.7499, Loss: 0.00272001\n",
            "Iteration 5/10, Entropy: 269.7505, Loss: 0.00260727\n",
            "Iteration 5/10, Entropy: 269.7519, Loss: 0.00264088\n",
            "Iteration 5/10, Entropy: 269.7540, Loss: 0.00268841\n",
            "Iteration 5/10, Entropy: 269.7582, Loss: 0.00264149\n",
            "Iteration 6/10, Entropy: 269.7606, Loss: 0.00271008\n",
            "Iteration 6/10, Entropy: 269.7603, Loss: 0.00258652\n",
            "Iteration 6/10, Entropy: 269.7564, Loss: 0.00261771\n",
            "Iteration 6/10, Entropy: 269.7494, Loss: 0.00267756\n",
            "Iteration 6/10, Entropy: 269.7446, Loss: 0.00262806\n",
            "Iteration 7/10, Entropy: 269.7402, Loss: 0.00269100\n",
            "Iteration 7/10, Entropy: 269.7370, Loss: 0.00257017\n",
            "Iteration 7/10, Entropy: 269.7352, Loss: 0.00260277\n",
            "Iteration 7/10, Entropy: 269.7357, Loss: 0.00266274\n",
            "Iteration 7/10, Entropy: 269.7400, Loss: 0.00261583\n",
            "Iteration 8/10, Entropy: 269.7458, Loss: 0.00268480\n",
            "Iteration 8/10, Entropy: 269.7527, Loss: 0.00256794\n",
            "Iteration 8/10, Entropy: 269.7591, Loss: 0.00259918\n",
            "Iteration 8/10, Entropy: 269.7649, Loss: 0.00265471\n",
            "Iteration 8/10, Entropy: 269.7709, Loss: 0.00260630\n",
            "Iteration 9/10, Entropy: 269.7742, Loss: 0.00267589\n",
            "Iteration 9/10, Entropy: 269.7752, Loss: 0.00256106\n",
            "Iteration 9/10, Entropy: 269.7748, Loss: 0.00259437\n",
            "Iteration 9/10, Entropy: 269.7734, Loss: 0.00265039\n",
            "Iteration 9/10, Entropy: 269.7735, Loss: 0.00260180\n",
            "Iteration 10/10, Entropy: 269.7740, Loss: 0.00267183\n",
            "Iteration 10/10, Entropy: 269.7741, Loss: 0.00255376\n",
            "Iteration 10/10, Entropy: 269.7743, Loss: 0.00258574\n",
            "Iteration 10/10, Entropy: 269.7758, Loss: 0.00264559\n",
            "Iteration 10/10, Entropy: 269.7796, Loss: 0.00259524\n",
            "weight diff tensor(3.8207e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.1.self_attn.v_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 269.2073, Loss: 0.00012504\n",
            "Iteration 1/10, Entropy: 269.2713, Loss: 0.00011674\n",
            "Iteration 1/10, Entropy: 269.3554, Loss: 0.00011678\n",
            "Iteration 1/10, Entropy: 269.4594, Loss: 0.00011518\n",
            "Iteration 1/10, Entropy: 269.5576, Loss: 0.00011389\n",
            "Iteration 2/10, Entropy: 269.6588, Loss: 0.00011000\n",
            "Iteration 2/10, Entropy: 269.7355, Loss: 0.00010934\n",
            "Iteration 2/10, Entropy: 269.7802, Loss: 0.00010585\n",
            "Iteration 2/10, Entropy: 269.8266, Loss: 0.00011094\n",
            "Iteration 2/10, Entropy: 269.8551, Loss: 0.00010912\n",
            "Iteration 3/10, Entropy: 269.8903, Loss: 0.00010841\n",
            "Iteration 3/10, Entropy: 269.9193, Loss: 0.00010712\n",
            "Iteration 3/10, Entropy: 269.9464, Loss: 0.00010475\n",
            "Iteration 3/10, Entropy: 269.9731, Loss: 0.00010765\n",
            "Iteration 3/10, Entropy: 269.9922, Loss: 0.00010649\n",
            "Iteration 4/10, Entropy: 270.0171, Loss: 0.00010797\n",
            "Iteration 4/10, Entropy: 270.0526, Loss: 0.00010492\n",
            "Iteration 4/10, Entropy: 270.0928, Loss: 0.00010313\n",
            "Iteration 4/10, Entropy: 270.1279, Loss: 0.00010394\n",
            "Iteration 4/10, Entropy: 270.1638, Loss: 0.00010185\n",
            "Iteration 5/10, Entropy: 270.1974, Loss: 0.00010324\n",
            "Iteration 5/10, Entropy: 270.2249, Loss: 0.00010210\n",
            "Iteration 5/10, Entropy: 270.2260, Loss: 0.00010019\n",
            "Iteration 5/10, Entropy: 270.2355, Loss: 0.00010260\n",
            "Iteration 5/10, Entropy: 270.2450, Loss: 0.00010275\n",
            "Iteration 6/10, Entropy: 270.2605, Loss: 0.00010495\n",
            "Iteration 6/10, Entropy: 270.2736, Loss: 0.00010149\n",
            "Iteration 6/10, Entropy: 270.2804, Loss: 0.00009823\n",
            "Iteration 6/10, Entropy: 270.2949, Loss: 0.00009997\n",
            "Iteration 6/10, Entropy: 270.3041, Loss: 0.00010054\n",
            "Iteration 7/10, Entropy: 270.3086, Loss: 0.00010208\n",
            "Iteration 7/10, Entropy: 270.3027, Loss: 0.00009928\n",
            "Iteration 7/10, Entropy: 270.2839, Loss: 0.00009651\n",
            "Iteration 7/10, Entropy: 270.2761, Loss: 0.00009961\n",
            "Iteration 7/10, Entropy: 270.2928, Loss: 0.00009872\n",
            "Iteration 8/10, Entropy: 270.3129, Loss: 0.00010124\n",
            "Iteration 8/10, Entropy: 270.3326, Loss: 0.00009838\n",
            "Iteration 8/10, Entropy: 270.3535, Loss: 0.00009719\n",
            "Iteration 8/10, Entropy: 270.3691, Loss: 0.00009917\n",
            "Iteration 8/10, Entropy: 270.3715, Loss: 0.00009877\n",
            "Iteration 9/10, Entropy: 270.3645, Loss: 0.00010134\n",
            "Iteration 9/10, Entropy: 270.3607, Loss: 0.00009770\n",
            "Iteration 9/10, Entropy: 270.3630, Loss: 0.00009595\n",
            "Iteration 9/10, Entropy: 270.3709, Loss: 0.00009905\n",
            "Iteration 9/10, Entropy: 270.3790, Loss: 0.00009848\n",
            "Iteration 10/10, Entropy: 270.3892, Loss: 0.00010095\n",
            "Iteration 10/10, Entropy: 270.4027, Loss: 0.00009713\n",
            "Iteration 10/10, Entropy: 270.4041, Loss: 0.00009554\n",
            "Iteration 10/10, Entropy: 270.4046, Loss: 0.00009829\n",
            "Iteration 10/10, Entropy: 270.4056, Loss: 0.00009718\n",
            "weight diff tensor(2.1286e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.1.self_attn.q_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 266.4565, Loss: 0.00208643\n",
            "Iteration 1/10, Entropy: 266.4821, Loss: 0.00196792\n",
            "Iteration 1/10, Entropy: 266.5221, Loss: 0.00191702\n",
            "Iteration 1/10, Entropy: 266.5345, Loss: 0.00190814\n",
            "Iteration 1/10, Entropy: 266.5405, Loss: 0.00187750\n",
            "Iteration 2/10, Entropy: 266.5540, Loss: 0.00188069\n",
            "Iteration 2/10, Entropy: 266.5709, Loss: 0.00185763\n",
            "Iteration 2/10, Entropy: 266.5863, Loss: 0.00187288\n",
            "Iteration 2/10, Entropy: 266.5960, Loss: 0.00185567\n",
            "Iteration 2/10, Entropy: 266.6019, Loss: 0.00184720\n",
            "Iteration 3/10, Entropy: 266.6148, Loss: 0.00182950\n",
            "Iteration 3/10, Entropy: 266.6416, Loss: 0.00181805\n",
            "Iteration 3/10, Entropy: 266.6740, Loss: 0.00180752\n",
            "Iteration 3/10, Entropy: 266.7019, Loss: 0.00180291\n",
            "Iteration 3/10, Entropy: 266.7130, Loss: 0.00179859\n",
            "Iteration 4/10, Entropy: 266.7061, Loss: 0.00179492\n",
            "Iteration 4/10, Entropy: 266.6933, Loss: 0.00177076\n",
            "Iteration 4/10, Entropy: 266.6748, Loss: 0.00178857\n",
            "Iteration 4/10, Entropy: 266.6559, Loss: 0.00177777\n",
            "Iteration 4/10, Entropy: 266.6420, Loss: 0.00177701\n",
            "Iteration 5/10, Entropy: 266.6383, Loss: 0.00177591\n",
            "Iteration 5/10, Entropy: 266.6453, Loss: 0.00173787\n",
            "Iteration 5/10, Entropy: 266.6566, Loss: 0.00176209\n",
            "Iteration 5/10, Entropy: 266.6732, Loss: 0.00175847\n",
            "Iteration 5/10, Entropy: 266.6880, Loss: 0.00175419\n",
            "Iteration 6/10, Entropy: 266.6997, Loss: 0.00175160\n",
            "Iteration 6/10, Entropy: 266.7101, Loss: 0.00172134\n",
            "Iteration 6/10, Entropy: 266.7128, Loss: 0.00173446\n",
            "Iteration 6/10, Entropy: 266.7109, Loss: 0.00173649\n",
            "Iteration 6/10, Entropy: 266.7054, Loss: 0.00173337\n",
            "Iteration 7/10, Entropy: 266.7008, Loss: 0.00173335\n",
            "Iteration 7/10, Entropy: 266.6995, Loss: 0.00170204\n",
            "Iteration 7/10, Entropy: 266.6986, Loss: 0.00172162\n",
            "Iteration 7/10, Entropy: 266.7012, Loss: 0.00173264\n",
            "Iteration 7/10, Entropy: 266.7017, Loss: 0.00172518\n",
            "Iteration 8/10, Entropy: 266.7003, Loss: 0.00173350\n",
            "Iteration 8/10, Entropy: 266.7022, Loss: 0.00170063\n",
            "Iteration 8/10, Entropy: 266.7048, Loss: 0.00171554\n",
            "Iteration 8/10, Entropy: 266.7088, Loss: 0.00172514\n",
            "Iteration 8/10, Entropy: 266.7105, Loss: 0.00171797\n",
            "Iteration 9/10, Entropy: 266.7097, Loss: 0.00172243\n",
            "Iteration 9/10, Entropy: 266.7121, Loss: 0.00169395\n",
            "Iteration 9/10, Entropy: 266.7129, Loss: 0.00170629\n",
            "Iteration 9/10, Entropy: 266.7162, Loss: 0.00171758\n",
            "Iteration 9/10, Entropy: 266.7178, Loss: 0.00171076\n",
            "Iteration 10/10, Entropy: 266.7171, Loss: 0.00171662\n",
            "Iteration 10/10, Entropy: 266.7196, Loss: 0.00168710\n",
            "Iteration 10/10, Entropy: 266.7220, Loss: 0.00169528\n",
            "Iteration 10/10, Entropy: 266.7246, Loss: 0.00170651\n",
            "Iteration 10/10, Entropy: 266.7249, Loss: 0.00170178\n",
            "weight diff tensor(2.8619e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.1.self_attn.out_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 234.5117, Loss: 0.00003971\n",
            "Iteration 1/10, Entropy: 234.4275, Loss: 0.00003711\n",
            "Iteration 1/10, Entropy: 234.3449, Loss: 0.00003538\n",
            "Iteration 1/10, Entropy: 234.2774, Loss: 0.00003406\n",
            "Iteration 1/10, Entropy: 234.2779, Loss: 0.00003360\n",
            "Iteration 2/10, Entropy: 234.3035, Loss: 0.00003441\n",
            "Iteration 2/10, Entropy: 234.3858, Loss: 0.00003248\n",
            "Iteration 2/10, Entropy: 234.4378, Loss: 0.00003186\n",
            "Iteration 2/10, Entropy: 234.4475, Loss: 0.00003090\n",
            "Iteration 2/10, Entropy: 234.4544, Loss: 0.00003129\n",
            "Iteration 3/10, Entropy: 234.4778, Loss: 0.00003238\n",
            "Iteration 3/10, Entropy: 234.5219, Loss: 0.00003115\n",
            "Iteration 3/10, Entropy: 234.5588, Loss: 0.00003071\n",
            "Iteration 3/10, Entropy: 234.5813, Loss: 0.00003027\n",
            "Iteration 3/10, Entropy: 234.6036, Loss: 0.00003047\n",
            "Iteration 4/10, Entropy: 234.6298, Loss: 0.00003183\n",
            "Iteration 4/10, Entropy: 234.6809, Loss: 0.00003023\n",
            "Iteration 4/10, Entropy: 234.7396, Loss: 0.00002981\n",
            "Iteration 4/10, Entropy: 234.7800, Loss: 0.00002946\n",
            "Iteration 4/10, Entropy: 234.8006, Loss: 0.00002982\n",
            "Iteration 5/10, Entropy: 234.8218, Loss: 0.00003157\n",
            "Iteration 5/10, Entropy: 234.8536, Loss: 0.00002989\n",
            "Iteration 5/10, Entropy: 234.8865, Loss: 0.00002945\n",
            "Iteration 5/10, Entropy: 234.9042, Loss: 0.00002893\n",
            "Iteration 5/10, Entropy: 234.9238, Loss: 0.00002936\n",
            "Iteration 6/10, Entropy: 234.9373, Loss: 0.00003072\n",
            "Iteration 6/10, Entropy: 234.9674, Loss: 0.00002910\n",
            "Iteration 6/10, Entropy: 235.0086, Loss: 0.00002864\n",
            "Iteration 6/10, Entropy: 235.0499, Loss: 0.00002829\n",
            "Iteration 6/10, Entropy: 235.0763, Loss: 0.00002868\n",
            "Iteration 7/10, Entropy: 235.0895, Loss: 0.00003030\n",
            "Iteration 7/10, Entropy: 235.1081, Loss: 0.00002856\n",
            "Iteration 7/10, Entropy: 235.1337, Loss: 0.00002806\n",
            "Iteration 7/10, Entropy: 235.1675, Loss: 0.00002772\n",
            "Iteration 7/10, Entropy: 235.2056, Loss: 0.00002789\n",
            "Iteration 8/10, Entropy: 235.2557, Loss: 0.00002937\n",
            "Iteration 8/10, Entropy: 235.3492, Loss: 0.00002748\n",
            "Iteration 8/10, Entropy: 235.4663, Loss: 0.00002702\n",
            "Iteration 8/10, Entropy: 235.5743, Loss: 0.00002665\n",
            "Iteration 8/10, Entropy: 235.6568, Loss: 0.00002708\n",
            "Iteration 9/10, Entropy: 235.7269, Loss: 0.00002865\n",
            "Iteration 9/10, Entropy: 235.7957, Loss: 0.00002701\n",
            "Iteration 9/10, Entropy: 235.8754, Loss: 0.00002664\n",
            "Iteration 9/10, Entropy: 235.9591, Loss: 0.00002625\n",
            "Iteration 9/10, Entropy: 236.0237, Loss: 0.00002670\n",
            "Iteration 10/10, Entropy: 236.0843, Loss: 0.00002819\n",
            "Iteration 10/10, Entropy: 236.1504, Loss: 0.00002663\n",
            "Iteration 10/10, Entropy: 236.2155, Loss: 0.00002615\n",
            "Iteration 10/10, Entropy: 236.2737, Loss: 0.00002596\n",
            "Iteration 10/10, Entropy: 236.3071, Loss: 0.00002636\n",
            "weight diff tensor(2.0043e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.1.fc1 | Shape: torch.Size([3072, 768])\n",
            "Iteration 1/10, Entropy: 999.1394, Loss: 0.00211967\n",
            "Iteration 1/10, Entropy: 999.0959, Loss: 0.00170273\n",
            "Iteration 1/10, Entropy: 999.0449, Loss: 0.00156007\n",
            "Iteration 1/10, Entropy: 998.9914, Loss: 0.00147525\n",
            "Iteration 1/10, Entropy: 998.9584, Loss: 0.00139944\n",
            "Iteration 2/10, Entropy: 998.9539, Loss: 0.00139368\n",
            "Iteration 2/10, Entropy: 998.9747, Loss: 0.00140221\n",
            "Iteration 2/10, Entropy: 998.9971, Loss: 0.00138604\n",
            "Iteration 2/10, Entropy: 999.0057, Loss: 0.00137713\n",
            "Iteration 2/10, Entropy: 999.0079, Loss: 0.00134753\n",
            "Iteration 3/10, Entropy: 999.0240, Loss: 0.00133732\n",
            "Iteration 3/10, Entropy: 999.0421, Loss: 0.00133801\n",
            "Iteration 3/10, Entropy: 999.0642, Loss: 0.00132870\n",
            "Iteration 3/10, Entropy: 999.0890, Loss: 0.00132550\n",
            "Iteration 3/10, Entropy: 999.1193, Loss: 0.00128583\n",
            "Iteration 4/10, Entropy: 999.1390, Loss: 0.00126911\n",
            "Iteration 4/10, Entropy: 999.1583, Loss: 0.00126419\n",
            "Iteration 4/10, Entropy: 999.1870, Loss: 0.00124794\n",
            "Iteration 4/10, Entropy: 999.2225, Loss: 0.00124488\n",
            "Iteration 4/10, Entropy: 999.2651, Loss: 0.00121345\n",
            "Iteration 5/10, Entropy: 999.3077, Loss: 0.00120911\n",
            "Iteration 5/10, Entropy: 999.3430, Loss: 0.00121007\n",
            "Iteration 5/10, Entropy: 999.3686, Loss: 0.00120160\n",
            "Iteration 5/10, Entropy: 999.3832, Loss: 0.00121317\n",
            "Iteration 5/10, Entropy: 999.3849, Loss: 0.00118747\n",
            "Iteration 6/10, Entropy: 999.3826, Loss: 0.00119011\n",
            "Iteration 6/10, Entropy: 999.3879, Loss: 0.00119174\n",
            "Iteration 6/10, Entropy: 999.3947, Loss: 0.00118169\n",
            "Iteration 6/10, Entropy: 999.4086, Loss: 0.00119338\n",
            "Iteration 6/10, Entropy: 999.4230, Loss: 0.00117944\n",
            "Iteration 7/10, Entropy: 999.4413, Loss: 0.00118228\n",
            "Iteration 7/10, Entropy: 999.4636, Loss: 0.00117901\n",
            "Iteration 7/10, Entropy: 999.4771, Loss: 0.00116364\n",
            "Iteration 7/10, Entropy: 999.4861, Loss: 0.00117285\n",
            "Iteration 7/10, Entropy: 999.4880, Loss: 0.00116066\n",
            "Iteration 8/10, Entropy: 999.4868, Loss: 0.00116941\n",
            "Iteration 8/10, Entropy: 999.4857, Loss: 0.00116568\n",
            "Iteration 8/10, Entropy: 999.4832, Loss: 0.00115159\n",
            "Iteration 8/10, Entropy: 999.4854, Loss: 0.00116453\n",
            "Iteration 8/10, Entropy: 999.4889, Loss: 0.00114811\n",
            "Iteration 9/10, Entropy: 999.4963, Loss: 0.00115406\n",
            "Iteration 9/10, Entropy: 999.5082, Loss: 0.00115194\n",
            "Iteration 9/10, Entropy: 999.5210, Loss: 0.00113943\n",
            "Iteration 9/10, Entropy: 999.5351, Loss: 0.00115116\n",
            "Iteration 9/10, Entropy: 999.5484, Loss: 0.00113700\n",
            "Iteration 10/10, Entropy: 999.5646, Loss: 0.00114452\n",
            "Iteration 10/10, Entropy: 999.5854, Loss: 0.00114617\n",
            "Iteration 10/10, Entropy: 999.6014, Loss: 0.00113017\n",
            "Iteration 10/10, Entropy: 999.6152, Loss: 0.00114370\n",
            "Iteration 10/10, Entropy: 999.6262, Loss: 0.00112947\n",
            "weight diff tensor(2.8657e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.2.self_attn.k_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 270.0419, Loss: 0.00616568\n",
            "Iteration 1/10, Entropy: 270.0094, Loss: 0.00567003\n",
            "Iteration 1/10, Entropy: 270.0118, Loss: 0.00530434\n",
            "Iteration 1/10, Entropy: 270.0239, Loss: 0.00531351\n",
            "Iteration 1/10, Entropy: 270.0279, Loss: 0.00519496\n",
            "Iteration 2/10, Entropy: 270.0261, Loss: 0.00523999\n",
            "Iteration 2/10, Entropy: 270.0223, Loss: 0.00505926\n",
            "Iteration 2/10, Entropy: 270.0124, Loss: 0.00494983\n",
            "Iteration 2/10, Entropy: 270.0058, Loss: 0.00503696\n",
            "Iteration 2/10, Entropy: 269.9996, Loss: 0.00497398\n",
            "Iteration 3/10, Entropy: 269.9931, Loss: 0.00498524\n",
            "Iteration 3/10, Entropy: 269.9854, Loss: 0.00478928\n",
            "Iteration 3/10, Entropy: 269.9795, Loss: 0.00467545\n",
            "Iteration 3/10, Entropy: 269.9771, Loss: 0.00478459\n",
            "Iteration 3/10, Entropy: 269.9789, Loss: 0.00469862\n",
            "Iteration 4/10, Entropy: 269.9833, Loss: 0.00478838\n",
            "Iteration 4/10, Entropy: 269.9866, Loss: 0.00466876\n",
            "Iteration 4/10, Entropy: 269.9894, Loss: 0.00456286\n",
            "Iteration 4/10, Entropy: 269.9925, Loss: 0.00468019\n",
            "Iteration 4/10, Entropy: 269.9908, Loss: 0.00459370\n",
            "Iteration 5/10, Entropy: 269.9863, Loss: 0.00468855\n",
            "Iteration 5/10, Entropy: 269.9815, Loss: 0.00457753\n",
            "Iteration 5/10, Entropy: 269.9783, Loss: 0.00447715\n",
            "Iteration 5/10, Entropy: 269.9776, Loss: 0.00463075\n",
            "Iteration 5/10, Entropy: 269.9757, Loss: 0.00452369\n",
            "Iteration 6/10, Entropy: 269.9744, Loss: 0.00461348\n",
            "Iteration 6/10, Entropy: 269.9761, Loss: 0.00449012\n",
            "Iteration 6/10, Entropy: 269.9807, Loss: 0.00439213\n",
            "Iteration 6/10, Entropy: 269.9881, Loss: 0.00454911\n",
            "Iteration 6/10, Entropy: 269.9936, Loss: 0.00447194\n",
            "Iteration 7/10, Entropy: 269.9971, Loss: 0.00456485\n",
            "Iteration 7/10, Entropy: 270.0008, Loss: 0.00444547\n",
            "Iteration 7/10, Entropy: 270.0034, Loss: 0.00435106\n",
            "Iteration 7/10, Entropy: 270.0078, Loss: 0.00449893\n",
            "Iteration 7/10, Entropy: 270.0115, Loss: 0.00442162\n",
            "Iteration 8/10, Entropy: 270.0154, Loss: 0.00454806\n",
            "Iteration 8/10, Entropy: 270.0187, Loss: 0.00443158\n",
            "Iteration 8/10, Entropy: 270.0208, Loss: 0.00433424\n",
            "Iteration 8/10, Entropy: 270.0224, Loss: 0.00448347\n",
            "Iteration 8/10, Entropy: 270.0202, Loss: 0.00440587\n",
            "Iteration 9/10, Entropy: 270.0156, Loss: 0.00452503\n",
            "Iteration 9/10, Entropy: 270.0103, Loss: 0.00440117\n",
            "Iteration 9/10, Entropy: 270.0065, Loss: 0.00430773\n",
            "Iteration 9/10, Entropy: 270.0069, Loss: 0.00445260\n",
            "Iteration 9/10, Entropy: 270.0075, Loss: 0.00437201\n",
            "Iteration 10/10, Entropy: 270.0086, Loss: 0.00450812\n",
            "Iteration 10/10, Entropy: 270.0091, Loss: 0.00437792\n",
            "Iteration 10/10, Entropy: 270.0089, Loss: 0.00428618\n",
            "Iteration 10/10, Entropy: 270.0099, Loss: 0.00444142\n",
            "Iteration 10/10, Entropy: 270.0095, Loss: 0.00436423\n",
            "weight diff tensor(3.4823e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.2.self_attn.v_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 272.3033, Loss: 0.00034192\n",
            "Iteration 1/10, Entropy: 272.2190, Loss: 0.00031331\n",
            "Iteration 1/10, Entropy: 272.2013, Loss: 0.00029097\n",
            "Iteration 1/10, Entropy: 272.2225, Loss: 0.00028760\n",
            "Iteration 1/10, Entropy: 272.2288, Loss: 0.00029538\n",
            "Iteration 2/10, Entropy: 272.2377, Loss: 0.00028531\n",
            "Iteration 2/10, Entropy: 272.2550, Loss: 0.00027830\n",
            "Iteration 2/10, Entropy: 272.2706, Loss: 0.00027174\n",
            "Iteration 2/10, Entropy: 272.2826, Loss: 0.00027457\n",
            "Iteration 2/10, Entropy: 272.2838, Loss: 0.00027085\n",
            "Iteration 3/10, Entropy: 272.2864, Loss: 0.00027038\n",
            "Iteration 3/10, Entropy: 272.2917, Loss: 0.00027319\n",
            "Iteration 3/10, Entropy: 272.3015, Loss: 0.00027075\n",
            "Iteration 3/10, Entropy: 272.3175, Loss: 0.00027055\n",
            "Iteration 3/10, Entropy: 272.3085, Loss: 0.00026303\n",
            "Iteration 4/10, Entropy: 272.3108, Loss: 0.00025717\n",
            "Iteration 4/10, Entropy: 272.3193, Loss: 0.00025952\n",
            "Iteration 4/10, Entropy: 272.3385, Loss: 0.00025551\n",
            "Iteration 4/10, Entropy: 272.3560, Loss: 0.00026033\n",
            "Iteration 4/10, Entropy: 272.3663, Loss: 0.00025045\n",
            "Iteration 5/10, Entropy: 272.3688, Loss: 0.00024790\n",
            "Iteration 5/10, Entropy: 272.3802, Loss: 0.00024934\n",
            "Iteration 5/10, Entropy: 272.3893, Loss: 0.00024759\n",
            "Iteration 5/10, Entropy: 272.3989, Loss: 0.00025173\n",
            "Iteration 5/10, Entropy: 272.4030, Loss: 0.00024858\n",
            "Iteration 6/10, Entropy: 272.3989, Loss: 0.00024676\n",
            "Iteration 6/10, Entropy: 272.3973, Loss: 0.00024408\n",
            "Iteration 6/10, Entropy: 272.3974, Loss: 0.00024142\n",
            "Iteration 6/10, Entropy: 272.4042, Loss: 0.00025028\n",
            "Iteration 6/10, Entropy: 272.4199, Loss: 0.00024440\n",
            "Iteration 7/10, Entropy: 272.4247, Loss: 0.00024544\n",
            "Iteration 7/10, Entropy: 272.4229, Loss: 0.00024002\n",
            "Iteration 7/10, Entropy: 272.4195, Loss: 0.00023788\n",
            "Iteration 7/10, Entropy: 272.4245, Loss: 0.00024259\n",
            "Iteration 7/10, Entropy: 272.4347, Loss: 0.00023576\n",
            "Iteration 8/10, Entropy: 272.4391, Loss: 0.00023813\n",
            "Iteration 8/10, Entropy: 272.4472, Loss: 0.00023400\n",
            "Iteration 8/10, Entropy: 272.4692, Loss: 0.00023430\n",
            "Iteration 8/10, Entropy: 272.4767, Loss: 0.00023630\n",
            "Iteration 8/10, Entropy: 272.4779, Loss: 0.00023273\n",
            "Iteration 9/10, Entropy: 272.4781, Loss: 0.00023568\n",
            "Iteration 9/10, Entropy: 272.4871, Loss: 0.00023183\n",
            "Iteration 9/10, Entropy: 272.4986, Loss: 0.00023221\n",
            "Iteration 9/10, Entropy: 272.5078, Loss: 0.00023505\n",
            "Iteration 9/10, Entropy: 272.5124, Loss: 0.00023250\n",
            "Iteration 10/10, Entropy: 272.5071, Loss: 0.00023652\n",
            "Iteration 10/10, Entropy: 272.5090, Loss: 0.00022909\n",
            "Iteration 10/10, Entropy: 272.5177, Loss: 0.00022816\n",
            "Iteration 10/10, Entropy: 272.5201, Loss: 0.00023222\n",
            "Iteration 10/10, Entropy: 272.5173, Loss: 0.00022823\n",
            "weight diff tensor(2.3133e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.2.self_attn.q_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 269.3627, Loss: 0.00592252\n",
            "Iteration 1/10, Entropy: 269.3673, Loss: 0.00551370\n",
            "Iteration 1/10, Entropy: 269.3812, Loss: 0.00515697\n",
            "Iteration 1/10, Entropy: 269.3926, Loss: 0.00514046\n",
            "Iteration 1/10, Entropy: 269.3949, Loss: 0.00494570\n",
            "Iteration 2/10, Entropy: 269.3941, Loss: 0.00491840\n",
            "Iteration 2/10, Entropy: 269.3986, Loss: 0.00479843\n",
            "Iteration 2/10, Entropy: 269.4012, Loss: 0.00470892\n",
            "Iteration 2/10, Entropy: 269.3997, Loss: 0.00480639\n",
            "Iteration 2/10, Entropy: 269.3950, Loss: 0.00476069\n",
            "Iteration 3/10, Entropy: 269.3892, Loss: 0.00474814\n",
            "Iteration 3/10, Entropy: 269.3918, Loss: 0.00466843\n",
            "Iteration 3/10, Entropy: 269.3946, Loss: 0.00455778\n",
            "Iteration 3/10, Entropy: 269.3945, Loss: 0.00468940\n",
            "Iteration 3/10, Entropy: 269.3917, Loss: 0.00463079\n",
            "Iteration 4/10, Entropy: 269.3856, Loss: 0.00464275\n",
            "Iteration 4/10, Entropy: 269.3775, Loss: 0.00457451\n",
            "Iteration 4/10, Entropy: 269.3702, Loss: 0.00449490\n",
            "Iteration 4/10, Entropy: 269.3650, Loss: 0.00463129\n",
            "Iteration 4/10, Entropy: 269.3640, Loss: 0.00457114\n",
            "Iteration 5/10, Entropy: 269.3689, Loss: 0.00455508\n",
            "Iteration 5/10, Entropy: 269.3807, Loss: 0.00445476\n",
            "Iteration 5/10, Entropy: 269.3957, Loss: 0.00434488\n",
            "Iteration 5/10, Entropy: 269.4110, Loss: 0.00445743\n",
            "Iteration 5/10, Entropy: 269.4242, Loss: 0.00441599\n",
            "Iteration 6/10, Entropy: 269.4346, Loss: 0.00445699\n",
            "Iteration 6/10, Entropy: 269.4420, Loss: 0.00436856\n",
            "Iteration 6/10, Entropy: 269.4446, Loss: 0.00430500\n",
            "Iteration 6/10, Entropy: 269.4434, Loss: 0.00441436\n",
            "Iteration 6/10, Entropy: 269.4383, Loss: 0.00437661\n",
            "Iteration 7/10, Entropy: 269.4312, Loss: 0.00440669\n",
            "Iteration 7/10, Entropy: 269.4279, Loss: 0.00431397\n",
            "Iteration 7/10, Entropy: 269.4286, Loss: 0.00423908\n",
            "Iteration 7/10, Entropy: 269.4325, Loss: 0.00435008\n",
            "Iteration 7/10, Entropy: 269.4369, Loss: 0.00432272\n",
            "Iteration 8/10, Entropy: 269.4405, Loss: 0.00435302\n",
            "Iteration 8/10, Entropy: 269.4435, Loss: 0.00426921\n",
            "Iteration 8/10, Entropy: 269.4450, Loss: 0.00420854\n",
            "Iteration 8/10, Entropy: 269.4460, Loss: 0.00432236\n",
            "Iteration 8/10, Entropy: 269.4450, Loss: 0.00429092\n",
            "Iteration 9/10, Entropy: 269.4434, Loss: 0.00434196\n",
            "Iteration 9/10, Entropy: 269.4431, Loss: 0.00423689\n",
            "Iteration 9/10, Entropy: 269.4430, Loss: 0.00417364\n",
            "Iteration 9/10, Entropy: 269.4430, Loss: 0.00429381\n",
            "Iteration 9/10, Entropy: 269.4414, Loss: 0.00427514\n",
            "Iteration 10/10, Entropy: 269.4396, Loss: 0.00432489\n",
            "Iteration 10/10, Entropy: 269.4395, Loss: 0.00420475\n",
            "Iteration 10/10, Entropy: 269.4390, Loss: 0.00414857\n",
            "Iteration 10/10, Entropy: 269.4393, Loss: 0.00427383\n",
            "Iteration 10/10, Entropy: 269.4397, Loss: 0.00425479\n",
            "weight diff tensor(3.2319e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.2.self_attn.out_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 246.4933, Loss: 0.00001907\n",
            "Iteration 1/10, Entropy: 246.6777, Loss: 0.00001907\n",
            "Iteration 1/10, Entropy: 246.7484, Loss: 0.00001821\n",
            "Iteration 1/10, Entropy: 246.8387, Loss: 0.00001797\n",
            "Iteration 1/10, Entropy: 246.9046, Loss: 0.00001769\n",
            "Iteration 2/10, Entropy: 246.9664, Loss: 0.00001800\n",
            "Iteration 2/10, Entropy: 247.0145, Loss: 0.00001798\n",
            "Iteration 2/10, Entropy: 247.0479, Loss: 0.00001711\n",
            "Iteration 2/10, Entropy: 247.0798, Loss: 0.00001752\n",
            "Iteration 2/10, Entropy: 247.1182, Loss: 0.00001738\n",
            "Iteration 3/10, Entropy: 247.1461, Loss: 0.00001753\n",
            "Iteration 3/10, Entropy: 247.1689, Loss: 0.00001758\n",
            "Iteration 3/10, Entropy: 247.1805, Loss: 0.00001678\n",
            "Iteration 3/10, Entropy: 247.1989, Loss: 0.00001692\n",
            "Iteration 3/10, Entropy: 247.2287, Loss: 0.00001684\n",
            "Iteration 4/10, Entropy: 247.2645, Loss: 0.00001719\n",
            "Iteration 4/10, Entropy: 247.3075, Loss: 0.00001731\n",
            "Iteration 4/10, Entropy: 247.3434, Loss: 0.00001665\n",
            "Iteration 4/10, Entropy: 247.3663, Loss: 0.00001685\n",
            "Iteration 4/10, Entropy: 247.3794, Loss: 0.00001660\n",
            "Iteration 5/10, Entropy: 247.3957, Loss: 0.00001684\n",
            "Iteration 5/10, Entropy: 247.4252, Loss: 0.00001680\n",
            "Iteration 5/10, Entropy: 247.4543, Loss: 0.00001600\n",
            "Iteration 5/10, Entropy: 247.4781, Loss: 0.00001637\n",
            "Iteration 5/10, Entropy: 247.5047, Loss: 0.00001631\n",
            "Iteration 6/10, Entropy: 247.5247, Loss: 0.00001673\n",
            "Iteration 6/10, Entropy: 247.5510, Loss: 0.00001660\n",
            "Iteration 6/10, Entropy: 247.5609, Loss: 0.00001590\n",
            "Iteration 6/10, Entropy: 247.5700, Loss: 0.00001623\n",
            "Iteration 6/10, Entropy: 247.5905, Loss: 0.00001612\n",
            "Iteration 7/10, Entropy: 247.6220, Loss: 0.00001644\n",
            "Iteration 7/10, Entropy: 247.6637, Loss: 0.00001632\n",
            "Iteration 7/10, Entropy: 247.7010, Loss: 0.00001568\n",
            "Iteration 7/10, Entropy: 247.7428, Loss: 0.00001595\n",
            "Iteration 7/10, Entropy: 247.7985, Loss: 0.00001581\n",
            "Iteration 8/10, Entropy: 247.8595, Loss: 0.00001627\n",
            "Iteration 8/10, Entropy: 247.9223, Loss: 0.00001625\n",
            "Iteration 8/10, Entropy: 247.9591, Loss: 0.00001548\n",
            "Iteration 8/10, Entropy: 247.9731, Loss: 0.00001574\n",
            "Iteration 8/10, Entropy: 247.9839, Loss: 0.00001568\n",
            "Iteration 9/10, Entropy: 247.9985, Loss: 0.00001601\n",
            "Iteration 9/10, Entropy: 248.0070, Loss: 0.00001596\n",
            "Iteration 9/10, Entropy: 248.0335, Loss: 0.00001524\n",
            "Iteration 9/10, Entropy: 248.0742, Loss: 0.00001548\n",
            "Iteration 9/10, Entropy: 248.1188, Loss: 0.00001542\n",
            "Iteration 10/10, Entropy: 248.1555, Loss: 0.00001589\n",
            "Iteration 10/10, Entropy: 248.1860, Loss: 0.00001577\n",
            "Iteration 10/10, Entropy: 248.1971, Loss: 0.00001506\n",
            "Iteration 10/10, Entropy: 248.1976, Loss: 0.00001535\n",
            "Iteration 10/10, Entropy: 248.2032, Loss: 0.00001524\n",
            "weight diff tensor(1.0109e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.2.fc1 | Shape: torch.Size([3072, 768])\n",
            "Iteration 1/10, Entropy: 980.6491, Loss: 0.00163462\n",
            "Iteration 1/10, Entropy: 980.7601, Loss: 0.00156320\n",
            "Iteration 1/10, Entropy: 980.8280, Loss: 0.00150158\n",
            "Iteration 1/10, Entropy: 980.8978, Loss: 0.00149876\n",
            "Iteration 1/10, Entropy: 980.9996, Loss: 0.00146431\n",
            "Iteration 2/10, Entropy: 981.1180, Loss: 0.00146175\n",
            "Iteration 2/10, Entropy: 981.2443, Loss: 0.00144684\n",
            "Iteration 2/10, Entropy: 981.3741, Loss: 0.00141339\n",
            "Iteration 2/10, Entropy: 981.4907, Loss: 0.00143692\n",
            "Iteration 2/10, Entropy: 981.5988, Loss: 0.00141107\n",
            "Iteration 3/10, Entropy: 981.6916, Loss: 0.00141438\n",
            "Iteration 3/10, Entropy: 981.7795, Loss: 0.00140564\n",
            "Iteration 3/10, Entropy: 981.8719, Loss: 0.00137989\n",
            "Iteration 3/10, Entropy: 981.9502, Loss: 0.00140592\n",
            "Iteration 3/10, Entropy: 982.0228, Loss: 0.00138070\n",
            "Iteration 4/10, Entropy: 982.0899, Loss: 0.00138869\n",
            "Iteration 4/10, Entropy: 982.1537, Loss: 0.00137417\n",
            "Iteration 4/10, Entropy: 982.2085, Loss: 0.00135484\n",
            "Iteration 4/10, Entropy: 982.2442, Loss: 0.00138212\n",
            "Iteration 4/10, Entropy: 982.2697, Loss: 0.00136027\n",
            "Iteration 5/10, Entropy: 982.2800, Loss: 0.00137032\n",
            "Iteration 5/10, Entropy: 982.2971, Loss: 0.00135475\n",
            "Iteration 5/10, Entropy: 982.3208, Loss: 0.00133806\n",
            "Iteration 5/10, Entropy: 982.3468, Loss: 0.00136590\n",
            "Iteration 5/10, Entropy: 982.3829, Loss: 0.00134602\n",
            "Iteration 6/10, Entropy: 982.4230, Loss: 0.00136147\n",
            "Iteration 6/10, Entropy: 982.4655, Loss: 0.00134269\n",
            "Iteration 6/10, Entropy: 982.5055, Loss: 0.00132457\n",
            "Iteration 6/10, Entropy: 982.5311, Loss: 0.00135517\n",
            "Iteration 6/10, Entropy: 982.5500, Loss: 0.00133612\n",
            "Iteration 7/10, Entropy: 982.5618, Loss: 0.00134967\n",
            "Iteration 7/10, Entropy: 982.5708, Loss: 0.00133342\n",
            "Iteration 7/10, Entropy: 982.5727, Loss: 0.00131504\n",
            "Iteration 7/10, Entropy: 982.5611, Loss: 0.00134778\n",
            "Iteration 7/10, Entropy: 982.5525, Loss: 0.00132717\n",
            "Iteration 8/10, Entropy: 982.5539, Loss: 0.00133836\n",
            "Iteration 8/10, Entropy: 982.5685, Loss: 0.00132438\n",
            "Iteration 8/10, Entropy: 982.5964, Loss: 0.00130320\n",
            "Iteration 8/10, Entropy: 982.6259, Loss: 0.00133625\n",
            "Iteration 8/10, Entropy: 982.6527, Loss: 0.00131827\n",
            "Iteration 9/10, Entropy: 982.6763, Loss: 0.00133083\n",
            "Iteration 9/10, Entropy: 982.7008, Loss: 0.00131627\n",
            "Iteration 9/10, Entropy: 982.7285, Loss: 0.00129851\n",
            "Iteration 9/10, Entropy: 982.7471, Loss: 0.00133015\n",
            "Iteration 9/10, Entropy: 982.7577, Loss: 0.00131332\n",
            "Iteration 10/10, Entropy: 982.7636, Loss: 0.00132359\n",
            "Iteration 10/10, Entropy: 982.7767, Loss: 0.00131061\n",
            "Iteration 10/10, Entropy: 982.7969, Loss: 0.00129168\n",
            "Iteration 10/10, Entropy: 982.8120, Loss: 0.00132163\n",
            "Iteration 10/10, Entropy: 982.8297, Loss: 0.00130646\n",
            "weight diff tensor(3.5001e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.3.self_attn.k_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 274.4516, Loss: 0.00493690\n",
            "Iteration 1/10, Entropy: 274.4484, Loss: 0.00457855\n",
            "Iteration 1/10, Entropy: 274.4337, Loss: 0.00433899\n",
            "Iteration 1/10, Entropy: 274.4378, Loss: 0.00432657\n",
            "Iteration 1/10, Entropy: 274.4575, Loss: 0.00431134\n",
            "Iteration 2/10, Entropy: 274.4821, Loss: 0.00400430\n",
            "Iteration 2/10, Entropy: 274.4997, Loss: 0.00396594\n",
            "Iteration 2/10, Entropy: 274.5150, Loss: 0.00397082\n",
            "Iteration 2/10, Entropy: 274.5274, Loss: 0.00396198\n",
            "Iteration 2/10, Entropy: 274.5318, Loss: 0.00390042\n",
            "Iteration 3/10, Entropy: 274.5329, Loss: 0.00373524\n",
            "Iteration 3/10, Entropy: 274.5312, Loss: 0.00372141\n",
            "Iteration 3/10, Entropy: 274.5298, Loss: 0.00370465\n",
            "Iteration 3/10, Entropy: 274.5303, Loss: 0.00367285\n",
            "Iteration 3/10, Entropy: 274.5333, Loss: 0.00365546\n",
            "Iteration 4/10, Entropy: 274.5393, Loss: 0.00355569\n",
            "Iteration 4/10, Entropy: 274.5447, Loss: 0.00358708\n",
            "Iteration 4/10, Entropy: 274.5492, Loss: 0.00357120\n",
            "Iteration 4/10, Entropy: 274.5529, Loss: 0.00355589\n",
            "Iteration 4/10, Entropy: 274.5580, Loss: 0.00356739\n",
            "Iteration 5/10, Entropy: 274.5656, Loss: 0.00347868\n",
            "Iteration 5/10, Entropy: 274.5716, Loss: 0.00348097\n",
            "Iteration 5/10, Entropy: 274.5759, Loss: 0.00350299\n",
            "Iteration 5/10, Entropy: 274.5784, Loss: 0.00353170\n",
            "Iteration 5/10, Entropy: 274.5775, Loss: 0.00354076\n",
            "Iteration 6/10, Entropy: 274.5735, Loss: 0.00345803\n",
            "Iteration 6/10, Entropy: 274.5643, Loss: 0.00345318\n",
            "Iteration 6/10, Entropy: 274.5536, Loss: 0.00347056\n",
            "Iteration 6/10, Entropy: 274.5449, Loss: 0.00348993\n",
            "Iteration 6/10, Entropy: 274.5399, Loss: 0.00348378\n",
            "Iteration 7/10, Entropy: 274.5369, Loss: 0.00339537\n",
            "Iteration 7/10, Entropy: 274.5335, Loss: 0.00341182\n",
            "Iteration 7/10, Entropy: 274.5285, Loss: 0.00344399\n",
            "Iteration 7/10, Entropy: 274.5234, Loss: 0.00345903\n",
            "Iteration 7/10, Entropy: 274.5184, Loss: 0.00345289\n",
            "Iteration 8/10, Entropy: 274.5165, Loss: 0.00338066\n",
            "Iteration 8/10, Entropy: 274.5205, Loss: 0.00338686\n",
            "Iteration 8/10, Entropy: 274.5300, Loss: 0.00342257\n",
            "Iteration 8/10, Entropy: 274.5392, Loss: 0.00342149\n",
            "Iteration 8/10, Entropy: 274.5462, Loss: 0.00343106\n",
            "Iteration 9/10, Entropy: 274.5501, Loss: 0.00334454\n",
            "Iteration 9/10, Entropy: 274.5555, Loss: 0.00336398\n",
            "Iteration 9/10, Entropy: 274.5591, Loss: 0.00340639\n",
            "Iteration 9/10, Entropy: 274.5567, Loss: 0.00341258\n",
            "Iteration 9/10, Entropy: 274.5490, Loss: 0.00341605\n",
            "Iteration 10/10, Entropy: 274.5387, Loss: 0.00331672\n",
            "Iteration 10/10, Entropy: 274.5345, Loss: 0.00335149\n",
            "Iteration 10/10, Entropy: 274.5346, Loss: 0.00338628\n",
            "Iteration 10/10, Entropy: 274.5335, Loss: 0.00339777\n",
            "Iteration 10/10, Entropy: 274.5341, Loss: 0.00340499\n",
            "weight diff tensor(2.3993e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.3.self_attn.v_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 271.2823, Loss: 0.00050785\n",
            "Iteration 1/10, Entropy: 271.3185, Loss: 0.00047679\n",
            "Iteration 1/10, Entropy: 271.3990, Loss: 0.00043051\n",
            "Iteration 1/10, Entropy: 271.4241, Loss: 0.00045089\n",
            "Iteration 1/10, Entropy: 271.4510, Loss: 0.00042506\n",
            "Iteration 2/10, Entropy: 271.4293, Loss: 0.00039739\n",
            "Iteration 2/10, Entropy: 271.4130, Loss: 0.00038918\n",
            "Iteration 2/10, Entropy: 271.3932, Loss: 0.00038771\n",
            "Iteration 2/10, Entropy: 271.3709, Loss: 0.00039217\n",
            "Iteration 2/10, Entropy: 271.3612, Loss: 0.00038081\n",
            "Iteration 3/10, Entropy: 271.3639, Loss: 0.00038176\n",
            "Iteration 3/10, Entropy: 271.3705, Loss: 0.00038083\n",
            "Iteration 3/10, Entropy: 271.3941, Loss: 0.00037701\n",
            "Iteration 3/10, Entropy: 271.4165, Loss: 0.00037250\n",
            "Iteration 3/10, Entropy: 271.4388, Loss: 0.00036649\n",
            "Iteration 4/10, Entropy: 271.4508, Loss: 0.00036925\n",
            "Iteration 4/10, Entropy: 271.4480, Loss: 0.00037235\n",
            "Iteration 4/10, Entropy: 271.4378, Loss: 0.00036825\n",
            "Iteration 4/10, Entropy: 271.4275, Loss: 0.00036951\n",
            "Iteration 4/10, Entropy: 271.4203, Loss: 0.00036607\n",
            "Iteration 5/10, Entropy: 271.4148, Loss: 0.00036601\n",
            "Iteration 5/10, Entropy: 271.4035, Loss: 0.00036354\n",
            "Iteration 5/10, Entropy: 271.3958, Loss: 0.00036052\n",
            "Iteration 5/10, Entropy: 271.3958, Loss: 0.00036081\n",
            "Iteration 5/10, Entropy: 271.4044, Loss: 0.00036122\n",
            "Iteration 6/10, Entropy: 271.4301, Loss: 0.00035894\n",
            "Iteration 6/10, Entropy: 271.4504, Loss: 0.00036164\n",
            "Iteration 6/10, Entropy: 271.4630, Loss: 0.00035834\n",
            "Iteration 6/10, Entropy: 271.4732, Loss: 0.00035647\n",
            "Iteration 6/10, Entropy: 271.4749, Loss: 0.00035385\n",
            "Iteration 7/10, Entropy: 271.4800, Loss: 0.00035329\n",
            "Iteration 7/10, Entropy: 271.4873, Loss: 0.00035164\n",
            "Iteration 7/10, Entropy: 271.4937, Loss: 0.00034843\n",
            "Iteration 7/10, Entropy: 271.5003, Loss: 0.00035001\n",
            "Iteration 7/10, Entropy: 271.5114, Loss: 0.00035053\n",
            "Iteration 8/10, Entropy: 271.5259, Loss: 0.00034975\n",
            "Iteration 8/10, Entropy: 271.5390, Loss: 0.00034614\n",
            "Iteration 8/10, Entropy: 271.5476, Loss: 0.00034447\n",
            "Iteration 8/10, Entropy: 271.5587, Loss: 0.00033968\n",
            "Iteration 8/10, Entropy: 271.5678, Loss: 0.00033956\n",
            "Iteration 9/10, Entropy: 271.5792, Loss: 0.00034175\n",
            "Iteration 9/10, Entropy: 271.5931, Loss: 0.00033784\n",
            "Iteration 9/10, Entropy: 271.6029, Loss: 0.00033433\n",
            "Iteration 9/10, Entropy: 271.6131, Loss: 0.00033440\n",
            "Iteration 9/10, Entropy: 271.6193, Loss: 0.00033368\n",
            "Iteration 10/10, Entropy: 271.6271, Loss: 0.00033563\n",
            "Iteration 10/10, Entropy: 271.6357, Loss: 0.00033222\n",
            "Iteration 10/10, Entropy: 271.6415, Loss: 0.00032681\n",
            "Iteration 10/10, Entropy: 271.6402, Loss: 0.00032883\n",
            "Iteration 10/10, Entropy: 271.6378, Loss: 0.00033035\n",
            "weight diff tensor(3.5280e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.3.self_attn.q_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 274.1205, Loss: 0.00443748\n",
            "Iteration 1/10, Entropy: 274.0751, Loss: 0.00409386\n",
            "Iteration 1/10, Entropy: 274.0629, Loss: 0.00400386\n",
            "Iteration 1/10, Entropy: 274.0625, Loss: 0.00390798\n",
            "Iteration 1/10, Entropy: 274.0572, Loss: 0.00383009\n",
            "Iteration 2/10, Entropy: 274.0543, Loss: 0.00378128\n",
            "Iteration 2/10, Entropy: 274.0448, Loss: 0.00372502\n",
            "Iteration 2/10, Entropy: 274.0347, Loss: 0.00371026\n",
            "Iteration 2/10, Entropy: 274.0303, Loss: 0.00372327\n",
            "Iteration 2/10, Entropy: 274.0298, Loss: 0.00364812\n",
            "Iteration 3/10, Entropy: 274.0302, Loss: 0.00361857\n",
            "Iteration 3/10, Entropy: 274.0268, Loss: 0.00356299\n",
            "Iteration 3/10, Entropy: 274.0193, Loss: 0.00356318\n",
            "Iteration 3/10, Entropy: 274.0124, Loss: 0.00359469\n",
            "Iteration 3/10, Entropy: 274.0119, Loss: 0.00355950\n",
            "Iteration 4/10, Entropy: 274.0126, Loss: 0.00353412\n",
            "Iteration 4/10, Entropy: 274.0114, Loss: 0.00345437\n",
            "Iteration 4/10, Entropy: 274.0067, Loss: 0.00345483\n",
            "Iteration 4/10, Entropy: 274.0019, Loss: 0.00349607\n",
            "Iteration 4/10, Entropy: 273.9984, Loss: 0.00346322\n",
            "Iteration 5/10, Entropy: 273.9994, Loss: 0.00342849\n",
            "Iteration 5/10, Entropy: 274.0040, Loss: 0.00340915\n",
            "Iteration 5/10, Entropy: 274.0107, Loss: 0.00339729\n",
            "Iteration 5/10, Entropy: 274.0192, Loss: 0.00342392\n",
            "Iteration 5/10, Entropy: 274.0228, Loss: 0.00340866\n",
            "Iteration 6/10, Entropy: 274.0226, Loss: 0.00337833\n",
            "Iteration 6/10, Entropy: 274.0188, Loss: 0.00333307\n",
            "Iteration 6/10, Entropy: 274.0139, Loss: 0.00335543\n",
            "Iteration 6/10, Entropy: 274.0118, Loss: 0.00339274\n",
            "Iteration 6/10, Entropy: 274.0102, Loss: 0.00337473\n",
            "Iteration 7/10, Entropy: 274.0109, Loss: 0.00336671\n",
            "Iteration 7/10, Entropy: 274.0138, Loss: 0.00332061\n",
            "Iteration 7/10, Entropy: 274.0184, Loss: 0.00332551\n",
            "Iteration 7/10, Entropy: 274.0245, Loss: 0.00336652\n",
            "Iteration 7/10, Entropy: 274.0286, Loss: 0.00334363\n",
            "Iteration 8/10, Entropy: 274.0323, Loss: 0.00332186\n",
            "Iteration 8/10, Entropy: 274.0374, Loss: 0.00328877\n",
            "Iteration 8/10, Entropy: 274.0409, Loss: 0.00329386\n",
            "Iteration 8/10, Entropy: 274.0430, Loss: 0.00333827\n",
            "Iteration 8/10, Entropy: 274.0412, Loss: 0.00332822\n",
            "Iteration 9/10, Entropy: 274.0372, Loss: 0.00331229\n",
            "Iteration 9/10, Entropy: 274.0338, Loss: 0.00325308\n",
            "Iteration 9/10, Entropy: 274.0332, Loss: 0.00328048\n",
            "Iteration 9/10, Entropy: 274.0378, Loss: 0.00332451\n",
            "Iteration 9/10, Entropy: 274.0445, Loss: 0.00331092\n",
            "Iteration 10/10, Entropy: 274.0495, Loss: 0.00330443\n",
            "Iteration 10/10, Entropy: 274.0501, Loss: 0.00323370\n",
            "Iteration 10/10, Entropy: 274.0461, Loss: 0.00325751\n",
            "Iteration 10/10, Entropy: 274.0417, Loss: 0.00330113\n",
            "Iteration 10/10, Entropy: 274.0380, Loss: 0.00329084\n",
            "weight diff tensor(2.3604e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.3.self_attn.out_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 264.0016, Loss: 0.00001149\n",
            "Iteration 1/10, Entropy: 264.1465, Loss: 0.00001142\n",
            "Iteration 1/10, Entropy: 264.2321, Loss: 0.00001115\n",
            "Iteration 1/10, Entropy: 264.3014, Loss: 0.00001205\n",
            "Iteration 1/10, Entropy: 264.3866, Loss: 0.00001130\n",
            "Iteration 2/10, Entropy: 264.4657, Loss: 0.00001091\n",
            "Iteration 2/10, Entropy: 264.5193, Loss: 0.00001097\n",
            "Iteration 2/10, Entropy: 264.5727, Loss: 0.00001087\n",
            "Iteration 2/10, Entropy: 264.6067, Loss: 0.00001155\n",
            "Iteration 2/10, Entropy: 264.6577, Loss: 0.00001084\n",
            "Iteration 3/10, Entropy: 264.7096, Loss: 0.00001067\n",
            "Iteration 3/10, Entropy: 264.7632, Loss: 0.00001063\n",
            "Iteration 3/10, Entropy: 264.8210, Loss: 0.00001052\n",
            "Iteration 3/10, Entropy: 264.8782, Loss: 0.00001128\n",
            "Iteration 3/10, Entropy: 264.9313, Loss: 0.00001056\n",
            "Iteration 4/10, Entropy: 264.9691, Loss: 0.00001039\n",
            "Iteration 4/10, Entropy: 265.0209, Loss: 0.00001032\n",
            "Iteration 4/10, Entropy: 265.0550, Loss: 0.00001033\n",
            "Iteration 4/10, Entropy: 265.0920, Loss: 0.00001105\n",
            "Iteration 4/10, Entropy: 265.1303, Loss: 0.00001034\n",
            "Iteration 5/10, Entropy: 265.1679, Loss: 0.00001022\n",
            "Iteration 5/10, Entropy: 265.2001, Loss: 0.00001012\n",
            "Iteration 5/10, Entropy: 265.2367, Loss: 0.00001007\n",
            "Iteration 5/10, Entropy: 265.2772, Loss: 0.00001086\n",
            "Iteration 5/10, Entropy: 265.3243, Loss: 0.00001014\n",
            "Iteration 6/10, Entropy: 265.3703, Loss: 0.00001004\n",
            "Iteration 6/10, Entropy: 265.4056, Loss: 0.00001002\n",
            "Iteration 6/10, Entropy: 265.4329, Loss: 0.00000988\n",
            "Iteration 6/10, Entropy: 265.4565, Loss: 0.00001071\n",
            "Iteration 6/10, Entropy: 265.4760, Loss: 0.00001000\n",
            "Iteration 7/10, Entropy: 265.4857, Loss: 0.00000982\n",
            "Iteration 7/10, Entropy: 265.5059, Loss: 0.00000979\n",
            "Iteration 7/10, Entropy: 265.5346, Loss: 0.00000964\n",
            "Iteration 7/10, Entropy: 265.5539, Loss: 0.00001039\n",
            "Iteration 7/10, Entropy: 265.5684, Loss: 0.00000974\n",
            "Iteration 8/10, Entropy: 265.5890, Loss: 0.00000971\n",
            "Iteration 8/10, Entropy: 265.6176, Loss: 0.00000962\n",
            "Iteration 8/10, Entropy: 265.6541, Loss: 0.00000956\n",
            "Iteration 8/10, Entropy: 265.6802, Loss: 0.00001031\n",
            "Iteration 8/10, Entropy: 265.7123, Loss: 0.00000964\n",
            "Iteration 9/10, Entropy: 265.7371, Loss: 0.00000960\n",
            "Iteration 9/10, Entropy: 265.7633, Loss: 0.00000953\n",
            "Iteration 9/10, Entropy: 265.7823, Loss: 0.00000935\n",
            "Iteration 9/10, Entropy: 265.7954, Loss: 0.00001014\n",
            "Iteration 9/10, Entropy: 265.8167, Loss: 0.00000949\n",
            "Iteration 10/10, Entropy: 265.8369, Loss: 0.00000943\n",
            "Iteration 10/10, Entropy: 265.8565, Loss: 0.00000941\n",
            "Iteration 10/10, Entropy: 265.8766, Loss: 0.00000924\n",
            "Iteration 10/10, Entropy: 265.8882, Loss: 0.00001003\n",
            "Iteration 10/10, Entropy: 265.8994, Loss: 0.00000939\n",
            "weight diff tensor(7.0265e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.3.fc1 | Shape: torch.Size([3072, 768])\n",
            "Iteration 1/10, Entropy: 1003.9966, Loss: 0.00146034\n",
            "Iteration 1/10, Entropy: 1004.1749, Loss: 0.00132175\n",
            "Iteration 1/10, Entropy: 1004.4534, Loss: 0.00123194\n",
            "Iteration 1/10, Entropy: 1004.6464, Loss: 0.00120256\n",
            "Iteration 1/10, Entropy: 1004.7562, Loss: 0.00117634\n",
            "Iteration 2/10, Entropy: 1004.7832, Loss: 0.00116521\n",
            "Iteration 2/10, Entropy: 1004.8040, Loss: 0.00113880\n",
            "Iteration 2/10, Entropy: 1004.8391, Loss: 0.00113973\n",
            "Iteration 2/10, Entropy: 1004.8568, Loss: 0.00115252\n",
            "Iteration 2/10, Entropy: 1004.8788, Loss: 0.00114911\n",
            "Iteration 3/10, Entropy: 1004.9026, Loss: 0.00113720\n",
            "Iteration 3/10, Entropy: 1004.9288, Loss: 0.00112237\n",
            "Iteration 3/10, Entropy: 1004.9346, Loss: 0.00111262\n",
            "Iteration 3/10, Entropy: 1004.9431, Loss: 0.00111308\n",
            "Iteration 3/10, Entropy: 1004.9780, Loss: 0.00111228\n",
            "Iteration 4/10, Entropy: 1005.0284, Loss: 0.00111029\n",
            "Iteration 4/10, Entropy: 1005.0682, Loss: 0.00109228\n",
            "Iteration 4/10, Entropy: 1005.0946, Loss: 0.00108831\n",
            "Iteration 4/10, Entropy: 1005.1209, Loss: 0.00109142\n",
            "Iteration 4/10, Entropy: 1005.1395, Loss: 0.00109230\n",
            "Iteration 5/10, Entropy: 1005.1703, Loss: 0.00110081\n",
            "Iteration 5/10, Entropy: 1005.2044, Loss: 0.00107616\n",
            "Iteration 5/10, Entropy: 1005.2368, Loss: 0.00107645\n",
            "Iteration 5/10, Entropy: 1005.2659, Loss: 0.00108099\n",
            "Iteration 5/10, Entropy: 1005.3101, Loss: 0.00108641\n",
            "Iteration 6/10, Entropy: 1005.3565, Loss: 0.00109421\n",
            "Iteration 6/10, Entropy: 1005.4114, Loss: 0.00106975\n",
            "Iteration 6/10, Entropy: 1005.4512, Loss: 0.00107108\n",
            "Iteration 6/10, Entropy: 1005.4705, Loss: 0.00107466\n",
            "Iteration 6/10, Entropy: 1005.4861, Loss: 0.00107936\n",
            "Iteration 7/10, Entropy: 1005.5048, Loss: 0.00108600\n",
            "Iteration 7/10, Entropy: 1005.5397, Loss: 0.00106254\n",
            "Iteration 7/10, Entropy: 1005.5576, Loss: 0.00106342\n",
            "Iteration 7/10, Entropy: 1005.5490, Loss: 0.00106632\n",
            "Iteration 7/10, Entropy: 1005.5286, Loss: 0.00107101\n",
            "Iteration 8/10, Entropy: 1005.5137, Loss: 0.00108215\n",
            "Iteration 8/10, Entropy: 1005.5302, Loss: 0.00105668\n",
            "Iteration 8/10, Entropy: 1005.5509, Loss: 0.00105840\n",
            "Iteration 8/10, Entropy: 1005.5690, Loss: 0.00106236\n",
            "Iteration 8/10, Entropy: 1005.5913, Loss: 0.00106716\n",
            "Iteration 9/10, Entropy: 1005.6246, Loss: 0.00107425\n",
            "Iteration 9/10, Entropy: 1005.6682, Loss: 0.00105278\n",
            "Iteration 9/10, Entropy: 1005.7054, Loss: 0.00105328\n",
            "Iteration 9/10, Entropy: 1005.7195, Loss: 0.00105761\n",
            "Iteration 9/10, Entropy: 1005.7166, Loss: 0.00106133\n",
            "Iteration 10/10, Entropy: 1005.7126, Loss: 0.00107010\n",
            "Iteration 10/10, Entropy: 1005.7274, Loss: 0.00104891\n",
            "Iteration 10/10, Entropy: 1005.7512, Loss: 0.00104869\n",
            "Iteration 10/10, Entropy: 1005.7789, Loss: 0.00105362\n",
            "Iteration 10/10, Entropy: 1005.7996, Loss: 0.00105745\n",
            "weight diff tensor(2.9291e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.4.self_attn.k_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 273.0952, Loss: 0.00642652\n",
            "Iteration 1/10, Entropy: 273.0636, Loss: 0.00583000\n",
            "Iteration 1/10, Entropy: 273.0437, Loss: 0.00571933\n",
            "Iteration 1/10, Entropy: 273.0245, Loss: 0.00559637\n",
            "Iteration 1/10, Entropy: 273.0137, Loss: 0.00552462\n",
            "Iteration 2/10, Entropy: 273.0100, Loss: 0.00528362\n",
            "Iteration 2/10, Entropy: 273.0096, Loss: 0.00519732\n",
            "Iteration 2/10, Entropy: 273.0121, Loss: 0.00528259\n",
            "Iteration 2/10, Entropy: 273.0132, Loss: 0.00519967\n",
            "Iteration 2/10, Entropy: 273.0175, Loss: 0.00521772\n",
            "Iteration 3/10, Entropy: 273.0217, Loss: 0.00499336\n",
            "Iteration 3/10, Entropy: 273.0244, Loss: 0.00493268\n",
            "Iteration 3/10, Entropy: 273.0264, Loss: 0.00498879\n",
            "Iteration 3/10, Entropy: 273.0290, Loss: 0.00493041\n",
            "Iteration 3/10, Entropy: 273.0338, Loss: 0.00497727\n",
            "Iteration 4/10, Entropy: 273.0409, Loss: 0.00481029\n",
            "Iteration 4/10, Entropy: 273.0477, Loss: 0.00481824\n",
            "Iteration 4/10, Entropy: 273.0550, Loss: 0.00484019\n",
            "Iteration 4/10, Entropy: 273.0617, Loss: 0.00480924\n",
            "Iteration 4/10, Entropy: 273.0677, Loss: 0.00483785\n",
            "Iteration 5/10, Entropy: 273.0730, Loss: 0.00477467\n",
            "Iteration 5/10, Entropy: 273.0788, Loss: 0.00472448\n",
            "Iteration 5/10, Entropy: 273.0849, Loss: 0.00474217\n",
            "Iteration 5/10, Entropy: 273.0895, Loss: 0.00472766\n",
            "Iteration 5/10, Entropy: 273.0926, Loss: 0.00477751\n",
            "Iteration 6/10, Entropy: 273.0949, Loss: 0.00469540\n",
            "Iteration 6/10, Entropy: 273.0974, Loss: 0.00467409\n",
            "Iteration 6/10, Entropy: 273.1006, Loss: 0.00469208\n",
            "Iteration 6/10, Entropy: 273.1041, Loss: 0.00469836\n",
            "Iteration 6/10, Entropy: 273.1094, Loss: 0.00472376\n",
            "Iteration 7/10, Entropy: 273.1160, Loss: 0.00466883\n",
            "Iteration 7/10, Entropy: 273.1234, Loss: 0.00461986\n",
            "Iteration 7/10, Entropy: 273.1299, Loss: 0.00465282\n",
            "Iteration 7/10, Entropy: 273.1324, Loss: 0.00465620\n",
            "Iteration 7/10, Entropy: 273.1324, Loss: 0.00469103\n",
            "Iteration 8/10, Entropy: 273.1310, Loss: 0.00460711\n",
            "Iteration 8/10, Entropy: 273.1285, Loss: 0.00457741\n",
            "Iteration 8/10, Entropy: 273.1264, Loss: 0.00462008\n",
            "Iteration 8/10, Entropy: 273.1242, Loss: 0.00462215\n",
            "Iteration 8/10, Entropy: 273.1238, Loss: 0.00465418\n",
            "Iteration 9/10, Entropy: 273.1249, Loss: 0.00458611\n",
            "Iteration 9/10, Entropy: 273.1277, Loss: 0.00456948\n",
            "Iteration 9/10, Entropy: 273.1313, Loss: 0.00459458\n",
            "Iteration 9/10, Entropy: 273.1342, Loss: 0.00459911\n",
            "Iteration 9/10, Entropy: 273.1370, Loss: 0.00463479\n",
            "Iteration 10/10, Entropy: 273.1400, Loss: 0.00457698\n",
            "Iteration 10/10, Entropy: 273.1433, Loss: 0.00452960\n",
            "Iteration 10/10, Entropy: 273.1462, Loss: 0.00457457\n",
            "Iteration 10/10, Entropy: 273.1473, Loss: 0.00458796\n",
            "Iteration 10/10, Entropy: 273.1479, Loss: 0.00460408\n",
            "weight diff tensor(2.6746e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.4.self_attn.v_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 274.1133, Loss: 0.00046373\n",
            "Iteration 1/10, Entropy: 274.1975, Loss: 0.00042860\n",
            "Iteration 1/10, Entropy: 274.2840, Loss: 0.00039979\n",
            "Iteration 1/10, Entropy: 274.2897, Loss: 0.00038420\n",
            "Iteration 1/10, Entropy: 274.2895, Loss: 0.00036804\n",
            "Iteration 2/10, Entropy: 274.3161, Loss: 0.00037567\n",
            "Iteration 2/10, Entropy: 274.3257, Loss: 0.00035365\n",
            "Iteration 2/10, Entropy: 274.3645, Loss: 0.00035293\n",
            "Iteration 2/10, Entropy: 274.4086, Loss: 0.00035394\n",
            "Iteration 2/10, Entropy: 274.4502, Loss: 0.00035167\n",
            "Iteration 3/10, Entropy: 274.4915, Loss: 0.00036177\n",
            "Iteration 3/10, Entropy: 274.5202, Loss: 0.00035121\n",
            "Iteration 3/10, Entropy: 274.5365, Loss: 0.00034697\n",
            "Iteration 3/10, Entropy: 274.5535, Loss: 0.00034829\n",
            "Iteration 3/10, Entropy: 274.5741, Loss: 0.00033607\n",
            "Iteration 4/10, Entropy: 274.5955, Loss: 0.00033842\n",
            "Iteration 4/10, Entropy: 274.6147, Loss: 0.00032763\n",
            "Iteration 4/10, Entropy: 274.6244, Loss: 0.00032357\n",
            "Iteration 4/10, Entropy: 274.6378, Loss: 0.00032611\n",
            "Iteration 4/10, Entropy: 274.6534, Loss: 0.00032637\n",
            "Iteration 5/10, Entropy: 274.6723, Loss: 0.00032932\n",
            "Iteration 5/10, Entropy: 274.6842, Loss: 0.00032190\n",
            "Iteration 5/10, Entropy: 274.6941, Loss: 0.00031486\n",
            "Iteration 5/10, Entropy: 274.7023, Loss: 0.00031683\n",
            "Iteration 5/10, Entropy: 274.7150, Loss: 0.00031753\n",
            "Iteration 6/10, Entropy: 274.7328, Loss: 0.00032525\n",
            "Iteration 6/10, Entropy: 274.7490, Loss: 0.00031678\n",
            "Iteration 6/10, Entropy: 274.7631, Loss: 0.00031277\n",
            "Iteration 6/10, Entropy: 274.7771, Loss: 0.00031891\n",
            "Iteration 6/10, Entropy: 274.7994, Loss: 0.00031974\n",
            "Iteration 7/10, Entropy: 274.8212, Loss: 0.00032549\n",
            "Iteration 7/10, Entropy: 274.8346, Loss: 0.00031123\n",
            "Iteration 7/10, Entropy: 274.8431, Loss: 0.00030572\n",
            "Iteration 7/10, Entropy: 274.8518, Loss: 0.00031041\n",
            "Iteration 7/10, Entropy: 274.8587, Loss: 0.00030773\n",
            "Iteration 8/10, Entropy: 274.8658, Loss: 0.00031614\n",
            "Iteration 8/10, Entropy: 274.8855, Loss: 0.00030326\n",
            "Iteration 8/10, Entropy: 274.9081, Loss: 0.00029983\n",
            "Iteration 8/10, Entropy: 274.9210, Loss: 0.00030398\n",
            "Iteration 8/10, Entropy: 274.9299, Loss: 0.00030560\n",
            "Iteration 9/10, Entropy: 274.9340, Loss: 0.00031743\n",
            "Iteration 9/10, Entropy: 274.9476, Loss: 0.00029998\n",
            "Iteration 9/10, Entropy: 274.9559, Loss: 0.00029836\n",
            "Iteration 9/10, Entropy: 274.9591, Loss: 0.00030207\n",
            "Iteration 9/10, Entropy: 274.9675, Loss: 0.00030004\n",
            "Iteration 10/10, Entropy: 274.9728, Loss: 0.00031461\n",
            "Iteration 10/10, Entropy: 274.9831, Loss: 0.00029952\n",
            "Iteration 10/10, Entropy: 274.9949, Loss: 0.00029304\n",
            "Iteration 10/10, Entropy: 275.0017, Loss: 0.00029954\n",
            "Iteration 10/10, Entropy: 275.0028, Loss: 0.00029652\n",
            "weight diff tensor(2.7775e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.4.self_attn.q_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 273.9922, Loss: 0.00594904\n",
            "Iteration 1/10, Entropy: 274.0250, Loss: 0.00538628\n",
            "Iteration 1/10, Entropy: 274.0312, Loss: 0.00517361\n",
            "Iteration 1/10, Entropy: 274.0125, Loss: 0.00515919\n",
            "Iteration 1/10, Entropy: 273.9929, Loss: 0.00503321\n",
            "Iteration 2/10, Entropy: 273.9756, Loss: 0.00477385\n",
            "Iteration 2/10, Entropy: 273.9663, Loss: 0.00475735\n",
            "Iteration 2/10, Entropy: 273.9643, Loss: 0.00484010\n",
            "Iteration 2/10, Entropy: 273.9677, Loss: 0.00477718\n",
            "Iteration 2/10, Entropy: 273.9744, Loss: 0.00472031\n",
            "Iteration 3/10, Entropy: 273.9809, Loss: 0.00464246\n",
            "Iteration 3/10, Entropy: 273.9855, Loss: 0.00462470\n",
            "Iteration 3/10, Entropy: 273.9909, Loss: 0.00465882\n",
            "Iteration 3/10, Entropy: 273.9956, Loss: 0.00461360\n",
            "Iteration 3/10, Entropy: 273.9983, Loss: 0.00458161\n",
            "Iteration 4/10, Entropy: 274.0010, Loss: 0.00457867\n",
            "Iteration 4/10, Entropy: 274.0049, Loss: 0.00448817\n",
            "Iteration 4/10, Entropy: 274.0135, Loss: 0.00451530\n",
            "Iteration 4/10, Entropy: 274.0238, Loss: 0.00453078\n",
            "Iteration 4/10, Entropy: 274.0349, Loss: 0.00445773\n",
            "Iteration 5/10, Entropy: 274.0472, Loss: 0.00444674\n",
            "Iteration 5/10, Entropy: 274.0587, Loss: 0.00442441\n",
            "Iteration 5/10, Entropy: 274.0676, Loss: 0.00445757\n",
            "Iteration 5/10, Entropy: 274.0739, Loss: 0.00445003\n",
            "Iteration 5/10, Entropy: 274.0760, Loss: 0.00441220\n",
            "Iteration 6/10, Entropy: 274.0779, Loss: 0.00441969\n",
            "Iteration 6/10, Entropy: 274.0798, Loss: 0.00436277\n",
            "Iteration 6/10, Entropy: 274.0821, Loss: 0.00439009\n",
            "Iteration 6/10, Entropy: 274.0837, Loss: 0.00439016\n",
            "Iteration 6/10, Entropy: 274.0833, Loss: 0.00437051\n",
            "Iteration 7/10, Entropy: 274.0834, Loss: 0.00437817\n",
            "Iteration 7/10, Entropy: 274.0845, Loss: 0.00429366\n",
            "Iteration 7/10, Entropy: 274.0856, Loss: 0.00435514\n",
            "Iteration 7/10, Entropy: 274.0860, Loss: 0.00436911\n",
            "Iteration 7/10, Entropy: 274.0840, Loss: 0.00433942\n",
            "Iteration 8/10, Entropy: 274.0831, Loss: 0.00433684\n",
            "Iteration 8/10, Entropy: 274.0840, Loss: 0.00426901\n",
            "Iteration 8/10, Entropy: 274.0864, Loss: 0.00431219\n",
            "Iteration 8/10, Entropy: 274.0884, Loss: 0.00432002\n",
            "Iteration 8/10, Entropy: 274.0885, Loss: 0.00428878\n",
            "Iteration 9/10, Entropy: 274.0896, Loss: 0.00431325\n",
            "Iteration 9/10, Entropy: 274.0907, Loss: 0.00425922\n",
            "Iteration 9/10, Entropy: 274.0939, Loss: 0.00427571\n",
            "Iteration 9/10, Entropy: 274.0967, Loss: 0.00429228\n",
            "Iteration 9/10, Entropy: 274.0976, Loss: 0.00426688\n",
            "Iteration 10/10, Entropy: 274.0998, Loss: 0.00429653\n",
            "Iteration 10/10, Entropy: 274.1028, Loss: 0.00424458\n",
            "Iteration 10/10, Entropy: 274.1068, Loss: 0.00427178\n",
            "Iteration 10/10, Entropy: 274.1115, Loss: 0.00426768\n",
            "Iteration 10/10, Entropy: 274.1148, Loss: 0.00424907\n",
            "weight diff tensor(2.6288e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.4.self_attn.out_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 269.9526, Loss: 0.00000642\n",
            "Iteration 1/10, Entropy: 269.9856, Loss: 0.00000585\n",
            "Iteration 1/10, Entropy: 269.9851, Loss: 0.00000529\n",
            "Iteration 1/10, Entropy: 270.0013, Loss: 0.00000510\n",
            "Iteration 1/10, Entropy: 270.0554, Loss: 0.00000542\n",
            "Iteration 2/10, Entropy: 270.0955, Loss: 0.00000607\n",
            "Iteration 2/10, Entropy: 270.1497, Loss: 0.00000554\n",
            "Iteration 2/10, Entropy: 270.2153, Loss: 0.00000519\n",
            "Iteration 2/10, Entropy: 270.2762, Loss: 0.00000497\n",
            "Iteration 2/10, Entropy: 270.3296, Loss: 0.00000527\n",
            "Iteration 3/10, Entropy: 270.3886, Loss: 0.00000601\n",
            "Iteration 3/10, Entropy: 270.4401, Loss: 0.00000542\n",
            "Iteration 3/10, Entropy: 270.5012, Loss: 0.00000505\n",
            "Iteration 3/10, Entropy: 270.5507, Loss: 0.00000486\n",
            "Iteration 3/10, Entropy: 270.5958, Loss: 0.00000517\n",
            "Iteration 4/10, Entropy: 270.6570, Loss: 0.00000584\n",
            "Iteration 4/10, Entropy: 270.7223, Loss: 0.00000522\n",
            "Iteration 4/10, Entropy: 270.7908, Loss: 0.00000487\n",
            "Iteration 4/10, Entropy: 270.8614, Loss: 0.00000471\n",
            "Iteration 4/10, Entropy: 270.9261, Loss: 0.00000507\n",
            "Iteration 5/10, Entropy: 270.9779, Loss: 0.00000569\n",
            "Iteration 5/10, Entropy: 271.0183, Loss: 0.00000513\n",
            "Iteration 5/10, Entropy: 271.0302, Loss: 0.00000479\n",
            "Iteration 5/10, Entropy: 271.0315, Loss: 0.00000463\n",
            "Iteration 5/10, Entropy: 271.0384, Loss: 0.00000489\n",
            "Iteration 6/10, Entropy: 271.0574, Loss: 0.00000560\n",
            "Iteration 6/10, Entropy: 271.0781, Loss: 0.00000502\n",
            "Iteration 6/10, Entropy: 271.0993, Loss: 0.00000469\n",
            "Iteration 6/10, Entropy: 271.1134, Loss: 0.00000456\n",
            "Iteration 6/10, Entropy: 271.1313, Loss: 0.00000483\n",
            "Iteration 7/10, Entropy: 271.1566, Loss: 0.00000547\n",
            "Iteration 7/10, Entropy: 271.1894, Loss: 0.00000495\n",
            "Iteration 7/10, Entropy: 271.2138, Loss: 0.00000466\n",
            "Iteration 7/10, Entropy: 271.2361, Loss: 0.00000448\n",
            "Iteration 7/10, Entropy: 271.2476, Loss: 0.00000474\n",
            "Iteration 8/10, Entropy: 271.2599, Loss: 0.00000543\n",
            "Iteration 8/10, Entropy: 271.2762, Loss: 0.00000486\n",
            "Iteration 8/10, Entropy: 271.2919, Loss: 0.00000458\n",
            "Iteration 8/10, Entropy: 271.3021, Loss: 0.00000443\n",
            "Iteration 8/10, Entropy: 271.3079, Loss: 0.00000467\n",
            "Iteration 9/10, Entropy: 271.3218, Loss: 0.00000541\n",
            "Iteration 9/10, Entropy: 271.3517, Loss: 0.00000483\n",
            "Iteration 9/10, Entropy: 271.3843, Loss: 0.00000455\n",
            "Iteration 9/10, Entropy: 271.4040, Loss: 0.00000442\n",
            "Iteration 9/10, Entropy: 271.4270, Loss: 0.00000469\n",
            "Iteration 10/10, Entropy: 271.4493, Loss: 0.00000535\n",
            "Iteration 10/10, Entropy: 271.4660, Loss: 0.00000482\n",
            "Iteration 10/10, Entropy: 271.4783, Loss: 0.00000452\n",
            "Iteration 10/10, Entropy: 271.4731, Loss: 0.00000440\n",
            "Iteration 10/10, Entropy: 271.4714, Loss: 0.00000467\n",
            "weight diff tensor(2.7636e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.4.fc1 | Shape: torch.Size([3072, 768])\n",
            "Iteration 1/10, Entropy: 1058.8856, Loss: 0.00107720\n",
            "Iteration 1/10, Entropy: 1058.9110, Loss: 0.00108570\n",
            "Iteration 1/10, Entropy: 1058.9270, Loss: 0.00103823\n",
            "Iteration 1/10, Entropy: 1058.9551, Loss: 0.00101646\n",
            "Iteration 1/10, Entropy: 1058.9781, Loss: 0.00098963\n",
            "Iteration 2/10, Entropy: 1058.9811, Loss: 0.00098018\n",
            "Iteration 2/10, Entropy: 1058.9924, Loss: 0.00098279\n",
            "Iteration 2/10, Entropy: 1059.0431, Loss: 0.00096108\n",
            "Iteration 2/10, Entropy: 1059.0787, Loss: 0.00095897\n",
            "Iteration 2/10, Entropy: 1059.1099, Loss: 0.00093058\n",
            "Iteration 3/10, Entropy: 1059.1422, Loss: 0.00094007\n",
            "Iteration 3/10, Entropy: 1059.1672, Loss: 0.00093624\n",
            "Iteration 3/10, Entropy: 1059.1801, Loss: 0.00092579\n",
            "Iteration 3/10, Entropy: 1059.1765, Loss: 0.00092661\n",
            "Iteration 3/10, Entropy: 1059.1735, Loss: 0.00090324\n",
            "Iteration 4/10, Entropy: 1059.1859, Loss: 0.00092276\n",
            "Iteration 4/10, Entropy: 1059.2120, Loss: 0.00091573\n",
            "Iteration 4/10, Entropy: 1059.2491, Loss: 0.00090866\n",
            "Iteration 4/10, Entropy: 1059.2896, Loss: 0.00090759\n",
            "Iteration 4/10, Entropy: 1059.3214, Loss: 0.00089547\n",
            "Iteration 5/10, Entropy: 1059.3466, Loss: 0.00091461\n",
            "Iteration 5/10, Entropy: 1059.3713, Loss: 0.00090472\n",
            "Iteration 5/10, Entropy: 1059.3954, Loss: 0.00089370\n",
            "Iteration 5/10, Entropy: 1059.4135, Loss: 0.00089277\n",
            "Iteration 5/10, Entropy: 1059.4227, Loss: 0.00088147\n",
            "Iteration 6/10, Entropy: 1059.4302, Loss: 0.00090531\n",
            "Iteration 6/10, Entropy: 1059.4474, Loss: 0.00089474\n",
            "Iteration 6/10, Entropy: 1059.4650, Loss: 0.00088605\n",
            "Iteration 6/10, Entropy: 1059.4836, Loss: 0.00088880\n",
            "Iteration 6/10, Entropy: 1059.5051, Loss: 0.00086763\n",
            "Iteration 7/10, Entropy: 1059.5281, Loss: 0.00089418\n",
            "Iteration 7/10, Entropy: 1059.5519, Loss: 0.00088752\n",
            "Iteration 7/10, Entropy: 1059.5795, Loss: 0.00087823\n",
            "Iteration 7/10, Entropy: 1059.6079, Loss: 0.00087966\n",
            "Iteration 7/10, Entropy: 1059.6284, Loss: 0.00086178\n",
            "Iteration 8/10, Entropy: 1059.6482, Loss: 0.00088894\n",
            "Iteration 8/10, Entropy: 1059.6794, Loss: 0.00088382\n",
            "Iteration 8/10, Entropy: 1059.7222, Loss: 0.00087170\n",
            "Iteration 8/10, Entropy: 1059.7593, Loss: 0.00087245\n",
            "Iteration 8/10, Entropy: 1059.7864, Loss: 0.00085813\n",
            "Iteration 9/10, Entropy: 1059.8075, Loss: 0.00088623\n",
            "Iteration 9/10, Entropy: 1059.8256, Loss: 0.00087690\n",
            "Iteration 9/10, Entropy: 1059.8342, Loss: 0.00086452\n",
            "Iteration 9/10, Entropy: 1059.8311, Loss: 0.00086870\n",
            "Iteration 9/10, Entropy: 1059.8276, Loss: 0.00085213\n",
            "Iteration 10/10, Entropy: 1059.8309, Loss: 0.00088112\n",
            "Iteration 10/10, Entropy: 1059.8439, Loss: 0.00087317\n",
            "Iteration 10/10, Entropy: 1059.8621, Loss: 0.00086176\n",
            "Iteration 10/10, Entropy: 1059.8804, Loss: 0.00086583\n",
            "Iteration 10/10, Entropy: 1059.8947, Loss: 0.00084755\n",
            "weight diff tensor(1.8932e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.5.self_attn.k_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 270.7036, Loss: 0.00798624\n",
            "Iteration 1/10, Entropy: 270.7018, Loss: 0.00776265\n",
            "Iteration 1/10, Entropy: 270.7126, Loss: 0.00753831\n",
            "Iteration 1/10, Entropy: 270.7297, Loss: 0.00721219\n",
            "Iteration 1/10, Entropy: 270.7369, Loss: 0.00705603\n",
            "Iteration 2/10, Entropy: 270.7347, Loss: 0.00671241\n",
            "Iteration 2/10, Entropy: 270.7264, Loss: 0.00662605\n",
            "Iteration 2/10, Entropy: 270.7182, Loss: 0.00673505\n",
            "Iteration 2/10, Entropy: 270.7168, Loss: 0.00668006\n",
            "Iteration 2/10, Entropy: 270.7179, Loss: 0.00661582\n",
            "Iteration 3/10, Entropy: 270.7210, Loss: 0.00638805\n",
            "Iteration 3/10, Entropy: 270.7233, Loss: 0.00631473\n",
            "Iteration 3/10, Entropy: 270.7256, Loss: 0.00635183\n",
            "Iteration 3/10, Entropy: 270.7277, Loss: 0.00636363\n",
            "Iteration 3/10, Entropy: 270.7275, Loss: 0.00629924\n",
            "Iteration 4/10, Entropy: 270.7259, Loss: 0.00612995\n",
            "Iteration 4/10, Entropy: 270.7243, Loss: 0.00618360\n",
            "Iteration 4/10, Entropy: 270.7255, Loss: 0.00626630\n",
            "Iteration 4/10, Entropy: 270.7303, Loss: 0.00622072\n",
            "Iteration 4/10, Entropy: 270.7351, Loss: 0.00612700\n",
            "Iteration 5/10, Entropy: 270.7379, Loss: 0.00600006\n",
            "Iteration 5/10, Entropy: 270.7379, Loss: 0.00605030\n",
            "Iteration 5/10, Entropy: 270.7395, Loss: 0.00612676\n",
            "Iteration 5/10, Entropy: 270.7443, Loss: 0.00615133\n",
            "Iteration 5/10, Entropy: 270.7499, Loss: 0.00609337\n",
            "Iteration 6/10, Entropy: 270.7533, Loss: 0.00601888\n",
            "Iteration 6/10, Entropy: 270.7519, Loss: 0.00603813\n",
            "Iteration 6/10, Entropy: 270.7484, Loss: 0.00609451\n",
            "Iteration 6/10, Entropy: 270.7459, Loss: 0.00608477\n",
            "Iteration 6/10, Entropy: 270.7456, Loss: 0.00603348\n",
            "Iteration 7/10, Entropy: 270.7470, Loss: 0.00596026\n",
            "Iteration 7/10, Entropy: 270.7483, Loss: 0.00597526\n",
            "Iteration 7/10, Entropy: 270.7509, Loss: 0.00604160\n",
            "Iteration 7/10, Entropy: 270.7549, Loss: 0.00605648\n",
            "Iteration 7/10, Entropy: 270.7589, Loss: 0.00599715\n",
            "Iteration 8/10, Entropy: 270.7612, Loss: 0.00594133\n",
            "Iteration 8/10, Entropy: 270.7611, Loss: 0.00593535\n",
            "Iteration 8/10, Entropy: 270.7612, Loss: 0.00599960\n",
            "Iteration 8/10, Entropy: 270.7631, Loss: 0.00601728\n",
            "Iteration 8/10, Entropy: 270.7657, Loss: 0.00595711\n",
            "Iteration 9/10, Entropy: 270.7675, Loss: 0.00587740\n",
            "Iteration 9/10, Entropy: 270.7675, Loss: 0.00592188\n",
            "Iteration 9/10, Entropy: 270.7679, Loss: 0.00597739\n",
            "Iteration 9/10, Entropy: 270.7704, Loss: 0.00600102\n",
            "Iteration 9/10, Entropy: 270.7738, Loss: 0.00594920\n",
            "Iteration 10/10, Entropy: 270.7766, Loss: 0.00587696\n",
            "Iteration 10/10, Entropy: 270.7772, Loss: 0.00589949\n",
            "Iteration 10/10, Entropy: 270.7773, Loss: 0.00596931\n",
            "Iteration 10/10, Entropy: 270.7790, Loss: 0.00599674\n",
            "Iteration 10/10, Entropy: 270.7826, Loss: 0.00592600\n",
            "weight diff tensor(3.2389e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.5.self_attn.v_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 276.9258, Loss: 0.00024031\n",
            "Iteration 1/10, Entropy: 277.0146, Loss: 0.00024189\n",
            "Iteration 1/10, Entropy: 277.0386, Loss: 0.00023266\n",
            "Iteration 1/10, Entropy: 277.0806, Loss: 0.00023656\n",
            "Iteration 1/10, Entropy: 277.1103, Loss: 0.00022934\n",
            "Iteration 2/10, Entropy: 277.1397, Loss: 0.00022018\n",
            "Iteration 2/10, Entropy: 277.1561, Loss: 0.00022621\n",
            "Iteration 2/10, Entropy: 277.1599, Loss: 0.00022436\n",
            "Iteration 2/10, Entropy: 277.1707, Loss: 0.00022736\n",
            "Iteration 2/10, Entropy: 277.1671, Loss: 0.00022521\n",
            "Iteration 3/10, Entropy: 277.1719, Loss: 0.00022565\n",
            "Iteration 3/10, Entropy: 277.1823, Loss: 0.00021627\n",
            "Iteration 3/10, Entropy: 277.1951, Loss: 0.00021449\n",
            "Iteration 3/10, Entropy: 277.2043, Loss: 0.00021891\n",
            "Iteration 3/10, Entropy: 277.2234, Loss: 0.00021627\n",
            "Iteration 4/10, Entropy: 277.2444, Loss: 0.00021875\n",
            "Iteration 4/10, Entropy: 277.2654, Loss: 0.00021040\n",
            "Iteration 4/10, Entropy: 277.2826, Loss: 0.00020797\n",
            "Iteration 4/10, Entropy: 277.2932, Loss: 0.00021049\n",
            "Iteration 4/10, Entropy: 277.3024, Loss: 0.00020358\n",
            "Iteration 5/10, Entropy: 277.3056, Loss: 0.00021007\n",
            "Iteration 5/10, Entropy: 277.2944, Loss: 0.00020252\n",
            "Iteration 5/10, Entropy: 277.2836, Loss: 0.00020029\n",
            "Iteration 5/10, Entropy: 277.2864, Loss: 0.00020269\n",
            "Iteration 5/10, Entropy: 277.2908, Loss: 0.00019978\n",
            "Iteration 6/10, Entropy: 277.2998, Loss: 0.00020964\n",
            "Iteration 6/10, Entropy: 277.3091, Loss: 0.00020237\n",
            "Iteration 6/10, Entropy: 277.3249, Loss: 0.00020060\n",
            "Iteration 6/10, Entropy: 277.3428, Loss: 0.00020098\n",
            "Iteration 6/10, Entropy: 277.3489, Loss: 0.00019766\n",
            "Iteration 7/10, Entropy: 277.3547, Loss: 0.00020697\n",
            "Iteration 7/10, Entropy: 277.3601, Loss: 0.00020109\n",
            "Iteration 7/10, Entropy: 277.3584, Loss: 0.00019648\n",
            "Iteration 7/10, Entropy: 277.3620, Loss: 0.00019763\n",
            "Iteration 7/10, Entropy: 277.3621, Loss: 0.00019608\n",
            "Iteration 8/10, Entropy: 277.3630, Loss: 0.00020405\n",
            "Iteration 8/10, Entropy: 277.3680, Loss: 0.00019636\n",
            "Iteration 8/10, Entropy: 277.3724, Loss: 0.00019478\n",
            "Iteration 8/10, Entropy: 277.3816, Loss: 0.00019581\n",
            "Iteration 8/10, Entropy: 277.3924, Loss: 0.00019292\n",
            "Iteration 9/10, Entropy: 277.4016, Loss: 0.00020088\n",
            "Iteration 9/10, Entropy: 277.4164, Loss: 0.00019403\n",
            "Iteration 9/10, Entropy: 277.4262, Loss: 0.00019406\n",
            "Iteration 9/10, Entropy: 277.4373, Loss: 0.00019393\n",
            "Iteration 9/10, Entropy: 277.4345, Loss: 0.00019148\n",
            "Iteration 10/10, Entropy: 277.4317, Loss: 0.00020130\n",
            "Iteration 10/10, Entropy: 277.4333, Loss: 0.00019173\n",
            "Iteration 10/10, Entropy: 277.4363, Loss: 0.00018979\n",
            "Iteration 10/10, Entropy: 277.4397, Loss: 0.00019165\n",
            "Iteration 10/10, Entropy: 277.4404, Loss: 0.00019152\n",
            "weight diff tensor(1.8419e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.5.self_attn.q_proj | Shape: torch.Size([768, 768])\n",
            "Iteration 1/10, Entropy: 274.1803, Loss: 0.00578257\n",
            "Iteration 1/10, Entropy: 274.1758, Loss: 0.00548538\n",
            "Iteration 1/10, Entropy: 274.1641, Loss: 0.00535399\n",
            "Iteration 1/10, Entropy: 274.1562, Loss: 0.00516165\n",
            "Iteration 1/10, Entropy: 274.1586, Loss: 0.00512746\n",
            "Iteration 2/10, Entropy: 274.1621, Loss: 0.00498107\n",
            "Iteration 2/10, Entropy: 274.1636, Loss: 0.00498018\n",
            "Iteration 2/10, Entropy: 274.1654, Loss: 0.00495472\n",
            "Iteration 2/10, Entropy: 274.1664, Loss: 0.00487357\n",
            "Iteration 2/10, Entropy: 274.1689, Loss: 0.00481892\n",
            "Iteration 3/10, Entropy: 274.1743, Loss: 0.00473115\n",
            "Iteration 3/10, Entropy: 274.1815, Loss: 0.00475695\n",
            "Iteration 3/10, Entropy: 274.1895, Loss: 0.00478381\n",
            "Iteration 3/10, Entropy: 274.1950, Loss: 0.00474391\n",
            "Iteration 3/10, Entropy: 274.2004, Loss: 0.00474218\n",
            "Iteration 4/10, Entropy: 274.2057, Loss: 0.00463919\n",
            "Iteration 4/10, Entropy: 274.2116, Loss: 0.00466008\n",
            "Iteration 4/10, Entropy: 274.2186, Loss: 0.00466681\n",
            "Iteration 4/10, Entropy: 274.2239, Loss: 0.00468837\n",
            "Iteration 4/10, Entropy: 274.2278, Loss: 0.00468734\n",
            "Iteration 5/10, Entropy: 274.2287, Loss: 0.00455885\n",
            "Iteration 5/10, Entropy: 274.2286, Loss: 0.00459835\n",
            "Iteration 5/10, Entropy: 274.2280, Loss: 0.00461290\n",
            "Iteration 5/10, Entropy: 274.2250, Loss: 0.00459345\n",
            "Iteration 5/10, Entropy: 274.2223, Loss: 0.00459343\n",
            "Iteration 6/10, Entropy: 274.2194, Loss: 0.00452512\n",
            "Iteration 6/10, Entropy: 274.2180, Loss: 0.00453705\n",
            "Iteration 6/10, Entropy: 274.2180, Loss: 0.00457014\n",
            "Iteration 6/10, Entropy: 274.2189, Loss: 0.00456262\n",
            "Iteration 6/10, Entropy: 274.2213, Loss: 0.00455388\n",
            "Iteration 7/10, Entropy: 274.2249, Loss: 0.00450048\n",
            "Iteration 7/10, Entropy: 274.2302, Loss: 0.00448594\n",
            "Iteration 7/10, Entropy: 274.2357, Loss: 0.00452510\n",
            "Iteration 7/10, Entropy: 274.2398, Loss: 0.00453219\n",
            "Iteration 7/10, Entropy: 274.2432, Loss: 0.00452567\n",
            "Iteration 8/10, Entropy: 274.2446, Loss: 0.00448198\n",
            "Iteration 8/10, Entropy: 274.2460, Loss: 0.00445845\n",
            "Iteration 8/10, Entropy: 274.2467, Loss: 0.00449582\n",
            "Iteration 8/10, Entropy: 274.2466, Loss: 0.00451302\n",
            "Iteration 8/10, Entropy: 274.2472, Loss: 0.00450170\n",
            "Iteration 9/10, Entropy: 274.2490, Loss: 0.00444285\n",
            "Iteration 9/10, Entropy: 274.2525, Loss: 0.00443132\n",
            "Iteration 9/10, Entropy: 274.2565, Loss: 0.00447029\n",
            "Iteration 9/10, Entropy: 274.2601, Loss: 0.00448686\n",
            "Iteration 9/10, Entropy: 274.2641, Loss: 0.00447219\n",
            "Iteration 10/10, Entropy: 274.2667, Loss: 0.00443819\n",
            "Iteration 10/10, Entropy: 274.2689, Loss: 0.00439495\n",
            "Iteration 10/10, Entropy: 274.2687, Loss: 0.00445282\n",
            "Iteration 10/10, Entropy: 274.2664, Loss: 0.00448183\n",
            "Iteration 10/10, Entropy: 274.2645, Loss: 0.00446173\n",
            "weight diff tensor(2.8597e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: model.decoder.layers.5.self_attn.out_proj | Shape: torch.Size([768, 768])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5d1d1930ca70>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactivation_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mgptq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     gptq.fasterquant(\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBLOCK_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mpercdamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gptq.py\u001b[0m in \u001b[0;36mfasterquant\u001b[0;34m(self, blocksize, percdamp, group_size, actorder, static_groups)\u001b[0m\n\u001b[1;32m    152\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0mQ1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mLosses1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/quantizer.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/quantizer.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(x, scale, zero, maxq)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaxq\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mzero\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from safetensors.torch import save_file\n",
        "#from gptq import GPTQ\n",
        "import math\n",
        "\n",
        "# === CONFIG ===\n",
        "MODEL_NAME = \"facebook/opt-125m\"\n",
        "MODEL_NAME = \"databricks/dolly-v2-3b\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 2\n",
        "N_BATCHES = 5\n",
        "SEQ_LEN = 32\n",
        "NUM_BITS = 4\n",
        "BLOCK_SIZE = 128\n",
        "FIXED_T = 1000.0\n",
        "LR = 0.001\n",
        "NUM_ITERATIONS = 0\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "#model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",                # Automatically split layers across available GPUs/CPU\n",
        "    torch_dtype=\"float32\",               # Use float16 where possible\n",
        "    low_cpu_mem_usage=True            # Efficient weight loading\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# === Calibration Setup using TinyStories CSV ===\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"validation.csv\"        # Path to your TinyStories CSV\n",
        "TEXT_COLUMN = \"text\"               # Column containing stories\n",
        "N_CALIB_SAMPLES = 1000              # Number of samples to use\n",
        "\n",
        "# Load and preprocess CSV\n",
        "print(\"📖 Loading TinyStories from CSV...\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "assert TEXT_COLUMN in df.columns, f\"'{TEXT_COLUMN}' column not found in CSV.\"\n",
        "texts = df[TEXT_COLUMN].dropna().tolist()[:BATCH_SIZE * N_BATCHES]\n",
        "\n",
        "# Tokenize\n",
        "print(\"🔠 Tokenizing TinyStories for calibration...\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=SEQ_LEN,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "input_ids = encodings[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = encodings[\"attention_mask\"].to(DEVICE)\n",
        "input_batches = input_ids.split(BATCH_SIZE)\n",
        "mask_batches = attention_mask.split(BATCH_SIZE)\n",
        "def get_power_bins(a=0.5, num_bits=4, device=\"cpu\"):\n",
        "    q_levels = 2 ** num_bits\n",
        "    lin = torch.linspace(0, 1, q_levels, device=device)\n",
        "    scaled = (lin ** (1 / a)) * 0.5\n",
        "    bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "    return bins\n",
        "# === Define BlockwiseQuantizationOptim with GPTQ weight ===\n",
        "class BlockwiseQuantizationOptim(nn.Module):\n",
        "    def __init__(self, weight, block_size=128, num_bits=4, fixed_T=100.0, use_blockwise=True, a=0.5):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.num_bits = num_bits\n",
        "        self.fixed_T = fixed_T\n",
        "        self.a = a\n",
        "        self.original_shape = weight.shape\n",
        "        self.use_blockwise = use_blockwise\n",
        "        self.num_levels = 2 ** num_bits\n",
        "\n",
        "        if use_blockwise:\n",
        "            padded_rows = math.ceil(weight.size(0) / block_size) * block_size\n",
        "            padded_cols = math.ceil(weight.size(1) / block_size) * block_size\n",
        "            self.padded_weight = torch.zeros((padded_rows, padded_cols), device=weight.device)\n",
        "            self.padded_weight[:weight.size(0), :weight.size(1)] = weight\n",
        "\n",
        "            self.blocks = []\n",
        "            self.block_metadata = []\n",
        "            for i in range(0, padded_rows, block_size):\n",
        "                for j in range(0, padded_cols, block_size):\n",
        "                    block = self.padded_weight[i:i+block_size, j:j+block_size]\n",
        "                    self.blocks.append(block)\n",
        "                    self.block_metadata.append((i, j))\n",
        "\n",
        "            self.w_min = nn.ParameterList()\n",
        "            self.w_max = nn.ParameterList()\n",
        "            self.learnable_bins = nn.ParameterList()\n",
        "            for block in self.blocks:\n",
        "                w_min, w_max = block.min().detach(), block.max().detach()\n",
        "                pad = 0.05 * (w_max - w_min)\n",
        "                self.w_min.append(nn.Parameter((w_min - pad).view(1)))\n",
        "                self.w_max.append(nn.Parameter((w_max + pad).view(1)))\n",
        "                init_bins = self._get_power_bins().detach()\n",
        "                self.learnable_bins.append(nn.Parameter(init_bins.clone()))\n",
        "        else:\n",
        "            self.blocks = [weight]\n",
        "            self.block_metadata = [(0, 0)]\n",
        "            w_min, w_max = weight.min().detach(), weight.max().detach()\n",
        "            pad = 0.05 * (w_max - w_min)\n",
        "            self.w_min = nn.Parameter((w_min - pad).view(1))\n",
        "            self.w_max = nn.Parameter((w_max + pad).view(1))\n",
        "            init_bins = self._get_power_bins().detach()\n",
        "            self.learnable_bins = nn.Parameter(init_bins.clone())\n",
        "\n",
        "    def _get_power_bins(self):\n",
        "        lin = torch.linspace(0, 1, 2 ** self.num_bits)\n",
        "        scaled = (lin ** (1 / self.a)) * 0.5\n",
        "        bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "        return bins\n",
        "\n",
        "    def forward(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "        total_entropy = 0.0\n",
        "\n",
        "        if not self.use_blockwise:\n",
        "            blocks = [self.blocks[0]]\n",
        "            w_mins = [self.w_min]\n",
        "            w_maxs = [self.w_max]\n",
        "            bin_lists = [self.learnable_bins]\n",
        "        else:\n",
        "            blocks = self.blocks\n",
        "            w_mins = self.w_min\n",
        "            w_maxs = self.w_max\n",
        "            bin_lists = self.learnable_bins\n",
        "\n",
        "        for idx, block in enumerate(blocks):\n",
        "            w_min = w_mins[idx].clamp(max=w_maxs[idx].item() - eps)\n",
        "            w_max = w_maxs[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "\n",
        "            bins = bin_lists[idx].to(block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - bins)\n",
        "            soft_probs = torch.softmax(dists * self.fixed_T, dim=-1)\n",
        "            w_q = (soft_probs * bins).sum(dim=-1)\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "            # Entropy penalty\n",
        "            bin_mass = soft_probs.sum(dim=0)\n",
        "            bin_probs = bin_mass / (bin_mass.sum() + eps)\n",
        "            entropy = -(bin_probs * (bin_probs + eps).log()).sum()\n",
        "            total_entropy += entropy\n",
        "\n",
        "        if self.use_blockwise:\n",
        "            padded_out = torch.zeros_like(self.padded_weight)\n",
        "            for idx, (i, j) in enumerate(self.block_metadata):\n",
        "                padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "            return padded_out[:self.original_shape[0], :self.original_shape[1]], total_entropy\n",
        "        else:\n",
        "            return q_blocks[0], total_entropy\n",
        "\n",
        "    def export(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "\n",
        "        if not self.use_blockwise:\n",
        "            blocks = [self.blocks[0]]\n",
        "            w_mins = [self.w_min]\n",
        "            w_maxs = [self.w_max]\n",
        "            bin_lists = [self.learnable_bins]\n",
        "        else:\n",
        "            blocks = self.blocks\n",
        "            w_mins = self.w_min\n",
        "            w_maxs = self.w_max\n",
        "            bin_lists = self.learnable_bins\n",
        "\n",
        "        for idx, block in enumerate(blocks):\n",
        "            w_min = w_mins[idx].clamp(max=w_maxs[idx].item() - eps)\n",
        "            w_max = w_maxs[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "\n",
        "            bins = bin_lists[idx].to(block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - bins)\n",
        "            q_idx = torch.argmax(dists, dim=-1).to(torch.int32)\n",
        "            w_q = bins[q_idx]\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "        if self.use_blockwise:\n",
        "            padded_out = torch.zeros_like(self.padded_weight)\n",
        "            for idx, (i, j) in enumerate(self.block_metadata):\n",
        "                padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "            return padded_out[:self.original_shape[0], :self.original_shape[1]].cpu()\n",
        "        else:\n",
        "            return q_blocks[0].cpu()\n",
        "\n",
        "\n",
        "# === Quantization Loop for all Linear Layers ===\n",
        "safetensor_dict = {}\n",
        "flag = 0\n",
        "for name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "    # if \"lm_head\" in name:\n",
        "    #     continue\n",
        "    # if \"fc1\" in name:\n",
        "    #     continue\n",
        "\n",
        "    if \"embed_out\" in name:\n",
        "        continue\n",
        "\n",
        "\n",
        "    print(f\"\\n🔧 GPTQ + Blockwise Quantizing Layer: {name} | Shape: {module.weight.shape}\")\n",
        "\n",
        "    activation_batches = []\n",
        "    def hook_fn(mod, inp, out):\n",
        "        activation_batches.append(inp[0].detach())\n",
        "    hook = module.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, m in zip(input_batches, mask_batches):\n",
        "            model(input_ids=x, attention_mask=m)\n",
        "    hook.remove()\n",
        "\n",
        "    if not activation_batches:\n",
        "        continue\n",
        "    original_weight = module.weight.data.clone()\n",
        "    # gptq = GPTQ(module)\n",
        "    # for act in activation_batches:\n",
        "    #     gptq.add_batch(act, module(act))\n",
        "    # gptq.fasterquant(\n",
        "    #     blocksize=BLOCK_SIZE,\n",
        "    #     percdamp=0.01,\n",
        "    #     group_size=128,\n",
        "    #     actorder=True,\n",
        "    # )\n",
        "    q_weight = module.weight.data.clone()\n",
        "\n",
        "    quant_layer = BlockwiseQuantizationOptim(original_weight,num_bits = 4, use_blockwise=True,a=0.52)#.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(quant_layer.parameters(), lr=LR)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    #original_weight = module.weight.data.clone()\n",
        "\n",
        "    for it in range(NUM_ITERATIONS):\n",
        "        for act in activation_batches:\n",
        "            optimizer.zero_grad()\n",
        "            w_q, entropy = quant_layer()\n",
        "            recon = F.linear(act.to(DEVICE), w_q)\n",
        "            target = F.linear(act.to(DEVICE), original_weight)\n",
        "            loss = mse_loss(recon, target) + mse_loss(original_weight, w_q)\n",
        "            print(f\"Iteration {it + 1}/{NUM_ITERATIONS}, Entropy: {entropy.item():.4f}, Loss: {loss.item():.8f}\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        final_weight = quant_layer.export().to(module.weight.device)\n",
        "        loss = mse_loss(original_weight, final_weight)\n",
        "        print(\"weight diff\",loss)\n",
        "        module.weight.copy_(final_weight)\n",
        "        #safetensor_dict[name.replace(\".\", \"_\") + \".dequant\"] = final_weight\n",
        "    del quant_layer, optimizer, activation_batches\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# === Save Final Weights ===\n",
        "#save_file(safetensor_dict, \"quantized_blockwise_gptq.safetensors\")\n",
        "print(\"\\n✅ Finished GPTQ-initialized blockwise quantization for all layers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipOaV4-i5gLg",
        "outputId": "fc96649c-564b-40c8-e9bd-1358e26feb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Loading TinyStories from CSV...\n",
            "🔠 Tokenizing TinyStories for calibration...\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(3.4007e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.7337e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(1.7251e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.5286e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(3.3509e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.7315e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.4011e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(4.9944e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.6596e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.3338e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.5829e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(5.5860e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.7991e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.6629e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.2387e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(3.0334e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.8430e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.7147e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.4760e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(4.3143e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.9233e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.6852e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.1864e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(3.1973e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.5234e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.6981e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.1201e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.8775e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.5089e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.8526e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.1033e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.7362e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.6309e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.9500e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0664e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.6411e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.5112e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(2.0600e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0389e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.3569e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.7089e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.9107e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0118e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.2662e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.5000e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.9829e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(1.9961e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.2697e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.4800e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(2.1129e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0089e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.1295e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.5197e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(1.9806e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0098e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.3829e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.6712e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(2.1656e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0218e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.5205e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.6099e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(2.1791e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0097e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.5913e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.6310e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(2.1423e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0306e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.6297e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.7709e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(2.2143e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(2.0821e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.8201e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.18.attention.query_key_value | Shape: torch.Size([7680, 2560])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Tensor.item() cannot be called on meta tensors",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-b1a65e78179c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mfinal_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight diff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-b1a65e78179c>\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mw_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0mw_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mw_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_min\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_max\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_min\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36mmeta_local_scalar_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6583\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mregister_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_scalar_dense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6584\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_local_scalar_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor.item() cannot be called on meta tensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Tensor.item() cannot be called on meta tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "# ==== Test quantized model ====\n",
        "#model.eval()\n",
        "prompt = \"I like travelling to my\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=30)\n",
        "\n",
        "print(\"Sample Output:\", tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P3NUbrnZ34J",
        "outputId": "b43a57b8-472e-4402-ac40-3cc3c1369181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Output: I like travelling to my friends' houses. I like the idea of being able to go to my friends' houses and not having to worry about anything\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "# ==== Test quantized model ====\n",
        "#model.eval()\n",
        "prompt = \"once upon a time in\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=30)\n",
        "\n",
        "print(\"Sample Output:\", tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4mHB6i9g2bS",
        "outputId": "679252c1-bed9-4bc2-8706-60318179b1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Output: once upon a time in the land of Oz, there was a witch who had a beautiful daughter. The daughter was so beautiful that she was known as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyf_VU8Ygsmb",
        "outputId": "5ecd04aa-8b76-478e-fa3a-c3f3bd758351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50280, 2560)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
              "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "  )\n",
              "  (embed_out): Linear(in_features=2560, out_features=50280, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w00qDKi6bBuk",
        "outputId": "af9e321b-99c1-461b-b48f-f153b7691728"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",                # Automatically split layers across available GPUs/CPU\n",
        "    torch_dtype=\"float32\",               # Use float16 where possible\n",
        "    low_cpu_mem_usage=True            # Efficient weight loading\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "s0IRdla1ZKC6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def evaluate_hellaswag_one_sample(model,tokeniser, sample_index=None, max_new_tokens=30):\n",
        "    # Load one sample from HellaSwag\n",
        "    hellaswag = load_dataset(\"hellaswag\", split=\"validation\")\n",
        "    sample = hellaswag[sample_index if sample_index is not None else random.randint(0, len(hellaswag) - 1)]\n",
        "\n",
        "    ctx = sample[\"ctx\"]\n",
        "    options = sample[\"endings\"]\n",
        "    label = int(sample[\"label\"])  # Ground truth index (0-3)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"Context:\n",
        "{ctx}\n",
        "\n",
        "Which of the following is the most plausible continuation?\n",
        "\n",
        "Options:\n",
        "A. {options[0]}\n",
        "B. {options[1]}\n",
        "C. {options[2]}\n",
        "D. {options[3]}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Load quantized model and tokenizer\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    # model = AutoModelForCausalLM.from_pretrained(\n",
        "    #     model_path,\n",
        "    #     torch_dtype=torch.float16,\n",
        "    #     device_map=\"auto\"\n",
        "    # )\n",
        "    # model.eval()\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=inputs['input_ids'].shape[1] + max_new_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Try to extract answer letter (A/B/C/D)\n",
        "    match = re.search(r\"Answer[:\\- ]*\\s*([A-D])\", response)\n",
        "    predicted_letter = match.group(1) if match else \"Unknown\"\n",
        "\n",
        "    # Report\n",
        "    print(\"🧠 Prompt:\\n\", prompt)\n",
        "    print(\"🗣️ Model Output:\\n\", response)\n",
        "    print(f\"\\n✅ Predicted Answer: {predicted_letter}\")\n",
        "    print(f\"🎯 Ground Truth Answer: {chr(65 + label)} ({label})\")\n",
        "\n",
        "    return {\n",
        "        \"context\": ctx,\n",
        "        \"options\": options,\n",
        "        \"prompt\": prompt,\n",
        "        \"model_output\": response,\n",
        "        \"predicted\": predicted_letter,\n",
        "        \"ground_truth\": chr(65 + label)\n",
        "    }\n",
        "evaluate_hellaswag_one_sample(test_model,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIZ61EkAdyKY",
        "outputId": "cd6dd4d4-6c8e-47c2-9756-68b67006560b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Prompt:\n",
            " Context:\n",
            "[header] How to measure a saddle [title] Learn what the bars are. [step] When you're looking at new saddles, one of the things you'll need to pay attention to is the bars. The bars are the weight-distributing foundation for the saddle; the part of the saddle that rests on the horse and holds you up.\n",
            "\n",
            "Which of the following is the most plausible continuation?\n",
            "\n",
            "Options:\n",
            "A. There are two' bars' that distribute weight evenly on either side of the spine. If your saddle is well-fitting, the horse's back will be in contact with the entire length of the bars.\n",
            "B. Most new saddles have bars on their backs that hang to the sides. [substeps] If your horse's saddle only has bars, flip the bars over so that the back is facing away from you.\n",
            "C. The bars are located at the top of the horse and at either end of the saddle, which are the sides that you'll almost touch. [substeps] The bars range from two-hundred-200.\n",
            "D. While the bars are peerequisite for the saddle, they're also useful if you want an added comfort. [substeps] For a standard saddle, half of the weight on the bar weights toward the seat rather than the rear.\n",
            "\n",
            "Answer:\n",
            "🗣️ Model Output:\n",
            " Context:\n",
            "[header] How to measure a saddle [title] Learn what the bars are. [step] When you're looking at new saddles, one of the things you'll need to pay attention to is the bars. The bars are the weight-distributing foundation for the saddle; the part of the saddle that rests on the horse and holds you up.\n",
            "\n",
            "Which of the following is the most plausible continuation?\n",
            "\n",
            "Options:\n",
            "A. There are two' bars' that distribute weight evenly on either side of the spine. If your saddle is well-fitting, the horse's back will be in contact with the entire length of the bars.\n",
            "B. Most new saddles have bars on their backs that hang to the sides. [substeps] If your horse's saddle only has bars, flip the bars over so that the back is facing away from you.\n",
            "C. The bars are located at the top of the horse and at either end of the saddle, which are the sides that you'll almost touch. [substeps] The bars range from two-hundred-200.\n",
            "D. While the bars are peerequisite for the saddle, they're also useful if you want an added comfort. [substeps] For a standard saddle, half of the weight on the bar weights toward the seat rather than the rear.\n",
            "\n",
            "Answer: B\n",
            "\n",
            "[substeps] If your horse's saddle only has bars, flip the bars over so that the back is facing away from you.\n",
            "\n",
            "✅ Predicted Answer: B\n",
            "🎯 Ground Truth Answer: A (0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"[header] How to measure a saddle [title] Learn what the bars are. [step] When you're looking at new saddles, one of the things you'll need to pay attention to is the bars. The bars are the weight-distributing foundation for the saddle; the part of the saddle that rests on the horse and holds you up.\",\n",
              " 'options': [\"There are two' bars' that distribute weight evenly on either side of the spine. If your saddle is well-fitting, the horse's back will be in contact with the entire length of the bars.\",\n",
              "  \"Most new saddles have bars on their backs that hang to the sides. [substeps] If your horse's saddle only has bars, flip the bars over so that the back is facing away from you.\",\n",
              "  \"The bars are located at the top of the horse and at either end of the saddle, which are the sides that you'll almost touch. [substeps] The bars range from two-hundred-200.\",\n",
              "  \"While the bars are peerequisite for the saddle, they're also useful if you want an added comfort. [substeps] For a standard saddle, half of the weight on the bar weights toward the seat rather than the rear.\"],\n",
              " 'prompt': \"Context:\\n[header] How to measure a saddle [title] Learn what the bars are. [step] When you're looking at new saddles, one of the things you'll need to pay attention to is the bars. The bars are the weight-distributing foundation for the saddle; the part of the saddle that rests on the horse and holds you up.\\n\\nWhich of the following is the most plausible continuation?\\n\\nOptions:\\nA. There are two' bars' that distribute weight evenly on either side of the spine. If your saddle is well-fitting, the horse's back will be in contact with the entire length of the bars.\\nB. Most new saddles have bars on their backs that hang to the sides. [substeps] If your horse's saddle only has bars, flip the bars over so that the back is facing away from you.\\nC. The bars are located at the top of the horse and at either end of the saddle, which are the sides that you'll almost touch. [substeps] The bars range from two-hundred-200.\\nD. While the bars are peerequisite for the saddle, they're also useful if you want an added comfort. [substeps] For a standard saddle, half of the weight on the bar weights toward the seat rather than the rear.\\n\\nAnswer:\",\n",
              " 'model_output': \"Context:\\n[header] How to measure a saddle [title] Learn what the bars are. [step] When you're looking at new saddles, one of the things you'll need to pay attention to is the bars. The bars are the weight-distributing foundation for the saddle; the part of the saddle that rests on the horse and holds you up.\\n\\nWhich of the following is the most plausible continuation?\\n\\nOptions:\\nA. There are two' bars' that distribute weight evenly on either side of the spine. If your saddle is well-fitting, the horse's back will be in contact with the entire length of the bars.\\nB. Most new saddles have bars on their backs that hang to the sides. [substeps] If your horse's saddle only has bars, flip the bars over so that the back is facing away from you.\\nC. The bars are located at the top of the horse and at either end of the saddle, which are the sides that you'll almost touch. [substeps] The bars range from two-hundred-200.\\nD. While the bars are peerequisite for the saddle, they're also useful if you want an added comfort. [substeps] For a standard saddle, half of the weight on the bar weights toward the seat rather than the rear.\\n\\nAnswer: B\\n\\n[substeps] If your horse's saddle only has bars, flip the bars over so that the back is facing away from you.\",\n",
              " 'predicted': 'B',\n",
              " 'ground_truth': 'A'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def evaluate_piqa_one_sample(model, tokenizer, sample_index=None, max_new_tokens=30): # Added tokenizer argument\n",
        "    # Load PIQA dataset\n",
        "    dataset = load_dataset(\"piqa\", split=\"validation\")\n",
        "    sample = dataset[sample_index if sample_index is not None else random.randint(0, len(dataset) - 1)]\n",
        "\n",
        "    goal = sample[\"goal\"]\n",
        "    sol1 = sample[\"sol1\"]\n",
        "    sol2 = sample[\"sol2\"]\n",
        "    label = int(sample[\"label\"])  # correct answer (0 or 1)\n",
        "\n",
        "    # Format prompt for LLM\n",
        "    prompt = f\"\"\"Task: Choose the most physically plausible solution to the following goal.\n",
        "\n",
        "Goal: {goal}\n",
        "\n",
        "Options:\n",
        "A. {sol1}\n",
        "B. {sol2}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Inference\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # Use the passed tokenizer\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=inputs[\"input_ids\"].shape[1] + max_new_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True) # Use the passed tokenizer\n",
        "\n",
        "    # Extract A/B prediction\n",
        "    match = re.search(r\"Answer[:\\- ]*\\s*([A-B])\", response)\n",
        "    predicted = match.group(1) if match else \"Unknown\"\n",
        "\n",
        "    print(\"🧠 Prompt:\\n\", prompt)\n",
        "    print(\"🗣️ Model Output:\\n\", response)\n",
        "    print(f\"\\n✅ Predicted Answer: {predicted}\")\n",
        "    print(f\"🎯 Ground Truth Answer: {'A' if label == 0 else 'B'}\")\n",
        "\n",
        "    return {\n",
        "        \"goal\": goal,\n",
        "        \"option_A\": sol1,\n",
        "        \"option_B\": sol2,\n",
        "        \"prompt\": prompt,\n",
        "        \"model_output\": response,\n",
        "        \"predicted\": predicted,\n",
        "        \"ground_truth\": \"A\" if label == 0 else \"B\"\n",
        "    }\n",
        "\n",
        "# Call the function with the model and tokenizer objects\n",
        "evaluate_piqa_one_sample(model, tokenizer) # Pass the tokenizer object"
      ],
      "metadata": {
        "id": "CDQ_6zxbdyUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611787d8-50b3-49cb-a52d-41033e4cc194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Prompt:\n",
            " Task: Choose the most physically plausible solution to the following goal.\n",
            "\n",
            "Goal: How can you keep your taco shell crispy when building your tacos?\n",
            "\n",
            "Options:\n",
            "A. Place cheese in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\n",
            "B. Place sauce in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\n",
            "\n",
            "Answer:\n",
            "🗣️ Model Output:\n",
            " Task: Choose the most physically plausible solution to the following goal.\n",
            "\n",
            "Goal: How can you keep your taco shell crispy when building your tacos?\n",
            "\n",
            "Options:\n",
            "A. Place cheese in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\n",
            "B. Place sauce in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\n",
            "\n",
            "Answer: B. Place sauce in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in\n",
            "\n",
            "✅ Predicted Answer: B\n",
            "🎯 Ground Truth Answer: A\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'goal': 'How can you keep your taco shell crispy when building your tacos?',\n",
              " 'option_A': 'Place cheese in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.',\n",
              " 'option_B': 'Place sauce in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.',\n",
              " 'prompt': 'Task: Choose the most physically plausible solution to the following goal.\\n\\nGoal: How can you keep your taco shell crispy when building your tacos?\\n\\nOptions:\\nA. Place cheese in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\\nB. Place sauce in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\\n\\nAnswer:',\n",
              " 'model_output': 'Task: Choose the most physically plausible solution to the following goal.\\n\\nGoal: How can you keep your taco shell crispy when building your tacos?\\n\\nOptions:\\nA. Place cheese in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\\nB. Place sauce in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in other fillings.\\n\\nAnswer: B. Place sauce in the taco before any of the other fillings, it will form a shield over the shell and protect it from moisture in',\n",
              " 'predicted': 'B',\n",
              " 'ground_truth': 'A'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from safetensors.torch import save_file\n",
        "#from gptq import GPTQ\n",
        "import math\n",
        "\n",
        "# === CONFIG ===\n",
        "MODEL_NAME = \"facebook/opt-125m\"\n",
        "MODEL_NAME = \"databricks/dolly-v2-3b\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32\n",
        "N_BATCHES = 8\n",
        "SEQ_LEN = 128\n",
        "NUM_BITS = 3\n",
        "BLOCK_SIZE = 128\n",
        "FIXED_T = 1000.0\n",
        "LR = 0.001\n",
        "NUM_ITERATIONS = 2\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "#model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",                # Automatically split layers across available GPUs/CPU\n",
        "    torch_dtype=\"float32\",               # Use float16 where possible\n",
        "    low_cpu_mem_usage=True            # Efficient weight loading\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# === Calibration Setup using TinyStories CSV ===\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"validation.csv\"        # Path to your TinyStories CSV\n",
        "TEXT_COLUMN = \"text\"               # Column containing stories\n",
        "N_CALIB_SAMPLES = 1000              # Number of samples to use\n",
        "\n",
        "# Load and preprocess CSV\n",
        "print(\"📖 Loading TinyStories from CSV...\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "assert TEXT_COLUMN in df.columns, f\"'{TEXT_COLUMN}' column not found in CSV.\"\n",
        "texts = df[TEXT_COLUMN].dropna().tolist()[:BATCH_SIZE * N_BATCHES]\n",
        "\n",
        "# Tokenize\n",
        "print(\"🔠 Tokenizing TinyStories for calibration...\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=SEQ_LEN,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "input_ids = encodings[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = encodings[\"attention_mask\"].to(DEVICE)\n",
        "input_batches = input_ids.split(BATCH_SIZE)\n",
        "mask_batches = attention_mask.split(BATCH_SIZE)\n",
        "def get_power_bins(a=0.5, num_bits=4, device=\"cpu\"):\n",
        "    q_levels = 2 ** num_bits\n",
        "    lin = torch.linspace(0, 1, q_levels, device=device)\n",
        "    scaled = (lin ** (1 / a)) * 0.5\n",
        "    bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "    return bins\n",
        "# === Define BlockwiseQuantizationOptim with GPTQ weight ===\n",
        "class BlockwiseQuantizationOptim(nn.Module):\n",
        "    def __init__(self, weight, block_size=128, num_bits=4, fixed_T=100.0, use_blockwise=True, a=0.5):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.num_bits = num_bits\n",
        "        self.fixed_T = fixed_T\n",
        "        self.a = a\n",
        "        self.original_shape = weight.shape\n",
        "        self.use_blockwise = use_blockwise\n",
        "        self.num_levels = 2 ** num_bits\n",
        "\n",
        "        if use_blockwise:\n",
        "            padded_rows = math.ceil(weight.size(0) / block_size) * block_size\n",
        "            padded_cols = math.ceil(weight.size(1) / block_size) * block_size\n",
        "            self.padded_weight = torch.zeros((padded_rows, padded_cols), device=weight.device)\n",
        "            self.padded_weight[:weight.size(0), :weight.size(1)] = weight\n",
        "\n",
        "            self.blocks = []\n",
        "            self.block_metadata = []\n",
        "            for i in range(0, padded_rows, block_size):\n",
        "                for j in range(0, padded_cols, block_size):\n",
        "                    block = self.padded_weight[i:i+block_size, j:j+block_size]\n",
        "                    self.blocks.append(block)\n",
        "                    self.block_metadata.append((i, j))\n",
        "\n",
        "            self.w_min = nn.ParameterList()\n",
        "            self.w_max = nn.ParameterList()\n",
        "            self.learnable_bins = nn.ParameterList()\n",
        "            for block in self.blocks:\n",
        "                w_min, w_max = block.min().detach(), block.max().detach()\n",
        "                pad = 0.05 * (w_max - w_min)\n",
        "                self.w_min.append(nn.Parameter((w_min - pad).view(1)))\n",
        "                self.w_max.append(nn.Parameter((w_max + pad).view(1)))\n",
        "                init_bins = self._get_power_bins().detach()\n",
        "                self.learnable_bins.append(nn.Parameter(init_bins.clone()))\n",
        "        else:\n",
        "            self.blocks = [weight]\n",
        "            self.block_metadata = [(0, 0)]\n",
        "            w_min, w_max = weight.min().detach(), weight.max().detach()\n",
        "            pad = 0.05 * (w_max - w_min)\n",
        "            self.w_min = nn.Parameter((w_min - pad).view(1))\n",
        "            self.w_max = nn.Parameter((w_max + pad).view(1))\n",
        "            init_bins = self._get_power_bins().detach()\n",
        "            self.learnable_bins = nn.Parameter(init_bins.clone())\n",
        "\n",
        "    def _get_power_bins(self):\n",
        "        lin = torch.linspace(0, 1, 2 ** self.num_bits)\n",
        "        scaled = (lin ** (1 / self.a)) * 0.5\n",
        "        bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "        return bins\n",
        "\n",
        "    def forward(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "        total_entropy = 0.0\n",
        "\n",
        "        if not self.use_blockwise:\n",
        "            blocks = [self.blocks[0]]\n",
        "            w_mins = [self.w_min]\n",
        "            w_maxs = [self.w_max]\n",
        "            bin_lists = [self.learnable_bins]\n",
        "        else:\n",
        "            blocks = self.blocks\n",
        "            w_mins = self.w_min\n",
        "            w_maxs = self.w_max\n",
        "            bin_lists = self.learnable_bins\n",
        "\n",
        "        for idx, block in enumerate(blocks):\n",
        "            w_min = w_mins[idx].clamp(max=w_maxs[idx].item() - eps)\n",
        "            w_max = w_maxs[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "\n",
        "            bins = bin_lists[idx].to(block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - bins)\n",
        "            soft_probs = torch.softmax(dists * self.fixed_T, dim=-1)\n",
        "            w_q = (soft_probs * bins).sum(dim=-1)\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "            # Entropy penalty\n",
        "            bin_mass = soft_probs.sum(dim=0)\n",
        "            bin_probs = bin_mass / (bin_mass.sum() + eps)\n",
        "            entropy = -(bin_probs * (bin_probs + eps).log()).sum()\n",
        "            total_entropy += entropy\n",
        "\n",
        "        if self.use_blockwise:\n",
        "            padded_out = torch.zeros_like(self.padded_weight)\n",
        "            for idx, (i, j) in enumerate(self.block_metadata):\n",
        "                padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "            return padded_out[:self.original_shape[0], :self.original_shape[1]], total_entropy\n",
        "        else:\n",
        "            return q_blocks[0], total_entropy\n",
        "\n",
        "    def export(self):\n",
        "        eps = 1e-6\n",
        "        q_blocks = []\n",
        "\n",
        "        if not self.use_blockwise:\n",
        "            blocks = [self.blocks[0]]\n",
        "            w_mins = [self.w_min]\n",
        "            w_maxs = [self.w_max]\n",
        "            bin_lists = [self.learnable_bins]\n",
        "        else:\n",
        "            blocks = self.blocks\n",
        "            w_mins = self.w_min\n",
        "            w_maxs = self.w_max\n",
        "            bin_lists = self.learnable_bins\n",
        "\n",
        "        for idx, block in enumerate(blocks):\n",
        "            w_min = w_mins[idx].clamp(max=w_maxs[idx].item() - eps)\n",
        "            w_max = w_maxs[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (block - w_min) / (w_max - w_min + eps)\n",
        "\n",
        "            bins = bin_lists[idx].to(block.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - bins)\n",
        "            q_idx = torch.argmax(dists, dim=-1).to(torch.int32)\n",
        "            w_q = bins[q_idx]\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            q_blocks.append(w_deq)\n",
        "\n",
        "        if self.use_blockwise:\n",
        "            padded_out = torch.zeros_like(self.padded_weight)\n",
        "            for idx, (i, j) in enumerate(self.block_metadata):\n",
        "                padded_out[i:i+self.block_size, j:j+self.block_size] = q_blocks[idx]\n",
        "            return padded_out[:self.original_shape[0], :self.original_shape[1]].cpu()\n",
        "        else:\n",
        "            return q_blocks[0].cpu()\n",
        "\n",
        "\n",
        "# === Quantization Loop for all Linear Layers ===\n",
        "safetensor_dict = {}\n",
        "flag = 0\n",
        "for name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "    # if \"lm_head\" in name:\n",
        "    #     continue\n",
        "    # if \"fc1\" in name:\n",
        "    #     continue\n",
        "\n",
        "    if \"embed_out\" in name:\n",
        "        continue\n",
        "\n",
        "\n",
        "    print(f\"\\n🔧 GPTQ + Blockwise Quantizing Layer: {name} | Shape: {module.weight.shape}\")\n",
        "\n",
        "    activation_batches = []\n",
        "    def hook_fn(mod, inp, out):\n",
        "        activation_batches.append(inp[0].detach())\n",
        "    hook = module.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, m in zip(input_batches, mask_batches):\n",
        "            model(input_ids=x, attention_mask=m)\n",
        "    hook.remove()\n",
        "\n",
        "    if not activation_batches:\n",
        "        continue\n",
        "    original_weight = module.weight.data.clone()\n",
        "    # gptq = GPTQ(module)\n",
        "    # for act in activation_batches:\n",
        "    #     gptq.add_batch(act, module(act))\n",
        "    # gptq.fasterquant(\n",
        "    #     blocksize=BLOCK_SIZE,\n",
        "    #     percdamp=0.01,\n",
        "    #     group_size=128,\n",
        "    #     actorder=True,\n",
        "    # )\n",
        "    q_weight = module.weight.data.clone()\n",
        "\n",
        "    quant_layer = BlockwiseQuantizationOptim(original_weight,num_bits = 4, use_blockwise=True,a=0.52)#.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(quant_layer.parameters(), lr=LR)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    for it in range(NUM_ITERATIONS):\n",
        "        for x, m in zip(input_batches, mask_batches):\n",
        "            # CPU capture, per-batch\n",
        "            activation_holder = []\n",
        "            def hook_fn(mod, inp, out):\n",
        "                activation_holder.append(inp[0].detach().to('cpu'))\n",
        "            hook = module.register_forward_hook(hook_fn)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model(input_ids=x, attention_mask=m)\n",
        "            hook.remove()\n",
        "\n",
        "            act = activation_holder[0].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            w_q, entropy = quant_layer()\n",
        "            recon = F.linear(act, w_q)\n",
        "            target = F.linear(act, original_weight)\n",
        "            loss = mse_loss(recon, target) + mse_loss(original_weight, w_q)\n",
        "            print(f\"Iter {it+1}: Entropy={entropy.item():.4f}, Loss={loss.item():.6f}\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            del act, recon, target, w_q, activation_holder\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        final_weight = quant_layer.export().to(module.weight.device)\n",
        "        loss = mse_loss(original_weight, final_weight)\n",
        "        print(\"weight diff\",loss)\n",
        "        module.weight.copy_(final_weight)\n",
        "        #safetensor_dict[name.replace(\".\", \"_\") + \".dequant\"] = final_weight\n",
        "    del quant_layer, optimizer, activation_batches\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# === Save Final Weights ===\n",
        "#save_file(safetensor_dict, \"quantized_blockwise_gptq.safetensors\")\n",
        "print(\"\\n✅ Finished GPTQ-initialized blockwise quantization for all layers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bkNGSIalmJpP",
        "outputId": "901f586d-1e2a-4bbf-d843-2cb7beb3a6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Loading TinyStories from CSV...\n",
            "🔠 Tokenizing TinyStories for calibration...\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "Iter 1: Entropy=9166.7119, Loss=0.000596\n",
            "Iter 1: Entropy=9166.1787, Loss=0.000502\n",
            "Iter 1: Entropy=9165.9590, Loss=0.000441\n",
            "Iter 1: Entropy=9166.7803, Loss=0.000401\n",
            "Iter 1: Entropy=9168.5947, Loss=0.000371\n",
            "Iter 1: Entropy=9171.1123, Loss=0.000345\n",
            "Iter 1: Entropy=9174.0664, Loss=0.000320\n",
            "Iter 1: Entropy=9177.1309, Loss=0.000303\n",
            "Iter 2: Entropy=9180.1807, Loss=0.000285\n",
            "Iter 2: Entropy=9183.1104, Loss=0.000273\n",
            "Iter 2: Entropy=9185.8311, Loss=0.000259\n",
            "Iter 2: Entropy=9188.2900, Loss=0.000248\n",
            "Iter 2: Entropy=9190.4863, Loss=0.000236\n",
            "Iter 2: Entropy=9192.3740, Loss=0.000224\n",
            "Iter 2: Entropy=9193.9043, Loss=0.000213\n",
            "Iter 2: Entropy=9195.1504, Loss=0.000206\n",
            "weight diff tensor(2.8527e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "Iter 1: Entropy=3086.2485, Loss=0.000056\n",
            "Iter 1: Entropy=3086.2996, Loss=0.000042\n",
            "Iter 1: Entropy=3086.2681, Loss=0.000035\n",
            "Iter 1: Entropy=3086.5195, Loss=0.000031\n",
            "Iter 1: Entropy=3087.0256, Loss=0.000029\n",
            "Iter 1: Entropy=3087.7092, Loss=0.000025\n",
            "Iter 1: Entropy=3088.4265, Loss=0.000025\n",
            "Iter 1: Entropy=3089.0857, Loss=0.000022\n",
            "Iter 2: Entropy=3089.7048, Loss=0.000019\n",
            "Iter 2: Entropy=3090.2856, Loss=0.000018\n",
            "Iter 2: Entropy=3090.8281, Loss=0.000017\n",
            "Iter 2: Entropy=3091.3237, Loss=0.000016\n",
            "Iter 2: Entropy=3091.7610, Loss=0.000015\n",
            "Iter 2: Entropy=3092.1565, Loss=0.000014\n",
            "Iter 2: Entropy=3092.5151, Loss=0.000015\n",
            "Iter 2: Entropy=3092.8755, Loss=0.000014\n",
            "weight diff tensor(1.5802e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "Iter 1: Entropy=12538.4297, Loss=0.000487\n",
            "Iter 1: Entropy=12539.7471, Loss=0.000374\n",
            "Iter 1: Entropy=12541.4941, Loss=0.000329\n",
            "Iter 1: Entropy=12544.3145, Loss=0.000299\n",
            "Iter 1: Entropy=12547.8457, Loss=0.000273\n",
            "Iter 1: Entropy=12551.5732, Loss=0.000253\n",
            "Iter 1: Entropy=12555.1367, Loss=0.000248\n",
            "Iter 1: Entropy=12558.3496, Loss=0.000228\n",
            "Iter 2: Entropy=12561.2002, Loss=0.000217\n",
            "Iter 2: Entropy=12563.7334, Loss=0.000202\n",
            "Iter 2: Entropy=12566.0205, Loss=0.000196\n",
            "Iter 2: Entropy=12568.1348, Loss=0.000186\n",
            "Iter 2: Entropy=12570.0889, Loss=0.000180\n",
            "Iter 2: Entropy=12571.9023, Loss=0.000176\n",
            "Iter 2: Entropy=12573.6270, Loss=0.000175\n",
            "Iter 2: Entropy=12575.2197, Loss=0.000167\n",
            "weight diff tensor(1.7760e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "Iter 1: Entropy=12351.5020, Loss=0.004990\n",
            "Iter 1: Entropy=12350.1729, Loss=0.003355\n",
            "Iter 1: Entropy=12348.6592, Loss=0.001933\n",
            "Iter 1: Entropy=12346.9639, Loss=0.000924\n",
            "Iter 1: Entropy=12345.3262, Loss=0.000358\n",
            "Iter 1: Entropy=12343.8896, Loss=0.000130\n",
            "Iter 1: Entropy=12342.6846, Loss=0.000157\n",
            "Iter 1: Entropy=12341.7852, Loss=0.000327\n",
            "Iter 2: Entropy=12341.3076, Loss=0.000488\n",
            "Iter 2: Entropy=12341.2559, Loss=0.000585\n",
            "Iter 2: Entropy=12341.4795, Loss=0.000561\n",
            "Iter 2: Entropy=12341.9668, Loss=0.000436\n",
            "Iter 2: Entropy=12342.5840, Loss=0.000310\n",
            "Iter 2: Entropy=12343.2734, Loss=0.000199\n",
            "Iter 2: Entropy=12343.9678, Loss=0.000120\n",
            "Iter 2: Entropy=12344.6631, Loss=0.000107\n",
            "weight diff tensor(1.6536e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "Iter 1: Entropy=9247.0420, Loss=0.002093\n",
            "Iter 1: Entropy=9246.5908, Loss=0.001499\n",
            "Iter 1: Entropy=9246.2500, Loss=0.001156\n",
            "Iter 1: Entropy=9246.4619, Loss=0.000959\n",
            "Iter 1: Entropy=9247.2285, Loss=0.000835\n",
            "Iter 1: Entropy=9248.4336, Loss=0.000750\n",
            "Iter 1: Entropy=9249.8438, Loss=0.000687\n",
            "Iter 1: Entropy=9251.3936, Loss=0.000638\n",
            "Iter 2: Entropy=9253.0547, Loss=0.000598\n",
            "Iter 2: Entropy=9254.8135, Loss=0.000561\n",
            "Iter 2: Entropy=9256.5732, Loss=0.000532\n",
            "Iter 2: Entropy=9258.2480, Loss=0.000502\n",
            "Iter 2: Entropy=9259.8242, Loss=0.000474\n",
            "Iter 2: Entropy=9261.2598, Loss=0.000447\n",
            "Iter 2: Entropy=9262.5811, Loss=0.000426\n",
            "Iter 2: Entropy=9263.7549, Loss=0.000412\n",
            "weight diff tensor(3.3220e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "Iter 1: Entropy=3107.0024, Loss=0.000038\n",
            "Iter 1: Entropy=3107.9363, Loss=0.000036\n",
            "Iter 1: Entropy=3109.3564, Loss=0.000033\n",
            "Iter 1: Entropy=3111.0173, Loss=0.000031\n",
            "Iter 1: Entropy=3112.6790, Loss=0.000029\n",
            "Iter 1: Entropy=3114.2195, Loss=0.000027\n",
            "Iter 1: Entropy=3115.6409, Loss=0.000025\n",
            "Iter 1: Entropy=3117.0300, Loss=0.000024\n",
            "Iter 2: Entropy=3118.2993, Loss=0.000022\n",
            "Iter 2: Entropy=3119.4263, Loss=0.000022\n",
            "Iter 2: Entropy=3120.4294, Loss=0.000021\n",
            "Iter 2: Entropy=3121.3240, Loss=0.000021\n",
            "Iter 2: Entropy=3122.1357, Loss=0.000020\n",
            "Iter 2: Entropy=3122.8574, Loss=0.000019\n",
            "Iter 2: Entropy=3123.5100, Loss=0.000018\n",
            "Iter 2: Entropy=3124.1284, Loss=0.000018\n",
            "weight diff tensor(1.3567e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "Iter 1: Entropy=12461.4404, Loss=0.002182\n",
            "Iter 1: Entropy=12460.5732, Loss=0.001611\n",
            "Iter 1: Entropy=12459.3896, Loss=0.001253\n",
            "Iter 1: Entropy=12458.9395, Loss=0.001048\n",
            "Iter 1: Entropy=12459.3145, Loss=0.000892\n",
            "Iter 1: Entropy=12460.2891, Loss=0.000768\n",
            "Iter 1: Entropy=12461.6240, Loss=0.000674\n",
            "Iter 1: Entropy=12463.0078, Loss=0.000610\n",
            "Iter 2: Entropy=12464.3936, Loss=0.000559\n",
            "Iter 2: Entropy=12465.7266, Loss=0.000522\n",
            "Iter 2: Entropy=12466.9443, Loss=0.000489\n",
            "Iter 2: Entropy=12468.1328, Loss=0.000470\n",
            "Iter 2: Entropy=12469.2627, Loss=0.000445\n",
            "Iter 2: Entropy=12470.3936, Loss=0.000418\n",
            "Iter 2: Entropy=12471.5078, Loss=0.000395\n",
            "Iter 2: Entropy=12472.6230, Loss=0.000383\n",
            "weight diff tensor(2.5815e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "Iter 1: Entropy=12086.1729, Loss=0.001909\n",
            "Iter 1: Entropy=12085.6719, Loss=0.000528\n",
            "Iter 1: Entropy=12085.6104, Loss=0.000330\n",
            "Iter 1: Entropy=12085.5391, Loss=0.000517\n",
            "Iter 1: Entropy=12085.5215, Loss=0.000547\n",
            "Iter 1: Entropy=12085.5488, Loss=0.000391\n",
            "Iter 1: Entropy=12085.6719, Loss=0.000211\n",
            "Iter 1: Entropy=12085.9414, Loss=0.000154\n",
            "Iter 2: Entropy=12086.5352, Loss=0.000214\n",
            "Iter 2: Entropy=12087.4648, Loss=0.000289\n",
            "Iter 2: Entropy=12088.6533, Loss=0.000311\n",
            "Iter 2: Entropy=12090.0615, Loss=0.000220\n",
            "Iter 2: Entropy=12091.5635, Loss=0.000146\n",
            "Iter 2: Entropy=12093.0449, Loss=0.000110\n",
            "Iter 2: Entropy=12094.4766, Loss=0.000105\n",
            "Iter 2: Entropy=12095.7861, Loss=0.000134\n",
            "weight diff tensor(4.6981e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "Iter 1: Entropy=9308.6523, Loss=0.000863\n",
            "Iter 1: Entropy=9310.2402, Loss=0.000674\n",
            "Iter 1: Entropy=9311.7520, Loss=0.000573\n",
            "Iter 1: Entropy=9313.5664, Loss=0.000519\n",
            "Iter 1: Entropy=9315.9121, Loss=0.000481\n",
            "Iter 1: Entropy=9318.7422, Loss=0.000444\n",
            "Iter 1: Entropy=9321.8232, Loss=0.000413\n",
            "Iter 1: Entropy=9324.9619, Loss=0.000393\n",
            "Iter 2: Entropy=9328.1055, Loss=0.000371\n",
            "Iter 2: Entropy=9331.0859, Loss=0.000355\n",
            "Iter 2: Entropy=9333.8633, Loss=0.000339\n",
            "Iter 2: Entropy=9336.3887, Loss=0.000326\n",
            "Iter 2: Entropy=9338.6475, Loss=0.000313\n",
            "Iter 2: Entropy=9340.6016, Loss=0.000297\n",
            "Iter 2: Entropy=9342.2920, Loss=0.000285\n",
            "Iter 2: Entropy=9343.7188, Loss=0.000278\n",
            "weight diff tensor(2.3952e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "Iter 1: Entropy=3108.5574, Loss=0.000039\n",
            "Iter 1: Entropy=3109.1340, Loss=0.000030\n",
            "Iter 1: Entropy=3109.7473, Loss=0.000025\n",
            "Iter 1: Entropy=3110.5037, Loss=0.000021\n",
            "Iter 1: Entropy=3111.3347, Loss=0.000019\n",
            "Iter 1: Entropy=3112.2415, Loss=0.000017\n",
            "Iter 1: Entropy=3113.1631, Loss=0.000016\n",
            "Iter 1: Entropy=3114.0369, Loss=0.000015\n",
            "Iter 2: Entropy=3114.8247, Loss=0.000014\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6c1d2df68175>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_holder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mw_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from safetensors.torch import save_file\n",
        "#from gptq import GPTQ\n",
        "import math\n",
        "\n",
        "# === CONFIG ===\n",
        "MODEL_NAME = \"facebook/opt-125m\"\n",
        "MODEL_NAME = \"databricks/dolly-v2-3b\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32\n",
        "N_BATCHES = 8\n",
        "SEQ_LEN = 128\n",
        "NUM_BITS = 3\n",
        "BLOCK_SIZE = 128\n",
        "FIXED_T = 1000.0\n",
        "LR = 0.001\n",
        "NUM_ITERATIONS = 0\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "#model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",                # Automatically split layers across available GPUs/CPU\n",
        "    torch_dtype=\"float32\",               # Use float16 where possible\n",
        "    low_cpu_mem_usage=True            # Efficient weight loading\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# === Calibration Setup using TinyStories CSV ===\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"validation.csv\"        # Path to your TinyStories CSV\n",
        "TEXT_COLUMN = \"text\"               # Column containing stories\n",
        "N_CALIB_SAMPLES = 1000              # Number of samples to use\n",
        "\n",
        "# Load and preprocess CSV\n",
        "print(\"📖 Loading TinyStories from CSV...\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "assert TEXT_COLUMN in df.columns, f\"'{TEXT_COLUMN}' column not found in CSV.\"\n",
        "texts = df[TEXT_COLUMN].dropna().tolist()[:BATCH_SIZE * N_BATCHES]\n",
        "\n",
        "# Tokenize\n",
        "print(\"🔠 Tokenizing TinyStories for calibration...\")\n",
        "encodings = tokenizer(\n",
        "    texts,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=SEQ_LEN,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "input_ids = encodings[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = encodings[\"attention_mask\"].to(DEVICE)\n",
        "input_batches = input_ids.split(BATCH_SIZE)\n",
        "mask_batches = attention_mask.split(BATCH_SIZE)\n",
        "def get_power_bins(a=0.5, num_bits=4, device=\"cpu\"):\n",
        "    q_levels = 2 ** num_bits\n",
        "    lin = torch.linspace(0, 1, q_levels, device=device)\n",
        "    scaled = (lin ** (1 / a)) * 0.5\n",
        "    bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "    return bins\n",
        "class ChannelwiseQuantizationOptim(nn.Module):\n",
        "    def __init__(self, weight, num_bits=4, fixed_T=100.0, a=0.5):\n",
        "        super().__init__()\n",
        "        self.num_bits = num_bits\n",
        "        self.fixed_T = fixed_T\n",
        "        self.a = a\n",
        "        self.original_shape = weight.shape\n",
        "        self.num_levels = 2 ** num_bits\n",
        "\n",
        "        # Store rows (channels)\n",
        "        self.rows = [weight[i, :].detach() for i in range(weight.shape[0])]\n",
        "\n",
        "        # Create one min, max, and learnable bin set per channel\n",
        "        self.w_min = nn.ParameterList()\n",
        "        self.w_max = nn.ParameterList()\n",
        "        self.learnable_bins = nn.ParameterList()\n",
        "        for row in self.rows:\n",
        "            w_min, w_max = row.min(), row.max()\n",
        "            pad = 0.05 * (w_max - w_min)\n",
        "            self.w_min.append(nn.Parameter((w_min - pad).view(1)))\n",
        "            self.w_max.append(nn.Parameter((w_max + pad).view(1)))\n",
        "            init_bins = self._get_power_bins().detach()\n",
        "            self.learnable_bins.append(nn.Parameter(init_bins.clone()))\n",
        "\n",
        "    def _get_power_bins(self):\n",
        "        lin = torch.linspace(0, 1, self.num_levels)\n",
        "        scaled = (lin ** (1 / self.a)) * 0.5\n",
        "        bins = 0.5 + torch.cat([-scaled.flip(0), scaled[1:]])\n",
        "        return bins\n",
        "\n",
        "    def forward(self):\n",
        "        eps = 1e-6\n",
        "        quantized_rows = []\n",
        "        total_entropy = 0.0\n",
        "\n",
        "        for idx, row in enumerate(self.rows):\n",
        "            w_min = self.w_min[idx].clamp(max=self.w_max[idx].item() - eps)\n",
        "            w_max = self.w_max[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (row - w_min) / (w_max - w_min + eps)\n",
        "\n",
        "            bins = self.learnable_bins[idx].to(row.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - bins)\n",
        "            soft_probs = torch.softmax(dists * self.fixed_T, dim=-1)\n",
        "            w_q = (soft_probs * bins).sum(dim=-1)\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            quantized_rows.append(w_deq)\n",
        "\n",
        "            bin_mass = soft_probs.sum(dim=0)\n",
        "            bin_probs = bin_mass / (bin_mass.sum() + eps)\n",
        "            entropy = -(bin_probs * (bin_probs + eps).log()).sum()\n",
        "            total_entropy += entropy\n",
        "\n",
        "        return torch.stack(quantized_rows, dim=0), total_entropy\n",
        "\n",
        "    def export(self):\n",
        "        eps = 1e-6\n",
        "        quantized_rows = []\n",
        "\n",
        "        for idx, row in enumerate(self.rows):\n",
        "            w_min = self.w_min[idx].clamp(max=self.w_max[idx].item() - eps)\n",
        "            w_max = self.w_max[idx].clamp(min=w_min.item() + eps)\n",
        "            w_norm = (row - w_min) / (w_max - w_min + eps)\n",
        "\n",
        "            bins = self.learnable_bins[idx].to(row.device)\n",
        "            dists = -torch.abs(w_norm.unsqueeze(-1) - bins)\n",
        "            q_idx = torch.argmax(dists, dim=-1).to(torch.int32)\n",
        "            w_q = bins[q_idx]\n",
        "            w_deq = w_q * (w_max - w_min) + w_min\n",
        "            quantized_rows.append(w_deq)\n",
        "\n",
        "        return torch.stack(quantized_rows, dim=0).cpu()\n",
        "\n",
        "\n",
        "\n",
        "# === Quantization Loop for all Linear Layers ===\n",
        "safetensor_dict = {}\n",
        "flag = 0\n",
        "for name, module in model.named_modules():\n",
        "    if not isinstance(module, nn.Linear):\n",
        "        continue\n",
        "    # if \"lm_head\" in name:\n",
        "    #     continue\n",
        "    # if \"fc1\" in name:\n",
        "    #     continue\n",
        "\n",
        "    if \"embed_out\" in name:\n",
        "        continue\n",
        "\n",
        "\n",
        "    print(f\"\\n🔧 GPTQ + Blockwise Quantizing Layer: {name} | Shape: {module.weight.shape}\")\n",
        "\n",
        "    activation_batches = []\n",
        "    def hook_fn(mod, inp, out):\n",
        "        activation_batches.append(inp[0].detach())\n",
        "    hook = module.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, m in zip(input_batches, mask_batches):\n",
        "            model(input_ids=x, attention_mask=m)\n",
        "    hook.remove()\n",
        "\n",
        "    if not activation_batches:\n",
        "        continue\n",
        "    original_weight = module.weight.data.clone()\n",
        "    # gptq = GPTQ(module)\n",
        "    # for act in activation_batches:\n",
        "    #     gptq.add_batch(act, module(act))\n",
        "    # gptq.fasterquant(\n",
        "    #     blocksize=BLOCK_SIZE,\n",
        "    #     percdamp=0.01,\n",
        "    #     group_size=128,\n",
        "    #     actorder=True,\n",
        "    # )\n",
        "    q_weight = module.weight.data.clone()\n",
        "\n",
        "    quant_layer = ChannelwiseQuantizationOptim(original_weight,num_bits = 3,a=0.52)#.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(quant_layer.parameters(), lr=LR)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    for it in range(NUM_ITERATIONS):\n",
        "        for x, m in zip(input_batches, mask_batches):\n",
        "            # CPU capture, per-batch\n",
        "            activation_holder = []\n",
        "            def hook_fn(mod, inp, out):\n",
        "                activation_holder.append(inp[0].detach().to('cpu'))\n",
        "            hook = module.register_forward_hook(hook_fn)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model(input_ids=x, attention_mask=m)\n",
        "            hook.remove()\n",
        "\n",
        "            act = activation_holder[0].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            w_q, entropy = quant_layer()\n",
        "            recon = F.linear(act, w_q)\n",
        "            target = F.linear(act, original_weight)\n",
        "            loss = mse_loss(recon, target) + mse_loss(original_weight, w_q)\n",
        "            print(f\"Iter {it+1}: Entropy={entropy.item():.4f}, Loss={loss.item():.6f}\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            del act, recon, target, w_q, activation_holder\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        final_weight = quant_layer.export().to(module.weight.device)\n",
        "        loss = mse_loss(original_weight, final_weight)\n",
        "        print(\"weight diff\",loss)\n",
        "        module.weight.copy_(final_weight)\n",
        "        #safetensor_dict[name.replace(\".\", \"_\") + \".dequant\"] = final_weight\n",
        "    del quant_layer, optimizer, activation_batches\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# === Save Final Weights ===\n",
        "#save_file(safetensor_dict, \"quantized_blockwise_gptq.safetensors\")\n",
        "print(\"\\n✅ Finished GPTQ-initialized blockwise quantization for all layers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFA6uN1BHZ0O",
        "outputId": "c5dbc341-00dc-47e8-d048-768a6bee008d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📖 Loading TinyStories from CSV...\n",
            "🔠 Tokenizing TinyStories for calibration...\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.2316e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(4.1550e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(6.8068e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.0.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(3.9587e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(9.8122e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(4.8100e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.3671e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.1.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(6.3879e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.7700e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(4.1263e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.4033e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.2.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(6.6321e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.8797e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(5.4220e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.3090e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.3.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(6.5055e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.9258e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(5.8497e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.1772e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.4.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(6.5214e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.5166e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.3197e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.0097e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.5.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(7.1665e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.3014e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.4379e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.9039e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.6.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.7671e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.3159e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.8270e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.8414e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.7.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.9930e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.3600e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(7.1304e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.7121e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.8.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.4446e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.3500e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(7.3172e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.5812e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.9.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.5209e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.2918e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(7.1448e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.4712e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.10.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.1589e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.1690e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(7.8174e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.4294e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.11.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.1535e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.2359e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.0863e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.5491e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.12.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.3566e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.4638e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(7.7444e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.5318e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.13.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(8.8291e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(8.9322e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.4408e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.6508e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.14.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(9.0642e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(9.3696e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.5288e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.6917e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.15.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.0213e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(9.4017e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.1309e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.7841e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.16.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.0277e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(1.0426e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.6703e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(7.9264e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.17.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.1284e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.18.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(1.0747e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.18.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.3201e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.18.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.0429e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.18.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.1735e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.19.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(1.1424e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.19.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.2651e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.19.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.2115e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.19.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.1193e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.20.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(1.2197e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.20.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.2454e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.20.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.4304e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.20.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.1901e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.21.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(1.2900e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.21.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(8.0079e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.21.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.6182e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.21.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.1962e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.22.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(1.5643e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.22.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(7.4593e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.22.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.8496e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.22.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.2154e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.23.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.0909e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.23.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.6203e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.23.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.1790e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.23.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.1911e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.24.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.3010e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.24.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.4389e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.24.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.2919e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.24.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.2514e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.25.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.3465e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.25.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.4386e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.25.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.3598e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.25.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.3007e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.26.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.3617e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.26.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(5.5908e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.26.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.2994e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.26.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.4695e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.27.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.5192e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.27.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.6120e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.27.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.2718e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.27.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.5835e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.28.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.4822e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.28.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.4897e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.28.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.2911e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.28.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.8952e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.29.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.3930e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.29.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(6.1049e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.29.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.1862e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.29.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.1559e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.30.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.6716e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.30.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(5.6745e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.30.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(9.1741e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.30.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(2.2727e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.31.attention.query_key_value | Shape: torch.Size([7680, 2560])\n",
            "weight diff tensor(2.0358e-05, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.31.attention.dense | Shape: torch.Size([2560, 2560])\n",
            "weight diff tensor(4.7300e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.31.mlp.dense_h_to_4h | Shape: torch.Size([10240, 2560])\n",
            "weight diff tensor(8.8075e-06, device='cuda:0')\n",
            "\n",
            "🔧 GPTQ + Blockwise Quantizing Layer: gpt_neox.layers.31.mlp.dense_4h_to_h | Shape: torch.Size([2560, 10240])\n",
            "weight diff tensor(1.9775e-05, device='cuda:0')\n",
            "\n",
            "✅ Finished GPTQ-initialized blockwise quantization for all layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
        "# ==== Test quantized model ====\n",
        "#model.eval()\n",
        "prompt = \"I like travelling\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_length=30)\n",
        "\n",
        "print(\"Sample Output:\", tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFTHulsxIZKz",
        "outputId": "69298abe-5518-4678-c627-532302994086"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Output: I like travelling with a companion, and I like to be alone at times. I'm a very social person, but I need alone time to recharge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hte8Df_rYOkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}